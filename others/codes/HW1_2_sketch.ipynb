{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENGR-E 533: Deep Learning Systems\n",
    "## Homework 1\n",
    "\n",
    "### Khandokar Md. Nayem (knayem@iu.edu)\n",
    "### Mar 7, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary files and set environment parameters\n",
    "My assigned Node is `r-006` and GPU `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy -> Input Data\n",
    "sn, sr=librosa.load('train_dirty_male.wav', sr=None)\n",
    "X=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "X_mag = np.abs(X)\n",
    "\n",
    "# Clean -> Label\n",
    "s, sr=librosa.load('train_clean_male.wav', sr=None)\n",
    "S=librosa.stft(s, n_fft=1024, hop_length=512)\n",
    "S_mag = np.abs(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(X,Y, batch_size):\n",
    "    num_samples, _ = X.shape\n",
    "    \n",
    "    selected_indics = np.random.randint(num_samples, size=batch_size)\n",
    "#     print(selected_indics)\n",
    "    return X[selected_indics], Y[selected_indics]\n",
    "    \n",
    "    \n",
    "def next_batch_2(X,Y, batch_size):\n",
    "    num_samples, _ = X.shape\n",
    "    \n",
    "    selected_indics = np.random.randint(num_samples-batch_size)\n",
    "#     print(selected_indics)\n",
    "    return X[selected_indics:selected_indics+batch_size], Y[selected_indics:selected_indics+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,y = next_batch_2(X_mag.T, S_mag.T, 128)\n",
    "# print(x.shape, y.shape)\n",
    "\n",
    "NUM_ITERATION = 1000\n",
    "BATCH_SIZE = 64\n",
    "# Small epsilon value for the BN transform\n",
    "epsilon = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier Initialization of Weights\n",
    "These are the weight initialization function used in defining model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable (shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = np.sqrt(2.0/sum(shape)) )\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable (shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = np.sqrt(1.0/sum(shape)) )\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the fully connected model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 513]) \n",
    "\n",
    "W_1 = weight_variable([513, 1024])\n",
    "b_1 = bias_variable([1024])\n",
    "\n",
    "W_2 = weight_variable([1024, 1024])\n",
    "b_2 = bias_variable([1024])\n",
    "\n",
    "W_3 = weight_variable([1024, 1024])\n",
    "b_3 = bias_variable([1024])\n",
    "\n",
    "W_4 = weight_variable([1024, 1024])\n",
    "b_4 = bias_variable([1024])\n",
    "\n",
    "W_5 = weight_variable([1024, 513])\n",
    "b_5 = bias_variable([513])\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, [None, 513]) # original\n",
    "\n",
    "\n",
    "# Layer connections and Activation functions\n",
    "y_1 = tf.nn.relu(tf.matmul(x, W_1) + b_1)\n",
    "y_2 = tf.nn.relu(tf.matmul(y_1, W_2) + b_2)\n",
    "y_3 = tf.nn.relu(tf.matmul(y_2, W_3) + b_3)\n",
    "y_4 = tf.nn.relu(tf.matmul(y_3, W_4) + b_4)\n",
    "y =  tf.nn.relu(tf.matmul(y_4, W_5) + b_5) # predicted\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "mse = tf.reduce_sum( tf.losses.mean_squared_error(labels=y_, predictions=y) )\n",
    "# mse = -tf.reduce_sum(y_*tf.log(y))\n",
    "train_step = tf.train.AdamOptimizer().minimize(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model\n",
    "If we increase the number of iteration (Line 10, currently 1000), `Accuracy` increases; but more time is needed to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration to control GPU use\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.33\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "\n",
    "# Train Model\n",
    "for _ in range(NUM_ITERATION):\n",
    "#     batch_xs, batch_ys = next_batch(X_mag.T,S_mag.T, BATCH_SIZE)\n",
    "#     sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    batch_xs, batch_ys = next_batch_2(X_mag.T,S_mag.T, BATCH_SIZE)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "#     sess.run(train_step, feed_dict={x: X_mag.T, y_: S_mag.T})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513, 142)\n"
     ]
    }
   ],
   "source": [
    "# Load Test data-1\n",
    "sn, sr=librosa.load('test_x_01.wav', sr=None)\n",
    "X_test_01=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "X_mag_test_01 = np.abs(X_test_01)\n",
    "\n",
    "# Load Test data-2\n",
    "sn, sr=librosa.load('test_x_02.wav', sr=None)\n",
    "X_test_02=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "X_mag_test_02 = np.abs(X_test_02)\n",
    "\n",
    "print(X_mag_test_01.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model-1\n",
    "S_hat_mag_test_01=sess.run(y, feed_dict={x: X_mag_test_01.T})\n",
    "S_hat_test_01=(X_test_01/X_mag_test_01)*S_hat_mag_test_01.T\n",
    "S_hat_01=librosa.istft(S_hat_test_01, hop_length=512)\n",
    "librosa.output.write_wav('test_s_01_recons.wav', S_hat_01, sr)\n",
    "\n",
    "# Test model-2\n",
    "S_hat_mag_test_02=sess.run(y, feed_dict={x: X_mag_test_02.T})\n",
    "S_hat_test_02=(X_test_02/X_mag_test_02)*S_hat_mag_test_02.T\n",
    "S_hat_02=librosa.istft(S_hat_test_02, hop_length=512)\n",
    "librosa.output.write_wav('test_s_02_recons.wav', S_hat_02, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Norm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm_wrapper(inputs, is_training, decay = 0.999):\n",
    "\n",
    "    scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n",
    "    beta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n",
    "    pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "    pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "\n",
    "    if is_training:\n",
    "        batch_mean, batch_var = tf.nn.moments(inputs,[0])\n",
    "        train_mean = tf.assign(pop_mean,\n",
    "                               pop_mean * decay + batch_mean * (1 - decay))\n",
    "        train_var = tf.assign(pop_var,\n",
    "                              pop_var * decay + batch_var * (1 - decay))\n",
    "        \n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(inputs,\n",
    "                batch_mean, batch_var, beta, scale, epsilon)\n",
    "    else:\n",
    "        return tf.nn.batch_normalization(inputs,\n",
    "            pop_mean, pop_var, beta, scale, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(is_training):\n",
    "    # Placeholders\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 513])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 513])\n",
    "\n",
    "    # Layer 1\n",
    "    w1 = weight_variable([513, 1024])\n",
    "    z1 = tf.matmul(x,w1)\n",
    "    bn1 = batch_norm_wrapper(z1, is_training)\n",
    "    l1 = tf.nn.relu(bn1)\n",
    "\n",
    "    #Layer 2\n",
    "    w2 = weight_variable([1024, 1024])\n",
    "    z2 = tf.matmul(l1,w2)\n",
    "    bn2 = batch_norm_wrapper(z2, is_training)\n",
    "    l2 = tf.nn.relu(bn2)\n",
    "    \n",
    "#     #Layer 3\n",
    "#     w4 = weight_variable([1024, 1024])\n",
    "#     z4 = tf.matmul(l2,w4)\n",
    "#     bn4 = batch_norm_wrapper(z4, is_training)\n",
    "#     l3 = tf.nn.relu(bn4)\n",
    "\n",
    "    # relu\n",
    "    w3 = weight_variable([1024, 513])\n",
    "    b3 = bias_variable([513])\n",
    "    y  = tf.nn.relu(tf.matmul(l2, w3)+b3)\n",
    "\n",
    "    # Loss, Optimizer and Predictions\n",
    "#     cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "    cross_entropy = tf.reduce_sum( tf.losses.mean_squared_error(labels=y_, predictions=y) )\n",
    "\n",
    "#     train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "    train_step = tf.train.AdamOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "    return (x, y_), train_step, y, tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "(x, y_), train_step, y_hat,saver = build_graph(is_training=True)\n",
    "\n",
    "# Configuration to control GPU use\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.33\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Train Model\n",
    "for _ in range(NUM_ITERATION):\n",
    "    batch_xs, batch_ys = next_batch_2(X_mag.T,S_mag.T, BATCH_SIZE)\n",
    "#     train_step.run(feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    \n",
    "saved_model = saver.save(sess, './temp-bn-save') \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./temp-bn-save\n"
     ]
    }
   ],
   "source": [
    "sess.close()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "(x, y_), train_step, y_hat, saver = build_graph(is_training=False)\n",
    "\n",
    "# Configuration to control GPU use\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.33\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "saver.restore(sess, './temp-bn-save')\n",
    "\n",
    "\n",
    "# Load Test data-1\n",
    "sn, sr=librosa.load('test_x_01.wav', sr=None)\n",
    "X_test_01=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "X_mag_test_01 = np.abs(X_test_01)\n",
    "\n",
    "# Load Test data-2\n",
    "sn, sr=librosa.load('test_x_02.wav', sr=None)\n",
    "X_test_02=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "X_mag_test_02 = np.abs(X_test_02)\n",
    "\n",
    "# Test model-1\n",
    "S_hat_mag_test_01=sess.run(y_hat, feed_dict={x: X_mag_test_01.T})\n",
    "S_hat_test_01=(X_test_01/X_mag_test_01)*S_hat_mag_test_01.T\n",
    "S_hat_01=librosa.istft(S_hat_test_01, hop_length=512)\n",
    "librosa.output.write_wav('test_s_01_recons.wav', S_hat_01, sr)\n",
    "\n",
    "# Test model-2\n",
    "S_hat_mag_test_02=sess.run(y_hat, feed_dict={x: X_mag_test_02.T})\n",
    "S_hat_test_02=(X_test_02/X_mag_test_02)*S_hat_mag_test_02.T\n",
    "S_hat_02=librosa.istft(S_hat_test_02, hop_length=512)\n",
    "librosa.output.write_wav('test_s_02_recons.wav', S_hat_02, sr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
