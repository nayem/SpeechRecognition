{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENGR-E 533: Deep Learning Systems\n",
    "## Homework 2\n",
    "\n",
    "### Khandokar Md. Nayem (knayem@iu.edu)\n",
    "### Apr 3, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary files and set environment parameters\n",
    "My assigned Node is `r-005` and GPU `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Dropout, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, TimeDistributed\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables\n",
    "\n",
    "Directory names, file formates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_directory = '/N/u/knayem/data/timit-homework/'\n",
    "\n",
    "PATH_train = 'tr/'\n",
    "PATH_val = 'v/'\n",
    "PATH_test = 'te/'\n",
    "PATH_denoise = 'te/'\n",
    "\n",
    "CLEAN_format_train = 'trs*.wav'\n",
    "NOISE_format_train = 'trn*.wav'\n",
    "MIX_format_train = 'trx*.wav'\n",
    "\n",
    "CLEAN_format_val = 'vs*.wav'\n",
    "NOISE_format_val = 'vn*.wav'\n",
    "MIX_format_val = 'vx*.wav'\n",
    "\n",
    "MIX_format_test = 'tex*.wav'\n",
    "\n",
    "Max_RNN = 5 # Number of Time Stamps in a RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Names\n",
    "\n",
    "Since there are huge number of files for both training, validation and testing, for convinience we write them in files. These are the name of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_S = 'TRAIN_S.txt'\n",
    "TRAIN_N = 'TRAIN_N.txt'\n",
    "TRAIN_X_cmplx = 'TRAIN_X_cmplx.txt'\n",
    "TRAIN_X = 'TRAIN_X.txt'\n",
    "\n",
    "VAL_S = 'VAL_S.txt'\n",
    "VAL_S_cmplx = 'VAL_S_cmplx.txt'\n",
    "VAL_N = 'VAL_N.txt'\n",
    "VAL_X_cmplx = 'VAL_X_cmplx.txt'\n",
    "VAL_X = 'VAL_X.txt'\n",
    "\n",
    "\n",
    "TEST_S = 'TEST_S.txt'\n",
    "TEST_N = 'TEST_N.txt'\n",
    "TEST_X_cmplx = 'TEST_X_cmplx.txt'\n",
    "TEST_X = 'TEST_X.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a .wav file\n",
    "A function to get the Complex and Magnitude Spectum of a file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprossed_data(file_name):\n",
    "    \n",
    "    sn, sr=librosa.load(file_name, sr=None)\n",
    "    X=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "    X_mag = np.abs(X)\n",
    "#     print('X_mag',X_mag.shape)\n",
    "    return X, X_mag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Train Data  \n",
    "\n",
    "Here we open the files, and read the Train Dataset. We need to run this portion for the first time only. This file reading needs lots fo time. After saving in file once, we can load data from files unless you want complex version of Noisy Speech. Complex numbers are hard to retrive from files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_train_s = []\n",
    "DATA_train_n = []\n",
    "DATA_train_x = []\n",
    "DATA_train_x_cmplx = []\n",
    "\n",
    "with open(TRAIN_S,'wb') as fs, open(TRAIN_N,'wb') as fn, open(TRAIN_X_cmplx,'wb') as fx_cmplx, open(TRAIN_X,'wb') as fx: \n",
    "    \n",
    "    for file_s, file_n, file_x in zip(sorted(glob.glob(PATH_directory+PATH_train+CLEAN_format_train)),sorted(glob.glob(PATH_directory+PATH_train+NOISE_format_train)),sorted(glob.glob(PATH_directory+PATH_train+MIX_format_train))):\n",
    "        _,s = preprossed_data(file_s)\n",
    "        DATA_train_s.append(np.array(s))\n",
    "        np.savetxt(fs, s, fmt='%.5f')\n",
    "        fs.write(b'\\n')\n",
    "        \n",
    "        _,n = preprossed_data(file_n)\n",
    "        DATA_train_n.append(np.array(n))\n",
    "        np.savetxt(fn, n, fmt='%.5f')\n",
    "        fn.write(b'\\n')\n",
    "        \n",
    "        x_cmplx,x = preprossed_data(file_x)\n",
    "        DATA_train_x.append(np.array(x))\n",
    "        np.savetxt(fx, x, fmt='%.5f')\n",
    "        fx.write(b'\\n')\n",
    "        \n",
    "        DATA_train_x_cmplx.append(np.array(x_cmplx))\n",
    "        np.savetxt(fx_cmplx, x_cmplx, fmt='%.5f')\n",
    "        fx_cmplx.write(b'\\n')\n",
    "        \n",
    "        \n",
    "fs.close()\n",
    "fn.close()\n",
    "fx_cmplx.close()\n",
    "fx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Validate Data\n",
    "\n",
    "Here we open the files, and read the Validation Dataset. To evaluate SNR of validation data, we need to run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ...\n",
      "2 ...\n",
      "3 ...\n",
      "4 ...\n",
      "5 ...\n",
      "6 ...\n",
      "7 ...\n",
      "8 ...\n",
      "9 ...\n",
      "10 ...\n",
      "11 ...\n",
      "12 ...\n",
      "13 ...\n",
      "14 ...\n",
      "15 ...\n",
      "16 ...\n",
      "17 ...\n",
      "18 ...\n",
      "19 ...\n",
      "20 ...\n",
      "21 ...\n",
      "22 ...\n",
      "23 ...\n",
      "24 ...\n",
      "25 ...\n",
      "26 ...\n",
      "27 ...\n",
      "28 ...\n",
      "29 ...\n",
      "30 ...\n",
      "31 ...\n",
      "32 ...\n",
      "33 ...\n",
      "34 ...\n",
      "35 ...\n",
      "36 ...\n",
      "37 ...\n",
      "38 ...\n",
      "39 ...\n",
      "40 ...\n",
      "41 ...\n",
      "42 ...\n",
      "43 ...\n",
      "44 ...\n",
      "45 ...\n",
      "46 ...\n",
      "47 ...\n",
      "48 ...\n",
      "49 ...\n",
      "50 ...\n",
      "51 ...\n",
      "52 ...\n",
      "53 ...\n",
      "54 ...\n",
      "55 ...\n",
      "56 ...\n",
      "57 ...\n",
      "58 ...\n",
      "59 ...\n",
      "60 ...\n",
      "61 ...\n",
      "62 ...\n",
      "63 ...\n",
      "64 ...\n",
      "65 ...\n",
      "66 ...\n",
      "67 ...\n",
      "68 ...\n",
      "69 ...\n",
      "70 ...\n",
      "71 ...\n",
      "72 ...\n",
      "73 ...\n",
      "74 ...\n",
      "75 ...\n",
      "76 ...\n",
      "77 ...\n",
      "78 ...\n",
      "79 ...\n",
      "80 ...\n",
      "81 ...\n",
      "82 ...\n",
      "83 ...\n",
      "84 ...\n",
      "85 ...\n",
      "86 ...\n",
      "87 ...\n",
      "88 ...\n",
      "89 ...\n",
      "90 ...\n",
      "91 ...\n",
      "92 ...\n",
      "93 ...\n",
      "94 ...\n",
      "95 ...\n",
      "96 ...\n",
      "97 ...\n",
      "98 ...\n",
      "99 ...\n",
      "100 ...\n",
      "101 ...\n",
      "102 ...\n",
      "103 ...\n",
      "104 ...\n",
      "105 ...\n",
      "106 ...\n",
      "107 ...\n",
      "108 ...\n",
      "109 ...\n",
      "110 ...\n",
      "111 ...\n",
      "112 ...\n",
      "113 ...\n",
      "114 ...\n",
      "115 ...\n",
      "116 ...\n",
      "117 ...\n",
      "118 ...\n",
      "119 ...\n",
      "120 ...\n",
      "121 ...\n",
      "122 ...\n",
      "123 ...\n",
      "124 ...\n",
      "125 ...\n",
      "126 ...\n",
      "127 ...\n",
      "128 ...\n",
      "129 ...\n",
      "130 ...\n",
      "131 ...\n",
      "132 ...\n",
      "133 ...\n",
      "134 ...\n",
      "135 ...\n",
      "136 ...\n",
      "137 ...\n",
      "138 ...\n",
      "139 ...\n",
      "140 ...\n",
      "141 ...\n",
      "142 ...\n",
      "143 ...\n",
      "144 ...\n",
      "145 ...\n",
      "146 ...\n",
      "147 ...\n",
      "148 ...\n",
      "149 ...\n",
      "150 ...\n",
      "151 ...\n",
      "152 ...\n",
      "153 ...\n",
      "154 ...\n",
      "155 ...\n",
      "156 ...\n",
      "157 ...\n",
      "158 ...\n",
      "159 ...\n",
      "160 ...\n",
      "161 ...\n",
      "162 ...\n",
      "163 ...\n",
      "164 ...\n",
      "165 ...\n",
      "166 ...\n",
      "167 ...\n",
      "168 ...\n",
      "169 ...\n",
      "170 ...\n",
      "171 ...\n",
      "172 ...\n",
      "173 ...\n",
      "174 ...\n",
      "175 ...\n",
      "176 ...\n",
      "177 ...\n",
      "178 ...\n",
      "179 ...\n",
      "180 ...\n",
      "181 ...\n",
      "182 ...\n",
      "183 ...\n",
      "184 ...\n",
      "185 ...\n",
      "186 ...\n",
      "187 ...\n",
      "188 ...\n",
      "189 ...\n",
      "190 ...\n",
      "191 ...\n",
      "192 ...\n",
      "193 ...\n",
      "194 ...\n",
      "195 ...\n",
      "196 ...\n",
      "197 ...\n",
      "198 ...\n",
      "199 ...\n",
      "200 ...\n",
      "201 ...\n",
      "202 ...\n",
      "203 ...\n",
      "204 ...\n",
      "205 ...\n",
      "206 ...\n",
      "207 ...\n",
      "208 ...\n",
      "209 ...\n",
      "210 ...\n",
      "211 ...\n",
      "212 ...\n",
      "213 ...\n",
      "214 ...\n",
      "215 ...\n",
      "216 ...\n",
      "217 ...\n",
      "218 ...\n",
      "219 ...\n",
      "220 ...\n",
      "221 ...\n",
      "222 ...\n",
      "223 ...\n",
      "224 ...\n",
      "225 ...\n",
      "226 ...\n",
      "227 ...\n",
      "228 ...\n",
      "229 ...\n",
      "230 ...\n",
      "231 ...\n",
      "232 ...\n",
      "233 ...\n",
      "234 ...\n",
      "235 ...\n",
      "236 ...\n",
      "237 ...\n",
      "238 ...\n",
      "239 ...\n",
      "240 ...\n",
      "241 ...\n",
      "242 ...\n",
      "243 ...\n",
      "244 ...\n",
      "245 ...\n",
      "246 ...\n",
      "247 ...\n",
      "248 ...\n",
      "249 ...\n",
      "250 ...\n",
      "251 ...\n",
      "252 ...\n",
      "253 ...\n",
      "254 ...\n",
      "255 ...\n",
      "256 ...\n",
      "257 ...\n",
      "258 ...\n",
      "259 ...\n",
      "260 ...\n",
      "261 ...\n",
      "262 ...\n",
      "263 ...\n",
      "264 ...\n",
      "265 ...\n",
      "266 ...\n",
      "267 ...\n",
      "268 ...\n",
      "269 ...\n",
      "270 ...\n",
      "271 ...\n",
      "272 ...\n",
      "273 ...\n",
      "274 ...\n",
      "275 ...\n",
      "276 ...\n",
      "277 ...\n",
      "278 ...\n",
      "279 ...\n",
      "280 ...\n",
      "281 ...\n",
      "282 ...\n",
      "283 ...\n",
      "284 ...\n",
      "285 ...\n",
      "286 ...\n",
      "287 ...\n",
      "288 ...\n",
      "289 ...\n",
      "290 ...\n",
      "291 ...\n",
      "292 ...\n",
      "293 ...\n",
      "294 ...\n",
      "295 ...\n",
      "296 ...\n",
      "297 ...\n",
      "298 ...\n",
      "299 ...\n",
      "300 ...\n",
      "301 ...\n",
      "302 ...\n",
      "303 ...\n",
      "304 ...\n",
      "305 ...\n",
      "306 ...\n",
      "307 ...\n",
      "308 ...\n",
      "309 ...\n",
      "310 ...\n",
      "311 ...\n",
      "312 ...\n",
      "313 ...\n",
      "314 ...\n",
      "315 ...\n",
      "316 ...\n",
      "317 ...\n",
      "318 ...\n",
      "319 ...\n",
      "320 ...\n",
      "321 ...\n",
      "322 ...\n",
      "323 ...\n",
      "324 ...\n",
      "325 ...\n",
      "326 ...\n",
      "327 ...\n",
      "328 ...\n",
      "329 ...\n",
      "330 ...\n",
      "331 ...\n",
      "332 ...\n",
      "333 ...\n",
      "334 ...\n",
      "335 ...\n",
      "336 ...\n",
      "337 ...\n",
      "338 ...\n",
      "339 ...\n",
      "340 ...\n",
      "341 ...\n",
      "342 ...\n",
      "343 ...\n",
      "344 ...\n",
      "345 ...\n",
      "346 ...\n",
      "347 ...\n",
      "348 ...\n",
      "349 ...\n",
      "350 ...\n",
      "351 ...\n",
      "352 ...\n",
      "353 ...\n",
      "354 ...\n",
      "355 ...\n",
      "356 ...\n",
      "357 ...\n",
      "358 ...\n",
      "359 ...\n",
      "360 ...\n",
      "361 ...\n",
      "362 ...\n",
      "363 ...\n",
      "364 ...\n",
      "365 ...\n",
      "366 ...\n",
      "367 ...\n",
      "368 ...\n",
      "369 ...\n",
      "370 ...\n",
      "371 ...\n",
      "372 ...\n",
      "373 ...\n",
      "374 ...\n",
      "375 ...\n",
      "376 ...\n",
      "377 ...\n",
      "378 ...\n",
      "379 ...\n",
      "380 ...\n",
      "381 ...\n",
      "382 ...\n",
      "383 ...\n",
      "384 ...\n",
      "385 ...\n",
      "386 ...\n",
      "387 ...\n",
      "388 ...\n",
      "389 ...\n",
      "390 ...\n",
      "391 ...\n",
      "392 ...\n",
      "393 ...\n",
      "394 ...\n",
      "395 ...\n",
      "396 ...\n",
      "397 ...\n",
      "398 ...\n",
      "399 ...\n",
      "400 ...\n",
      "401 ...\n",
      "402 ...\n",
      "403 ...\n",
      "404 ...\n",
      "405 ...\n",
      "406 ...\n",
      "407 ...\n",
      "408 ...\n",
      "409 ...\n",
      "410 ...\n",
      "411 ...\n",
      "412 ...\n",
      "413 ...\n",
      "414 ...\n",
      "415 ...\n",
      "416 ...\n",
      "417 ...\n",
      "418 ...\n",
      "419 ...\n",
      "420 ...\n",
      "421 ...\n",
      "422 ...\n",
      "423 ...\n",
      "424 ...\n",
      "425 ...\n",
      "426 ...\n",
      "427 ...\n",
      "428 ...\n",
      "429 ...\n",
      "430 ...\n",
      "431 ...\n",
      "432 ...\n",
      "433 ...\n",
      "434 ...\n",
      "435 ...\n",
      "436 ...\n",
      "437 ...\n",
      "438 ...\n",
      "439 ...\n",
      "440 ...\n",
      "441 ...\n",
      "442 ...\n",
      "443 ...\n",
      "444 ...\n",
      "445 ...\n",
      "446 ...\n",
      "447 ...\n",
      "448 ...\n",
      "449 ...\n",
      "450 ...\n",
      "451 ...\n",
      "452 ...\n",
      "453 ...\n",
      "454 ...\n",
      "455 ...\n",
      "456 ...\n",
      "457 ...\n",
      "458 ...\n",
      "459 ...\n",
      "460 ...\n",
      "461 ...\n",
      "462 ...\n",
      "463 ...\n",
      "464 ...\n",
      "465 ...\n",
      "466 ...\n",
      "467 ...\n",
      "468 ...\n",
      "469 ...\n",
      "470 ...\n",
      "471 ...\n",
      "472 ...\n",
      "473 ...\n",
      "474 ...\n",
      "475 ...\n",
      "476 ...\n",
      "477 ...\n",
      "478 ...\n",
      "479 ...\n",
      "480 ...\n",
      "481 ...\n",
      "482 ...\n",
      "483 ...\n",
      "484 ...\n",
      "485 ...\n",
      "486 ...\n",
      "487 ...\n",
      "488 ...\n",
      "489 ...\n",
      "490 ...\n",
      "491 ...\n",
      "492 ...\n",
      "493 ...\n",
      "494 ...\n",
      "495 ...\n",
      "496 ...\n",
      "497 ...\n",
      "498 ...\n",
      "499 ...\n",
      "500 ...\n",
      "501 ...\n",
      "502 ...\n",
      "503 ...\n",
      "504 ...\n",
      "505 ...\n",
      "506 ...\n",
      "507 ...\n",
      "508 ...\n",
      "509 ...\n",
      "510 ...\n",
      "511 ...\n",
      "512 ...\n",
      "513 ...\n",
      "514 ...\n",
      "515 ...\n",
      "516 ...\n",
      "517 ...\n",
      "518 ...\n",
      "519 ...\n",
      "520 ...\n",
      "521 ...\n",
      "522 ...\n",
      "523 ...\n",
      "524 ...\n",
      "525 ...\n",
      "526 ...\n",
      "527 ...\n",
      "528 ...\n",
      "529 ...\n",
      "530 ...\n",
      "531 ...\n",
      "532 ...\n",
      "533 ...\n",
      "534 ...\n",
      "535 ...\n",
      "536 ...\n",
      "537 ...\n",
      "538 ...\n",
      "539 ...\n",
      "540 ...\n",
      "541 ...\n",
      "542 ...\n",
      "543 ...\n",
      "544 ...\n",
      "545 ...\n",
      "546 ...\n",
      "547 ...\n",
      "548 ...\n",
      "549 ...\n",
      "550 ...\n",
      "551 ...\n",
      "552 ...\n",
      "553 ...\n",
      "554 ...\n",
      "555 ...\n",
      "556 ...\n",
      "557 ...\n",
      "558 ...\n",
      "559 ...\n",
      "560 ...\n",
      "561 ...\n",
      "562 ...\n",
      "563 ...\n",
      "564 ...\n",
      "565 ...\n",
      "566 ...\n",
      "567 ...\n",
      "568 ...\n",
      "569 ...\n",
      "570 ...\n",
      "571 ...\n",
      "572 ...\n",
      "573 ...\n",
      "574 ...\n",
      "575 ...\n",
      "576 ...\n",
      "577 ...\n",
      "578 ...\n",
      "579 ...\n",
      "580 ...\n",
      "581 ...\n",
      "582 ...\n",
      "583 ...\n",
      "584 ...\n",
      "585 ...\n",
      "586 ...\n",
      "587 ...\n",
      "588 ...\n",
      "589 ...\n",
      "590 ...\n",
      "591 ...\n",
      "592 ...\n",
      "593 ...\n",
      "594 ...\n",
      "595 ...\n",
      "596 ...\n",
      "597 ...\n",
      "598 ...\n",
      "599 ...\n",
      "600 ...\n",
      "601 ...\n",
      "602 ...\n",
      "603 ...\n",
      "604 ...\n",
      "605 ...\n",
      "606 ...\n",
      "607 ...\n",
      "608 ...\n",
      "609 ...\n",
      "610 ...\n",
      "611 ...\n",
      "612 ...\n",
      "613 ...\n",
      "614 ...\n",
      "615 ...\n",
      "616 ...\n",
      "617 ...\n",
      "618 ...\n",
      "619 ...\n",
      "620 ...\n",
      "621 ...\n",
      "622 ...\n",
      "623 ...\n",
      "624 ...\n",
      "625 ...\n",
      "626 ...\n",
      "627 ...\n",
      "628 ...\n",
      "629 ...\n",
      "630 ...\n",
      "631 ...\n",
      "632 ...\n",
      "633 ...\n",
      "634 ...\n",
      "635 ...\n",
      "636 ...\n",
      "637 ...\n",
      "638 ...\n",
      "639 ...\n",
      "640 ...\n",
      "641 ...\n",
      "642 ...\n",
      "643 ...\n",
      "644 ...\n",
      "645 ...\n",
      "646 ...\n",
      "647 ...\n",
      "648 ...\n",
      "649 ...\n",
      "650 ...\n",
      "651 ...\n",
      "652 ...\n",
      "653 ...\n",
      "654 ...\n",
      "655 ...\n",
      "656 ...\n",
      "657 ...\n",
      "658 ...\n",
      "659 ...\n",
      "660 ...\n",
      "661 ...\n",
      "662 ...\n",
      "663 ...\n",
      "664 ...\n",
      "665 ...\n",
      "666 ...\n",
      "667 ...\n",
      "668 ...\n",
      "669 ...\n",
      "670 ...\n",
      "671 ...\n",
      "672 ...\n",
      "673 ...\n",
      "674 ...\n",
      "675 ...\n",
      "676 ...\n",
      "677 ...\n",
      "678 ...\n",
      "679 ...\n",
      "680 ...\n",
      "681 ...\n",
      "682 ...\n",
      "683 ...\n",
      "684 ...\n",
      "685 ...\n",
      "686 ...\n",
      "687 ...\n",
      "688 ...\n",
      "689 ...\n",
      "690 ...\n",
      "691 ...\n",
      "692 ...\n",
      "693 ...\n",
      "694 ...\n",
      "695 ...\n",
      "696 ...\n",
      "697 ...\n",
      "698 ...\n",
      "699 ...\n",
      "700 ...\n",
      "701 ...\n",
      "702 ...\n",
      "703 ...\n",
      "704 ...\n",
      "705 ...\n",
      "706 ...\n",
      "707 ...\n",
      "708 ...\n",
      "709 ...\n",
      "710 ...\n",
      "711 ...\n",
      "712 ...\n",
      "713 ...\n",
      "714 ...\n",
      "715 ...\n",
      "716 ...\n",
      "717 ...\n",
      "718 ...\n",
      "719 ...\n",
      "720 ...\n",
      "721 ...\n",
      "722 ...\n",
      "723 ...\n",
      "724 ...\n",
      "725 ...\n",
      "726 ...\n",
      "727 ...\n",
      "728 ...\n",
      "729 ...\n",
      "730 ...\n",
      "731 ...\n",
      "732 ...\n",
      "733 ...\n",
      "734 ...\n",
      "735 ...\n",
      "736 ...\n",
      "737 ...\n",
      "738 ...\n",
      "739 ...\n",
      "740 ...\n",
      "741 ...\n",
      "742 ...\n",
      "743 ...\n",
      "744 ...\n",
      "745 ...\n",
      "746 ...\n",
      "747 ...\n",
      "748 ...\n",
      "749 ...\n",
      "750 ...\n",
      "751 ...\n",
      "752 ...\n",
      "753 ...\n",
      "754 ...\n",
      "755 ...\n",
      "756 ...\n",
      "757 ...\n",
      "758 ...\n",
      "759 ...\n",
      "760 ...\n",
      "761 ...\n",
      "762 ...\n",
      "763 ...\n",
      "764 ...\n",
      "765 ...\n",
      "766 ...\n",
      "767 ...\n",
      "768 ...\n",
      "769 ...\n",
      "770 ...\n",
      "771 ...\n",
      "772 ...\n",
      "773 ...\n",
      "774 ...\n",
      "775 ...\n",
      "776 ...\n",
      "777 ...\n",
      "778 ...\n",
      "779 ...\n",
      "780 ...\n",
      "781 ...\n",
      "782 ...\n",
      "783 ...\n",
      "784 ...\n",
      "785 ...\n",
      "786 ...\n",
      "787 ...\n",
      "788 ...\n",
      "789 ...\n",
      "790 ...\n",
      "791 ...\n",
      "792 ...\n",
      "793 ...\n",
      "794 ...\n",
      "795 ...\n",
      "796 ...\n",
      "797 ...\n",
      "798 ...\n",
      "799 ...\n",
      "800 ...\n",
      "801 ...\n",
      "802 ...\n",
      "803 ...\n",
      "804 ...\n",
      "805 ...\n",
      "806 ...\n",
      "807 ...\n",
      "808 ...\n",
      "809 ...\n",
      "810 ...\n",
      "811 ...\n",
      "812 ...\n",
      "813 ...\n",
      "814 ...\n",
      "815 ...\n",
      "816 ...\n",
      "817 ...\n",
      "818 ...\n",
      "819 ...\n",
      "820 ...\n",
      "821 ...\n",
      "822 ...\n",
      "823 ...\n",
      "824 ...\n",
      "825 ...\n",
      "826 ...\n",
      "827 ...\n",
      "828 ...\n",
      "829 ...\n",
      "830 ...\n",
      "831 ...\n",
      "832 ...\n",
      "833 ...\n",
      "834 ...\n",
      "835 ...\n",
      "836 ...\n",
      "837 ...\n",
      "838 ...\n",
      "839 ...\n",
      "840 ...\n",
      "841 ...\n",
      "842 ...\n",
      "843 ...\n",
      "844 ...\n",
      "845 ...\n",
      "846 ...\n",
      "847 ...\n",
      "848 ...\n",
      "849 ...\n",
      "850 ...\n",
      "851 ...\n",
      "852 ...\n",
      "853 ...\n",
      "854 ...\n",
      "855 ...\n",
      "856 ...\n",
      "857 ...\n",
      "858 ...\n",
      "859 ...\n",
      "860 ...\n",
      "861 ...\n",
      "862 ...\n",
      "863 ...\n",
      "864 ...\n",
      "865 ...\n",
      "866 ...\n",
      "867 ...\n",
      "868 ...\n",
      "869 ...\n",
      "870 ...\n",
      "871 ...\n",
      "872 ...\n",
      "873 ...\n",
      "874 ...\n",
      "875 ...\n",
      "876 ...\n",
      "877 ...\n",
      "878 ...\n",
      "879 ...\n",
      "880 ...\n",
      "881 ...\n",
      "882 ...\n",
      "883 ...\n",
      "884 ...\n",
      "885 ...\n",
      "886 ...\n",
      "887 ...\n",
      "888 ...\n",
      "889 ...\n",
      "890 ...\n",
      "891 ...\n",
      "892 ...\n",
      "893 ...\n",
      "894 ...\n",
      "895 ...\n",
      "896 ...\n",
      "897 ...\n",
      "898 ...\n",
      "899 ...\n",
      "900 ...\n",
      "901 ...\n",
      "902 ...\n",
      "903 ...\n",
      "904 ...\n",
      "905 ...\n",
      "906 ...\n",
      "907 ...\n",
      "908 ...\n",
      "909 ...\n",
      "910 ...\n",
      "911 ...\n",
      "912 ...\n",
      "913 ...\n",
      "914 ...\n",
      "915 ...\n",
      "916 ...\n",
      "917 ...\n",
      "918 ...\n",
      "919 ...\n",
      "920 ...\n",
      "921 ...\n",
      "922 ...\n",
      "923 ...\n",
      "924 ...\n",
      "925 ...\n",
      "926 ...\n",
      "927 ...\n",
      "928 ...\n",
      "929 ...\n",
      "930 ...\n",
      "931 ...\n",
      "932 ...\n",
      "933 ...\n",
      "934 ...\n",
      "935 ...\n",
      "936 ...\n",
      "937 ...\n",
      "938 ...\n",
      "939 ...\n",
      "940 ...\n",
      "941 ...\n",
      "942 ...\n",
      "943 ...\n",
      "944 ...\n",
      "945 ...\n",
      "946 ...\n",
      "947 ...\n",
      "948 ...\n",
      "949 ...\n",
      "950 ...\n",
      "951 ...\n",
      "952 ...\n",
      "953 ...\n",
      "954 ...\n",
      "955 ...\n",
      "956 ...\n",
      "957 ...\n",
      "958 ...\n",
      "959 ...\n",
      "960 ...\n",
      "961 ...\n",
      "962 ...\n",
      "963 ...\n",
      "964 ...\n",
      "965 ...\n",
      "966 ...\n",
      "967 ...\n",
      "968 ...\n",
      "969 ...\n",
      "970 ...\n",
      "971 ...\n",
      "972 ...\n",
      "973 ...\n",
      "974 ...\n",
      "975 ...\n",
      "976 ...\n",
      "977 ...\n",
      "978 ...\n",
      "979 ...\n",
      "980 ...\n",
      "981 ...\n",
      "982 ...\n",
      "983 ...\n",
      "984 ...\n",
      "985 ...\n",
      "986 ...\n",
      "987 ...\n",
      "988 ...\n",
      "989 ...\n",
      "990 ...\n",
      "991 ...\n",
      "992 ...\n",
      "993 ...\n",
      "994 ...\n",
      "995 ...\n",
      "996 ...\n",
      "997 ...\n",
      "998 ...\n",
      "999 ...\n",
      "1000 ...\n",
      "1001 ...\n",
      "1002 ...\n",
      "1003 ...\n",
      "1004 ...\n",
      "1005 ...\n",
      "1006 ...\n",
      "1007 ...\n",
      "1008 ...\n",
      "1009 ...\n",
      "1010 ...\n",
      "1011 ...\n",
      "1012 ...\n",
      "1013 ...\n",
      "1014 ...\n",
      "1015 ...\n",
      "1016 ...\n",
      "1017 ...\n",
      "1018 ...\n",
      "1019 ...\n",
      "1020 ...\n",
      "1021 ...\n",
      "1022 ...\n",
      "1023 ...\n",
      "1024 ...\n",
      "1025 ...\n",
      "1026 ...\n",
      "1027 ...\n",
      "1028 ...\n",
      "1029 ...\n",
      "1030 ...\n",
      "1031 ...\n",
      "1032 ...\n",
      "1033 ...\n",
      "1034 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1035 ...\n",
      "1036 ...\n",
      "1037 ...\n",
      "1038 ...\n",
      "1039 ...\n",
      "1040 ...\n",
      "1041 ...\n",
      "1042 ...\n",
      "1043 ...\n",
      "1044 ...\n",
      "1045 ...\n",
      "1046 ...\n",
      "1047 ...\n",
      "1048 ...\n",
      "1049 ...\n",
      "1050 ...\n",
      "1051 ...\n",
      "1052 ...\n",
      "1053 ...\n",
      "1054 ...\n",
      "1055 ...\n",
      "1056 ...\n",
      "1057 ...\n",
      "1058 ...\n",
      "1059 ...\n",
      "1060 ...\n",
      "1061 ...\n",
      "1062 ...\n",
      "1063 ...\n",
      "1064 ...\n",
      "1065 ...\n",
      "1066 ...\n",
      "1067 ...\n",
      "1068 ...\n",
      "1069 ...\n",
      "1070 ...\n",
      "1071 ...\n",
      "1072 ...\n",
      "1073 ...\n",
      "1074 ...\n",
      "1075 ...\n",
      "1076 ...\n",
      "1077 ...\n",
      "1078 ...\n",
      "1079 ...\n",
      "1080 ...\n",
      "1081 ...\n",
      "1082 ...\n",
      "1083 ...\n",
      "1084 ...\n",
      "1085 ...\n",
      "1086 ...\n",
      "1087 ...\n",
      "1088 ...\n",
      "1089 ...\n",
      "1090 ...\n",
      "1091 ...\n",
      "1092 ...\n",
      "1093 ...\n",
      "1094 ...\n",
      "1095 ...\n",
      "1096 ...\n",
      "1097 ...\n",
      "1098 ...\n",
      "1099 ...\n",
      "1100 ...\n",
      "1101 ...\n",
      "1102 ...\n",
      "1103 ...\n",
      "1104 ...\n",
      "1105 ...\n",
      "1106 ...\n",
      "1107 ...\n",
      "1108 ...\n",
      "1109 ...\n",
      "1110 ...\n",
      "1111 ...\n",
      "1112 ...\n",
      "1113 ...\n",
      "1114 ...\n",
      "1115 ...\n",
      "1116 ...\n",
      "1117 ...\n",
      "1118 ...\n",
      "1119 ...\n",
      "1120 ...\n",
      "1121 ...\n",
      "1122 ...\n",
      "1123 ...\n",
      "1124 ...\n",
      "1125 ...\n",
      "1126 ...\n",
      "1127 ...\n",
      "1128 ...\n",
      "1129 ...\n",
      "1130 ...\n",
      "1131 ...\n",
      "1132 ...\n",
      "1133 ...\n",
      "1134 ...\n",
      "1135 ...\n",
      "1136 ...\n",
      "1137 ...\n",
      "1138 ...\n",
      "1139 ...\n",
      "1140 ...\n",
      "1141 ...\n",
      "1142 ...\n",
      "1143 ...\n",
      "1144 ...\n",
      "1145 ...\n",
      "1146 ...\n",
      "1147 ...\n",
      "1148 ...\n",
      "1149 ...\n",
      "1150 ...\n",
      "1151 ...\n",
      "1152 ...\n",
      "1153 ...\n",
      "1154 ...\n",
      "1155 ...\n",
      "1156 ...\n",
      "1157 ...\n",
      "1158 ...\n",
      "1159 ...\n",
      "1160 ...\n",
      "1161 ...\n",
      "1162 ...\n",
      "1163 ...\n",
      "1164 ...\n",
      "1165 ...\n",
      "1166 ...\n",
      "1167 ...\n",
      "1168 ...\n",
      "1169 ...\n",
      "1170 ...\n",
      "1171 ...\n",
      "1172 ...\n",
      "1173 ...\n",
      "1174 ...\n",
      "1175 ...\n",
      "1176 ...\n",
      "1177 ...\n",
      "1178 ...\n",
      "1179 ...\n",
      "1180 ...\n",
      "1181 ...\n",
      "1182 ...\n",
      "1183 ...\n",
      "1184 ...\n",
      "1185 ...\n",
      "1186 ...\n",
      "1187 ...\n",
      "1188 ...\n",
      "1189 ...\n",
      "1190 ...\n",
      "1191 ...\n",
      "1192 ...\n",
      "1193 ...\n",
      "1194 ...\n",
      "1195 ...\n",
      "1196 ...\n",
      "1197 ...\n",
      "1198 ...\n",
      "1199 ...\n",
      "1200 ...\n"
     ]
    }
   ],
   "source": [
    "DATA_val_s = []\n",
    "DATA_val_s_cmplx = []\n",
    "DATA_val_n = []\n",
    "DATA_val_x_cmplx = []\n",
    "DATA_val_x = []\n",
    "\n",
    "with open(VAL_S,'wb') as fs, open(VAL_N,'wb') as fn, open(VAL_X_cmplx,'wb') as fx_cmplx, open(VAL_X,'wb') as fx, open(VAL_S_cmplx,'wb') as fs_cmplx: \n",
    "    count = 1\n",
    "    for file_s, file_n, file_x in zip(sorted(glob.glob(PATH_directory+PATH_val+CLEAN_format_val)),sorted(glob.glob(PATH_directory+PATH_val+NOISE_format_val)),sorted(glob.glob(PATH_directory+PATH_val+MIX_format_val))):\n",
    "        s_cmplx,s = preprossed_data(file_s)\n",
    "        DATA_val_s.append(np.array(s))\n",
    "        np.savetxt(fs, s, fmt='%.5f')\n",
    "        fs.write(b'\\n')\n",
    "        \n",
    "        DATA_val_s_cmplx.append(np.array(s_cmplx))\n",
    "        np.savetxt(fs_cmplx, s_cmplx, fmt='%.5f')\n",
    "        fs_cmplx.write(b'\\n')\n",
    "        \n",
    "        _,n = preprossed_data(file_n)\n",
    "        DATA_val_n.append(np.array(n))\n",
    "        np.savetxt(fn, n, fmt='%.5f')\n",
    "        fn.write(b'\\n')\n",
    "        \n",
    "        x_cmplx,x = preprossed_data(file_x)\n",
    "        DATA_val_x.append(np.array(x))\n",
    "        np.savetxt(fx, x, fmt='%.5f')\n",
    "        fx.write(b'\\n')\n",
    "        \n",
    "        DATA_val_x_cmplx.append(np.array(x_cmplx))\n",
    "        np.savetxt(fx_cmplx, x_cmplx, fmt='%.5f')\n",
    "        fx_cmplx.write(b'\\n')\n",
    "        \n",
    "        print(count, '...')\n",
    "        count += 1\n",
    "        \n",
    "fs.close()\n",
    "fs_cmplx.close()\n",
    "fn.close()\n",
    "fx_cmplx.close()\n",
    "fx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Test Data\n",
    "\n",
    "Here we open the files, and read the Test Dataset. To construct Test output files, we need to run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_test_x_cmplx = []\n",
    "DATA_test_x = []\n",
    "\n",
    "with open(TEST_S,'wb') as fs, open(TEST_N,'wb') as fn, open(TEST_X_cmplx,'wb') as fx_cmplx, open(TEST_X,'wb') as fx: \n",
    "    count = 1\n",
    "    for file_x in sorted(glob.glob(PATH_directory+PATH_test+MIX_format_test)):\n",
    "        x_cmplx,x = preprossed_data(file_x)\n",
    "        DATA_test_x.append(np.array(x))\n",
    "        np.savetxt(fx, x, fmt='%.5f')\n",
    "        fx.write(b'\\n')\n",
    "        \n",
    "        DATA_test_x_cmplx.append(np.array(x_cmplx))\n",
    "        np.savetxt(fx_cmplx, x_cmplx, fmt='%.5f')\n",
    "        fx_cmplx.write(b'\\n')\n",
    "        \n",
    "        print(count,'...')\n",
    "        count += 1\n",
    "        \n",
    "fx_cmplx.close()\n",
    "fx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Function to load data from files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_name):\n",
    "    \n",
    "    with open(file_name) as f:\n",
    "        lines=f.readlines()\n",
    "        print(len(lines))\n",
    "        sentence_full=[]\n",
    "        count = 0\n",
    "        sentence=[]\n",
    "        for line in lines:\n",
    "\n",
    "            if count < 513:\n",
    "                if count ==0:\n",
    "                    sentence=np.array(np.fromstring(line, dtype=float, sep=' '), ndmin=2)\n",
    "                    count+=1\n",
    "                else:\n",
    "                    myarray = np.array(np.fromstring(line, dtype=float, sep=' '), ndmin=2)\n",
    "                    sentence=np.concatenate((sentence, myarray), axis=0)\n",
    "                    count+=1\n",
    "            else:\n",
    "                sentence_full.append(sentence) \n",
    "                count=0\n",
    "                sentence=[]\n",
    "                \n",
    "        return sentence_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Train files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616800\n",
      "616800\n",
      "616800\n",
      "616800\n"
     ]
    }
   ],
   "source": [
    "DATA_train_s = load_file(TRAIN_S)\n",
    "DATA_train_n = load_file(TRAIN_N)\n",
    "DATA_train_x_cmplx = load_file(TRAIN_X_cmplx)\n",
    "DATA_train_x = load_file(TRAIN_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Validation files. Do not run this if you want to calculate SNR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA_val_s = load_file(VAL_S)\n",
    "DATA_val_n = load_file(VAL_N)\n",
    "DATA_val_x_cmplx = load_file(VAL_X_cmplx)\n",
    "DATA_val_x = load_file(VAL_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Test files. Do not run this if you want to creat wav output files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA_test_x_cmplx = load_file(TEST_X_cmplx)\n",
    "DATA_test_x = load_file(TEST_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Labels (IRM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_train_M=[ (1.0*(DATA_train_s[i]>DATA_train_n[i])) for i in range(len(DATA_train_s)) ]\n",
    "\n",
    "DATA_val_M=[ (1.0*(DATA_val_s[i]>DATA_val_n[i])) for i in range(len(DATA_val_s)) ]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Generator with Mini-Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch_mb(X_,Y_,mini_batch):\n",
    "    \n",
    "    batch_x, batch_y = None, None\n",
    "    \n",
    "    for e,(x,y) in enumerate(zip(X_,Y_)):\n",
    "        \n",
    "        batch_x = np.array(x.T) if batch_x is None else np.concatenate( (batch_x,x.T), axis=0)\n",
    "        batch_y = np.array(y.T) if batch_y is None else np.concatenate( (batch_y,y.T), axis=0)\n",
    "        \n",
    "        if e>0 and (e+1)%10==0:\n",
    "\n",
    "            batch_x, temp_x = None, batch_x\n",
    "            batch_y, temp_y = None, batch_y\n",
    "            \n",
    "            temp_x = temp_x.reshape((-1,Max_RNN,513))\n",
    "            temp_y = temp_y.reshape((-1,Max_RNN,513))\n",
    "            \n",
    "            total_mini_batch = temp_x.shape[0]//mini_batch\n",
    "     \n",
    "            for mb in range(total_mini_batch):\n",
    "                start_b = (mb*mini_batch)\n",
    "                end_b = ((mb+1)*mini_batch)\n",
    "\n",
    "                yield temp_x[start_b:end_b],temp_y[start_b:end_b]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(X_,Y_):\n",
    "    \n",
    "    batch_x, batch_y = None, None\n",
    "    \n",
    "    for e,(x,y) in enumerate(zip(X_,Y_)):\n",
    "        \n",
    "        batch_x = np.array(x.T) if batch_x is None else np.concatenate( (batch_x,x.T), axis=0)\n",
    "        batch_y = np.array(y.T) if batch_y is None else np.concatenate( (batch_y,y.T), axis=0)\n",
    "\n",
    "        if e>0 and (e+1)%10==0:\n",
    "            temp_x, batch_x = batch_x, None\n",
    "            temp_y, batch_y = batch_y, None\n",
    "            \n",
    "            temp_x = temp_x.reshape((-1,Max_RNN,513))\n",
    "            temp_y = temp_y.reshape((-1,Max_RNN,513))\n",
    "        \n",
    "            yield temp_x,temp_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Single Layer RNN (GRU)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(GRU(Max_RNN,input_shape=(Max_RNN,513), return_sequences=True))\n",
    "\n",
    "model.add(Dense(513, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit_generator( next_batch(DATA_train_x, DATA_train_M), epochs=20, steps_per_epoch=120, validation_data=next_batch(DATA_val_x, DATA_val_M), validation_steps=120, shuffle=True)\n",
    "\n",
    "# Final evaluation of the model\n",
    "\n",
    "scores = model.evaluate_generator(next_batch(DATA_val_x, DATA_val_M), verbose=0)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Single Layer RNN (GRU) with Mini-batch (10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(GRU(Max_RNN,input_shape=(Max_RNN,513), return_sequences=True))\n",
    "\n",
    "model.add(Dense(513, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit_generator( next_batch_mb(DATA_train_x, DATA_train_M,10), epochs=20, steps_per_epoch=700, validation_data=next_batch_mb(DATA_val_x, DATA_val_M,10), validation_steps=700)\n",
    "\n",
    "# Final evaluation of the model\n",
    "\n",
    "scores = model.evaluate_generator(next_batch(DATA_val_x, DATA_val_M), verbose=0)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Two layer Bi-directional RNN (GRU), droupout=0.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.01%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Bidirectional(GRU(Max_RNN, return_sequences=True), input_shape=(Max_RNN,513)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(GRU(Max_RNN, return_sequences=True)))\n",
    "\n",
    "model.add(Dense(513, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "for e in range(5):\n",
    "    for (b_x,b_y), (v_x,v_y) in zip(next_batch(DATA_train_x, DATA_train_M), next_batch(DATA_val_x, DATA_val_M)):\n",
    "        model.fit(b_x, b_y, validation_data=(v_x,v_y), shuffle=True, batch_size=256)\n",
    "\n",
    "        \n",
    "# Final evaluation of the model\n",
    "scores = []\n",
    "for v_x,v_y in next_batch(DATA_val_x, DATA_val_x):\n",
    "    scores.append( model.evaluate(v_x, v_y,verbose=0)[1] )\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Generator for SNR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch_SXX_cmplxS_cmplx(S_,X_,X_cmplx_,S_cmplx_):\n",
    "    \n",
    "    batch_s = None\n",
    "    batch_x = None\n",
    "    batch_x_cmplx = None\n",
    "    batch_s_cmplx = None\n",
    "    \n",
    "    for e,(s,x,x_cmplx,s_cmplx) in enumerate( zip(S_,X_,X_cmplx_,S_cmplx_)): \n",
    "        batch_s = np.array(s.T) if batch_s is None else np.concatenate( (batch_s,s.T), axis=0)\n",
    "        batch_x = np.array(x.T) if batch_x is None else np.concatenate( (batch_x,x.T), axis=0)\n",
    "        batch_x_cmplx = np.array(x_cmplx.T) if batch_x_cmplx is None else np.concatenate( (batch_x_cmplx,x_cmplx.T), axis=0)\n",
    "        batch_s_cmplx = np.array(s_cmplx.T) if batch_s_cmplx is None else np.concatenate( (batch_s_cmplx,s_cmplx.T), axis=0)\n",
    "        \n",
    "        \n",
    "        if e>0 and (e+1)%10==0:\n",
    "            temp_s, batch_s = batch_s, None\n",
    "            temp_x, batch_x = batch_x, None\n",
    "            temp_x_cmplx, batch_x_cmplx = batch_x_cmplx, None\n",
    "            temp_s_cmplx, batch_s_cmplx = batch_s_cmplx, None\n",
    "            \n",
    "            temp_s = temp_s.reshape((-1,Max_RNN,513))\n",
    "            temp_x = temp_x.reshape((-1,Max_RNN,513))\n",
    "            temp_x_cmplx = temp_x_cmplx.reshape((-1,Max_RNN,513))\n",
    "\n",
    "            yield temp_s,temp_x,temp_x_cmplx,temp_s_cmplx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_s = 0.0\n",
    "sum_s_diff = 0.0\n",
    "\n",
    "\n",
    "for v_s,v_x,v_x_cmplx,v_s_cmplx in next_batch_SXX_cmplxS_cmplx(DATA_val_s,DATA_val_x,DATA_val_x_cmplx,DATA_val_s_cmplx):\n",
    "    \n",
    "    mask = model.predict(v_x)\n",
    "    S_hat = (mask) * v_x_cmplx\n",
    "    S_hat = S_hat.reshape(-1,513).T\n",
    "    S = v_s_cmplx.T\n",
    "\n",
    "    S_org = librosa.istft(S, hop_length=512)\n",
    "    S_pred = librosa.istft(S_hat, hop_length=512)\n",
    "\n",
    "    sum_s += np.sum(S_org*S_org)\n",
    "    sum_s_diff += np.sum((S_org-S_pred)*(S_org-S_pred))\n",
    "    \n",
    "acc = sum_s/ sum_s_diff\n",
    "print( 'SNR:', 10*np.log10(acc) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_audio():\n",
    "    mags = None\n",
    "    cmplxs = None\n",
    "    \n",
    "    for e, file_x in enumerate(fname_test[:10]):\n",
    "        print(e)\n",
    "        sn, sr = librosa.load(file_x, sr=None)\n",
    "        Sn = librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "        mag_Sn=np.abs(Sn)\n",
    "        \n",
    "        mags = np.array(mag_Sn.T) if mags is None else np.concatenate( (mags,mag_Sn.T), axis=0)\n",
    "        cmplxs = np.array(Sn.T) if cmplxs is None else np.concatenate( (cmplxs,Sn.T), axis=0)\n",
    "        \n",
    "        if e>0 and (e+1)%10==0:\n",
    "            temp, mags = mags, None\n",
    "            \n",
    "            temp = temp.reshape((-1,Max_RNN,513))\n",
    "            mask = model.predict(temp)\n",
    "            \n",
    "            mask=mask.reshape(-1,513)\n",
    "            S_hat = (mask) * cmplxs\n",
    "            S_hat = S_hat.T\n",
    "            \n",
    "            lenght_w = S_hat.shape[1]//10\n",
    "            for clip in range(10):\n",
    "                start_w = clip*lenght_w\n",
    "                end_w = (clip+1)*lenght_w\n",
    "                \n",
    "                wav = S_hat[:,start_w:end_w].T\n",
    "                S_time=librosa.istft(wav, hop_length=512)\n",
    "                fname = PATH_directory+PATH_denoise+ e + \"_redoise.wav\"\n",
    "                \n",
    "                print(audio_fname)\n",
    "                librosa.output.write_wav(fname, S_time, sr)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('DATA_val_sX.npy', DATA_val_sX)\n",
    "np.save('DATA_val_xX.npy', DATA_val_xX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_test_x (513, 44150)\n",
      "DATA_test_x (513, 5, 8830)\n"
     ]
    }
   ],
   "source": [
    "DATA_val_sX_f = np.load('DATA_val_sX.npy')\n",
    "DATA_val_xX_f = np.load('DATA_val_xX.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Dataset in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DATA_train_s' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-54af169ce951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DATA_train_s.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_train_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DATA_train_n.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_train_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DATA_train_x.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_train_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DATA_train_M.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_train_M\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DATA_train_s' is not defined"
     ]
    }
   ],
   "source": [
    "np.save('DATA_train_s.npy', DATA_train_s)\n",
    "np.save('DATA_train_n.npy', DATA_train_n)\n",
    "np.save('DATA_train_x.npy', DATA_train_x)\n",
    "np.save('DATA_train_M.npy', DATA_train_M)\n",
    "\n",
    "\n",
    "np.save('DATA_val_s.npy', DATA_val_s)\n",
    "np.save('DATA_val_n.npy', DATA_val_n)\n",
    "np.save('DATA_val_x.npy', DATA_val_x)\n",
    "np.save('DATA_val_M.npy', DATA_val_M)\n",
    "\n",
    "np.save('DATA_test_x.npy', DATA_test_x)\n",
    "\n",
    "np.save('DATA_val_sX.npy', DATA_val_sX)\n",
    "np.save('DATA_val_xX.npy', DATA_val_xX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_train_s_f = np.load('DATA_train_s.npy')\n",
    "DATA_train_n_f = np.load('DATA_train_n.npy')\n",
    "DATA_train_x_f = np.load('DATA_train_x.npy')\n",
    "DATA_train_M_f = np.load('DATA_train_M.npy')\n",
    "\n",
    "DATA_val_s_f = np.load('DATA_val_s.npy')\n",
    "DATA_val_n_f = np.load('DATA_val_n.npy')\n",
    "DATA_val_x_f = np.load('DATA_val_x.npy')\n",
    "DATA_val_M_f = np.load('DATA_val_M.npy')\n",
    "\n",
    "DATA_test_x_f = np.load('DATA_test_x.npy')\n",
    "\n",
    "DATA_val_sX_f = np.load('DATA_val_sX.npy')\n",
    "DATA_val_xX_f = np.load('DATA_val_xX.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_train_s_f (513, 5, 23710)\n",
      "DATA_train_M_f (513, 5, 23710)\n",
      "DATA_val_x_f (513, 5, 23710)\n",
      "DATA_test_x_f (513, 5, 8830)\n",
      "DATA_val_xX_f (513, 5, 23710)\n"
     ]
    }
   ],
   "source": [
    "print('DATA_train_s_f', DATA_train_s_f.shape)\n",
    "print('DATA_train_M_f', DATA_train_M_f.shape)\n",
    "print('DATA_val_x_f', DATA_val_x_f.shape)\n",
    "print('DATA_test_x_f', DATA_test_x_f.shape)\n",
    "print('DATA_val_xX_f', DATA_val_xX_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_train_s_f (513, 118550)\n",
      "DATA_train_s_f (118550, 513)\n",
      "DATA_train_s_f (23710, 5, 513)\n"
     ]
    }
   ],
   "source": [
    "DATA_train_s_f = DATA_train_s_f.reshape(513,-1)\n",
    "print('DATA_train_s_f', DATA_train_s_f.shape)\n",
    "\n",
    "DATA_train_s_f = DATA_train_s_f.T\n",
    "print('DATA_train_s_f', DATA_train_s_f.shape)\n",
    "\n",
    "DATA_train_s_f = DATA_train_s_f.reshape(-1,5,513)\n",
    "print('DATA_train_s_f', DATA_train_s_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_train_n_f = (DATA_train_n_f.reshape(513,-1).T).reshape(-1,5,513)\n",
    "DATA_train_x_f = (DATA_train_x_f.reshape(513,-1).T).reshape(-1,5,513)\n",
    "DATA_train_M_f = (DATA_train_M_f.reshape(513,-1).T).reshape(-1,5,513)\n",
    "\n",
    "DATA_val_s_f = (DATA_val_s_f.reshape(513,-1).T).reshape(-1,5,513)\n",
    "DATA_val_n_f = (DATA_val_n_f.reshape(513,-1).T).reshape(-1,5,513)\n",
    "DATA_val_x_f = (DATA_val_x_f.reshape(513,-1).T).reshape(-1,5,513)\n",
    "DATA_val_M_f = (DATA_val_M_f.reshape(513,-1).T).reshape(-1,5,513)\n",
    "\n",
    "DATA_test_x_f = (DATA_test_x_f.reshape(513,-1).T).reshape(-1,5,513)\n",
    "\n",
    "DATA_val_sX_f = (DATA_val_sX_f.reshape(513,-1).T).reshape(-1,5,513)\n",
    "DATA_val_xX_f = (DATA_val_xX_f.reshape(513,-1).T).reshape(-1,5,513)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_train_s_f (23710, 5, 513)\n",
      "DATA_train_M_f (23710, 5, 513)\n",
      "DATA_val_x_f (23710, 5, 513)\n",
      "DATA_test_x_f (8830, 5, 513)\n",
      "DATA_val_xX_f (23710, 5, 513)\n"
     ]
    }
   ],
   "source": [
    "print('DATA_train_s_f', DATA_train_s_f.shape)\n",
    "print('DATA_train_M_f', DATA_train_M_f.shape)\n",
    "print('DATA_val_x_f', DATA_val_x_f.shape)\n",
    "print('DATA_test_x_f', DATA_test_x_f.shape)\n",
    "print('DATA_val_xX_f', DATA_val_xX_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23710, 5, 513, 2)\n",
      "(23710, 5, 513, 2)\n"
     ]
    }
   ],
   "source": [
    "tr_categorical_labels = to_categorical(DATA_train_M_f, num_classes=2)\n",
    "print(tr_categorical_labels.shape)\n",
    "\n",
    "v_categorical_labels = to_categorical(DATA_val_M_f, num_classes=2)\n",
    "print(v_categorical_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_RNN = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 5, 10)             15570     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 10)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5, 513)            5643      \n",
      "=================================================================\n",
      "Total params: 21,213\n",
      "Trainable params: 21,213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/Anaconda3-5.0.1/envs/e533/lib/python3.6/site-packages/keras/models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23710 samples, validate on 23710 samples\n",
      "Epoch 1/10\n",
      "23710/23710 [==============================] - 55s 2ms/step - loss: 0.5898 - acc: 0.6839 - val_loss: 0.5429 - val_acc: 0.7291\n",
      "Epoch 2/10\n",
      "23710/23710 [==============================] - 52s 2ms/step - loss: 0.5468 - acc: 0.7229 - val_loss: 0.5234 - val_acc: 0.7439\n",
      "Epoch 3/10\n",
      "23710/23710 [==============================] - 52s 2ms/step - loss: 0.5346 - acc: 0.7325 - val_loss: 0.5102 - val_acc: 0.7529\n",
      "Epoch 4/10\n",
      "23710/23710 [==============================] - 53s 2ms/step - loss: 0.5267 - acc: 0.7385 - val_loss: 0.5061 - val_acc: 0.7552\n",
      "Epoch 5/10\n",
      "23710/23710 [==============================] - 53s 2ms/step - loss: 0.5212 - acc: 0.7429 - val_loss: 0.5035 - val_acc: 0.7576\n",
      "Epoch 6/10\n",
      "23710/23710 [==============================] - 52s 2ms/step - loss: 0.5175 - acc: 0.7459 - val_loss: 0.4956 - val_acc: 0.7625\n",
      "Epoch 7/10\n",
      "23710/23710 [==============================] - 52s 2ms/step - loss: 0.5145 - acc: 0.7477 - val_loss: 0.4921 - val_acc: 0.7653\n",
      "Epoch 8/10\n",
      "23710/23710 [==============================] - 52s 2ms/step - loss: 0.5125 - acc: 0.7493 - val_loss: 0.4954 - val_acc: 0.7636\n",
      "Epoch 9/10\n",
      "23710/23710 [==============================] - 52s 2ms/step - loss: 0.5103 - acc: 0.7507 - val_loss: 0.4912 - val_acc: 0.7656\n",
      "Epoch 10/10\n",
      "23710/23710 [==============================] - 89s 4ms/step - loss: 0.5079 - acc: 0.7526 - val_loss: 0.4871 - val_acc: 0.7687\n",
      "Accuracy: 76.87%\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add( Bidirectional(GRU(Max_RNN, return_sequences=True), input_shape=(Max_RNN,513) ))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(513, activation='sigmoid'))\n",
    "          \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# model.fit(DATA_train_x_f.T, tr_categorical_labels, validation_data=(DATA_val_x_f.T,v_categorical_labels), shuffle=True, nb_epoch=100, batch_size=10)\n",
    "# # Final evaluation of the model\n",
    "# scores = model.evaluate(DATA_val_x_f.T, v_categorical_labels, verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "model.fit(DATA_train_x_f, DATA_train_M_f, validation_data=(DATA_val_x_f,DATA_val_M_f), shuffle=True, nb_epoch=10, batch_size=10)\n",
    "\n",
    "# model.fit(DATA_train_x_f, tr_categorical_labels, validation_data=(DATA_val_x_f,v_categorical_labels), shuffle=True, nb_epoch=10, batch_size=10)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(DATA_val_x_f, DATA_val_M_f, verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/Anaconda3-5.0.1/envs/e533/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `GRU` call to the Keras 2 API: `GRU(input_shape=(5, 513), return_sequences=True, units=513)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 5, 513)            1580553   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5, 513)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 5, 513)            263682    \n",
      "=================================================================\n",
      "Total params: 1,844,235\n",
      "Trainable params: 1,844,235\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 23710 samples, validate on 23710 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(GRU(output_dim = 513, input_shape=(Max_RNN,513), return_sequences=True))\n",
    "# model.add(GRU(output_dim = 513, input_length = 5, input_dim = 513, return_sequences=True))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "model.add(TimeDistributed(Dense(513, activation='sigmoid')))\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(DATA_train_x_f, DATA_train_M_f, validation_data=(DATA_val_x_f,DATA_val_M_f), shuffle=True, epochs=10, batch_size=10)\n",
    "\n",
    "# model.fit(DATA_train_x_f, tr_categorical_labels, validation_data=(DATA_val_x_f,v_categorical_labels), shuffle=True, nb_epoch=10, batch_size=10)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(DATA_val_x_f, DATA_val_M_f, verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprossed_dataX(file_name):\n",
    "    \n",
    "    sn, sr=librosa.load(file_name, sr=None)\n",
    "    X=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_val_sX = None\n",
    "DATA_val_xX = None\n",
    "\n",
    "for file_s, file_x in zip( sorted(glob.glob(PATH_directory+PATH_val+CLEAN_format_val)), sorted(glob.glob(PATH_directory+PATH_val+MIX_format_val))):\n",
    "    \n",
    "    DATA_val_sX = np.array(preprossed_dataX(file_s)) if DATA_val_sX is None else np.concatenate( (DATA_val_sX,preprossed_dataX(file_s)),axis=1)\n",
    "    DATA_val_xX = np.array(preprossed_dataX(file_x)) if DATA_val_xX is None else np.concatenate( (DATA_val_xX,preprossed_dataX(file_x)),axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_val_xX (513, 118550)\n",
      "DATA_val_xX (513, 5, 23710)\n"
     ]
    }
   ],
   "source": [
    "print('DATA_val_sX',DATA_val_sX.shape)\n",
    "print('DATA_val_xX',DATA_val_xX.shape)\n",
    "\n",
    "DATA_val_sX = DATA_val_sX.reshape( (513,5,-1))\n",
    "DATA_val_xX = DATA_val_xX.reshape( (513,5,-1))\n",
    "\n",
    "print('DATA_val_sX',DATA_val_sX.shape)\n",
    "print('DATA_val_xX',DATA_val_xX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('DATA_val_sX.npy', DATA_val_sX)\n",
    "np.save('DATA_val_xX.npy', DATA_val_xX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_val_sX_f = np.load('DATA_val_sX.npy')\n",
    "DATA_val_xX_f = np.load('DATA_val_xX.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model : the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 400 arrays: [array([[0.31088, 0.00439, 0.02531, ..., 0.03555, 0.05921, 0.02048],\n       [0.20403, 0.21771, 0.24856, ..., 0.07964, 0.19597, 0.11246],\n       [0.00054, 0.27443, 0.5295 , ..., 0.14783, 0.20548, 0.119...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-255f5ea9f2e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_test_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/Anaconda3-5.0.1/envs/e533/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[0;32m-> 1025\u001b[0;31m                                   steps=steps)\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/Anaconda3-5.0.1/envs/e533/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1822\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1823\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1824\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1825\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/Anaconda3-5.0.1/envs/e533/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model : the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 400 arrays: [array([[0.31088, 0.00439, 0.02531, ..., 0.03555, 0.05921, 0.02048],\n       [0.20403, 0.21771, 0.24856, ..., 0.07964, 0.19597, 0.11246],\n       [0.00054, 0.27443, 0.5295 , ..., 0.14783, 0.20548, 0.119..."
     ]
    }
   ],
   "source": [
    "labels = model.predict(DATA_test_x)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_hat = (labels) * DATA_val_xX_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513, 118550)\n",
      "(513, 118550)\n"
     ]
    }
   ],
   "source": [
    "# S_org = (DATA_val_s_f).flatten()\n",
    "# S_pred = S_hat.flatten()\n",
    "st = DATA_val_sX_f.reshape(513,-1)\n",
    "st_hat = S_hat.reshape(513,-1)\n",
    "\n",
    "print(st.shape)\n",
    "print(st_hat.shape)\n",
    "\n",
    "S_org = librosa.istft(st, hop_length=512)\n",
    "S_pred = librosa.istft(st_hat, hop_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60697088,) (60697088,)\n"
     ]
    }
   ],
   "source": [
    "print(S_pred.shape, S_org.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1112037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.139689683914185"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = np.sum(S_org*S_org)/ np.sum((S_org-S_pred)*(S_org-S_pred))\n",
    "print(acc)\n",
    "\n",
    "10*np.log10(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-09b5a3680f5d>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-09b5a3680f5d>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    sn, sr=librosa.load('train_dirty_male.wav', sr=None)\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Noisy -> Input Data\n",
    "sn, sr=librosa.load('train_dirty_male.wav', sr=None)\n",
    "X=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "X_mag = np.abs(X)\n",
    "\n",
    "# Clean -> Label\n",
    "s, sr=librosa.load('train_clean_male.wav', sr=None)\n",
    "S=librosa.stft(s, n_fft=1024, hop_length=512)\n",
    "S_mag = np.abs(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for generating next batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(X,Y, batch_size):\n",
    "    num_samples, _ = X.shape\n",
    "    \n",
    "    selected_indics = np.random.randint(num_samples-batch_size)\n",
    "#     print(selected_indics)\n",
    "    return X[selected_indics:selected_indics+batch_size], Y[selected_indics:selected_indics+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITERATION = 1000\n",
    "BATCH_SIZE = X_mag.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier Initialization of Weights\n",
    "These are the weight initialization function used in defining model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable (shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = np.sqrt(2.0/sum(shape)) )\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable (shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = np.sqrt(1.0/sum(shape)) )\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the fully connected model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 513]) \n",
    "\n",
    "W_1 = weight_variable([513, 1024])\n",
    "b_1 = bias_variable([1024])\n",
    "\n",
    "W_2 = weight_variable([1024, 1024])\n",
    "b_2 = bias_variable([1024])\n",
    "\n",
    "W_3 = weight_variable([1024, 1024])\n",
    "b_3 = bias_variable([1024])\n",
    "\n",
    "W_4 = weight_variable([1024, 1024])\n",
    "b_4 = bias_variable([1024])\n",
    "\n",
    "W_5 = weight_variable([1024, 513])\n",
    "b_5 = bias_variable([513])\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, [None, 513]) # original\n",
    "\n",
    "\n",
    "# Layer connections and Activation functions\n",
    "y_1 = tf.nn.relu(tf.matmul(x, W_1) + b_1)\n",
    "y_2 = tf.nn.relu(tf.matmul(y_1, W_2) + b_2)\n",
    "y_3 = tf.nn.relu(tf.matmul(y_2, W_3) + b_3)\n",
    "y_4 = tf.nn.relu(tf.matmul(y_3, W_4) + b_4)\n",
    "y =  tf.nn.relu(tf.matmul(y_2, W_5) + b_5) # predicted\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "mse = tf.reduce_sum( tf.losses.mean_squared_error(labels=y_, predictions=y) )\n",
    "train_step = tf.train.AdamOptimizer().minimize(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model\n",
    "We try different batch sizes, but for whole input dimension batch size gives better result and it removes the need of batch normalization. Since the input dimension is not very large, we can do this. For larger input dimensions, we have to adopt mini batch techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration to control GPU use\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.33\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "\n",
    "# Train Model\n",
    "for _ in range(NUM_ITERATION):\n",
    "    for _ in range(((X_mag.T).shape[0]//BATCH_SIZE)):\n",
    "        batch_xs, batch_ys = next_batch(X_mag.T,S_mag.T, BATCH_SIZE)\n",
    "        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513, 142)\n"
     ]
    }
   ],
   "source": [
    "# Load Test data-1\n",
    "sn, sr=librosa.load('test_x_01.wav', sr=None)\n",
    "X_test_01=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "X_mag_test_01 = np.abs(X_test_01)\n",
    "\n",
    "# Load Test data-2\n",
    "sn, sr=librosa.load('test_x_02.wav', sr=None)\n",
    "X_test_02=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "X_mag_test_02 = np.abs(X_test_02)\n",
    "\n",
    "print(X_mag_test_01.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Test Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model-1\n",
    "S_hat_mag_test_01=sess.run(y, feed_dict={x: X_mag_test_01.T})\n",
    "S_hat_test_01=(X_test_01/X_mag_test_01)*S_hat_mag_test_01.T\n",
    "S_hat_01=librosa.istft(S_hat_test_01, hop_length=512)\n",
    "librosa.output.write_wav('test_s_01_recons.wav', S_hat_01, sr)\n",
    "\n",
    "# Test model-2\n",
    "S_hat_mag_test_02=sess.run(y, feed_dict={x: X_mag_test_02.T})\n",
    "S_hat_test_02=(X_test_02/X_mag_test_02)*S_hat_mag_test_02.T\n",
    "S_hat_02=librosa.istft(S_hat_test_02, hop_length=512)\n",
    "librosa.output.write_wav('test_s_02_recons.wav', S_hat_02, sr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
