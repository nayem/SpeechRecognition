{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hw-2: Task 2\n",
    "### Taslima Akter\n",
    "### ID: takter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter\n",
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Dropout, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file names for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/e533/timit-homework/tr/trx0100.wav'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### File names for Training Data\n",
    "\n",
    "import glob\n",
    "import librosa\n",
    "fname_trn=[]\n",
    "for filename in glob.glob('/opt/e533/timit-homework/tr/trn*.wav'):\n",
    "    (fname_trn.append(filename))\n",
    "fname_trn.sort()\n",
    "# fname_trn\n",
    "\n",
    "fname_trs=[]\n",
    "for filename in glob.glob('/opt/e533/timit-homework/tr/trs*.wav'):\n",
    "    (fname_trs.append(filename))\n",
    "fname_trs.sort()\n",
    "# fname_trs\n",
    "\n",
    "fname_trx=[]\n",
    "for filename in glob.glob('/opt/e533/timit-homework/tr/trx*.wav'):\n",
    "    (fname_trx.append(filename))\n",
    "fname_trx.sort()\n",
    "fname_trx[100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file names for validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### File names for validation Data\n",
    "\n",
    "fname_val_n=[]\n",
    "for filename in glob.glob('/opt/e533/timit-homework/v/vn*.wav'):\n",
    "    (fname_val_n.append(filename))\n",
    "fname_val_n.sort()\n",
    "# print(fname_val_n)\n",
    "\n",
    "fname_val_s=[]\n",
    "for filename in glob.glob('/opt/e533/timit-homework/v/vs*.wav'):\n",
    "    (fname_val_s.append(filename))\n",
    "fname_val_s.sort()\n",
    "# fname_trs\n",
    "\n",
    "fname_val_x=[]\n",
    "for filename in glob.glob('/opt/e533/timit-homework/v/vx*.wav'):\n",
    "    (fname_val_x.append(filename))\n",
    "fname_val_x.sort()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file names for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### File names for Test Data\n",
    "\n",
    "fname_test=[]\n",
    "for filename in glob.glob('/opt/e533/timit-homework/te/tex*.wav'):\n",
    "    (fname_test.append(filename))\n",
    "fname_test.sort()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write data into txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function For writing file\n",
    "import librosa\n",
    "def write_file(file_name, fname_list):\n",
    "    ### Writing training data S\n",
    "\n",
    "    with open(file_name, 'wb') as fs:\n",
    "        for i in range(len(fname_list)):\n",
    "            sn, sr=librosa.load(fname_list[i], sr=None)\n",
    "            Sn=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "            mag_Sn=np.abs(Sn)\n",
    "    #         trn_arr=np.concatenate((trn_arr, mag_Sn), axis=1)\n",
    "            np.savetxt(fs, mag_Sn, fmt='%.5f')\n",
    "            fs.write(b'\\n')\n",
    "    fs.close()            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing X into file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function For writing just X into file\n",
    "import librosa\n",
    "def write_file_X(file_name, fname_list):\n",
    "    ### Writing training data S\n",
    "\n",
    "    with open(file_name, 'wb') as fs:\n",
    "        for i in range(len(fname_list)):\n",
    "            sn, sr=librosa.load(fname_list[i], sr=None)\n",
    "            Sn=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "#             mag_Sn=np.abs(Sn)\n",
    "    #         trn_arr=np.concatenate((trn_arr, mag_Sn), axis=1)\n",
    "            np.savetxt(fs, Sn, fmt='%.5f')\n",
    "            fs.write(b'\\n')\n",
    "    fs.close()            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n"
     ]
    }
   ],
   "source": [
    "### Writing training data S\n",
    "Train_complx_X=[]\n",
    "\n",
    "for i in range(len(fname_trx)):\n",
    "    print(i)\n",
    "    \n",
    "    sn, sr=librosa.load(fname_trx[i], sr=None)\n",
    "    Sn=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "    Train_complx_X.append(np.array(Sn))\n",
    "\n",
    "\n",
    "#             mag_Sn=np.abs(Sn)\n",
    "#         trn_arr=np.concatenate((trn_arr, mag_Sn), axis=1)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n"
     ]
    }
   ],
   "source": [
    "### Writing training data S\n",
    "val_complx_X=[]\n",
    "\n",
    "for i in range(len(fname_val_x)):\n",
    "    print(i)\n",
    "    \n",
    "    sn, sr=librosa.load(fname_val_x[i], sr=None)\n",
    "    Sn=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "    val_complx_X.append(np.array(Sn))\n",
    "\n",
    "\n",
    "#             mag_Sn=np.abs(Sn)\n",
    "#         trn_arr=np.concatenate((trn_arr, mag_Sn), axis=1)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n"
     ]
    }
   ],
   "source": [
    "### Writing training data S\n",
    "val_complx_S=[]\n",
    "\n",
    "for i in range(len(fname_val_s)):\n",
    "    print(i)\n",
    "    \n",
    "    sn, sr=librosa.load(fname_val_s[i], sr=None)\n",
    "    Sn=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "    val_complx_S.append(np.array(Sn))\n",
    "\n",
    "\n",
    "#             mag_Sn=np.abs(Sn)\n",
    "#         trn_arr=np.concatenate((trn_arr, mag_Sn), axis=1)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(Train_complx_X[0].shape)\n",
    "# Train_complx_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_complx_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write validation files\n",
    "\n",
    "write_file(\"validation_s.txt\", fname_val_s)\n",
    "write_file(\"validation_n.txt\", fname_val_n)\n",
    "write_file(\"validation_x.txt\", fname_val_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file_X(\"train_x_tr.txt\", fname_trx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file_X(\"validation_x_tr.txt\", fname_trx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file(\"test_data.txt\", fname_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Writing training data N\n",
    "import librosa\n",
    "\n",
    "count=0\n",
    "total_train_s=[]\n",
    "with open('train_n.txt', 'wb') as fn:\n",
    "    for i in range(len(fname_trn)):\n",
    "        sn, sr=librosa.load(fname_trn[i], sr=None)\n",
    "        Sn=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "        mag_Sn=np.abs(Sn)\n",
    "#         trn_arr=np.concatenate((trn_arr, mag_Sn), axis=1)\n",
    "        np.savetxt(fn, mag_Sn, fmt='%.5f')\n",
    "        fn.write(b'\\n')\n",
    "fn.close()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Writing training data S\n",
    "\n",
    "with open('train_s.txt', 'wb') as fs:\n",
    "    for i in range(len(fname_trs)):\n",
    "        sn, sr=librosa.load(fname_trs[i], sr=None)\n",
    "        Sn=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "        mag_Sn=np.abs(Sn)\n",
    "#         trn_arr=np.concatenate((trn_arr, mag_Sn), axis=1)\n",
    "        np.savetxt(fs, mag_Sn, fmt='%.5f')\n",
    "        fs.write(b'\\n')\n",
    "fs.close()            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Writing training file X\n",
    "\n",
    "with open('train_x.txt', 'wb') as fs:\n",
    "    for i in range(len(fname_trx)):\n",
    "        sn, sr=librosa.load(fname_trx[i], sr=None)\n",
    "        Sn=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "        mag_Sn=np.abs(Sn)\n",
    "#         trn_arr=np.concatenate((trn_arr, mag_Sn), axis=1)\n",
    "        np.savetxt(fs, mag_Sn, fmt='%.5f')\n",
    "        fs.write(b'\\n')\n",
    "fs.close()            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for Reading file\n",
    "\n",
    "def read_file(file_name):\n",
    "    with open(file_name) as f:\n",
    "        lines=f.readlines()\n",
    "        print(len(lines))\n",
    "        sentence_full=[]\n",
    "        count = 0\n",
    "        sentence=[]\n",
    "        for line in lines:\n",
    "\n",
    "            if count < 513:\n",
    "                if count ==0:\n",
    "                    sentence=np.array(np.fromstring(line, dtype=complex_, sep=' '), ndmin=2)\n",
    "                    count+=1\n",
    "                else:\n",
    "                    myarray = np.array(np.fromstring(line, dtype=complex_, sep=' '), ndmin=2)\n",
    "                    sentence=np.concatenate((sentence, myarray), axis=0)\n",
    "                    count+=1\n",
    "            else:\n",
    "                sentence_full.append(sentence) \n",
    "                count=0\n",
    "                sentence=[]\n",
    "        return sentence_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616800\n",
      "616800\n",
      "616800\n"
     ]
    }
   ],
   "source": [
    "data_train_n = read_file(\"train_n.txt\")\n",
    "data_train_s = read_file(\"train_s.txt\")\n",
    "data_train_x = read_file(\"train_x.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(513, 65)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_s[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616800\n",
      "616800\n",
      "616800\n"
     ]
    }
   ],
   "source": [
    "data_val_n = read_file(\"validation_n.txt\")\n",
    "data_val_s = read_file(\"validation_s.txt\")\n",
    "data_val_x = read_file(\"validation_x.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read X from training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616800\n",
      "616800\n"
     ]
    }
   ],
   "source": [
    "data_train_xtr = read_file(\"train_x_tr.txt\")\n",
    "data_val_xtr = read_file(\"validation_x_tr.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(513, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_val_xtr)\n",
    "data_val_xtr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205600\n"
     ]
    }
   ],
   "source": [
    "data_test = read_file(\"test_data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating M for training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculating M:\n",
    "data_train_M=[]\n",
    "data_val_M=[]\n",
    "for i in range(len(data_train_s)):\n",
    "    data_train_M.append(1*(data_train_s[i]>data_train_n[i]))\n",
    "    data_val_M.append(1*(data_val_s[i]>data_val_n[i]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,  99, 110, 147, 217, 312, 336, 230, 261, 235, 228, 190,\n",
       "       206, 284, 315, 238, 195, 219, 169, 342, 362, 319, 299, 345, 317,\n",
       "       273, 264, 291, 229, 203, 181, 199, 214, 203, 171, 117, 133, 217,\n",
       "       247, 289, 228, 158, 164, 256, 306, 271, 332, 350, 277, 188, 139,\n",
       "       258, 251, 215, 108,  23,  17, 153, 147, 142, 141, 128, 131, 129])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data_train_M[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(513, 65)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_M[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Batch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vec=None\n",
    "def next_batch(data, data_M):\n",
    "    feature_vec=None\n",
    "    count=0\n",
    "    for i in range(1200):\n",
    "        if count==10:\n",
    "            yield feature_vec\n",
    "        temp=data[i].T\n",
    "        temp_M=data_M[i].T\n",
    "        tuple_vec=(temp, temp_M)\n",
    "        feature_vec = np.array(tuple_vec) if feature_vec is None else np.concatenate( (feature_vec,tuple_vec),axis=1)\n",
    "#         M_vec = np.array(temp_M) if M_vec is None else np.concatenate( (M_vec,temp_M),axis=1)\n",
    "        count+=1\n",
    "        print(feature_vec.shape)\n",
    "#         yield feature_vec\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batchXSCmplx(X_, S_, X_cmplx, S_cmplx):\n",
    "    \n",
    "    batch_x = None\n",
    "    batch_s = None\n",
    "    batch_x_cmplx = None\n",
    "    batch_s_cmplx = None\n",
    "    \n",
    "    for e,(x, s, x_cmplx, s_cmplx) in enumerate(zip(X_, S_, X_cmplx, S_cmplx)): \n",
    "        batch_x = np.array(x.T) if batch_x is None else np.concatenate( (batch_x,x.T), axis=0)\n",
    "        batch_s = np.array(s.T) if batch_s is None else np.concatenate( (batch_s,s.T), axis=0)\n",
    "        batch_x_cmplx = np.array(x_cmplx.T) if batch_x_cmplx is None else np.concatenate( (batch_x_cmplx,x_cmplx.T), axis=0)\n",
    "        batch_s_cmplx = np.array(s_cmplx.T) if batch_s_cmplx is None else np.concatenate( (batch_s_cmplx,s_cmplx.T), axis=0)\n",
    " \n",
    "        if e>0 and (e+1)%10==0:\n",
    "            temp_x, batch_x = batch_x, None\n",
    "            temp_s, batch_s = batch_s, None\n",
    "            temp_x_cmplx, batch_x_cmplx = batch_x_cmplx, None\n",
    "            temp_s_cmplx, batch_s_cmplx = batch_s_cmplx, None\n",
    "            \n",
    "            temp_x = temp_x.reshape((-1,Max_RNN,513))\n",
    "            temp_s = temp_s.reshape((-1,Max_RNN,513))\n",
    "            temp_x_cmplx = temp_x_cmplx.reshape((-1,Max_RNN,513))\n",
    "#             temp_s_cmplx = temp_s_cmplx.reshape((-1,Max_RNN,513))\n",
    "\n",
    "            yield temp_x, temp_s, temp_x_cmplx, temp_s_cmplx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batchX(X_):\n",
    "    \n",
    "    batch_x = None\n",
    "    \n",
    "    for e,x in enumerate(X_): \n",
    "        batch_x = np.array(x.T) if batch_x is None else np.concatenate( (batch_x,x.T), axis=0)\n",
    "        \n",
    "        if e>0 and (e+1)%10==0:\n",
    "            temp_x, batch_x = batch_x, None\n",
    "            \n",
    "            temp_x = temp_x.reshape((-1,Max_RNN,513))\n",
    "\n",
    "            yield temp_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(X_,Y_):\n",
    "    \n",
    "    batch_x, batch_y = None, None\n",
    "    \n",
    "    for e,(x,y) in enumerate(zip(X_,Y_)):\n",
    "#         print(e)\n",
    "        \n",
    "        batch_x = np.array(x.T) if batch_x is None else np.concatenate( (batch_x,x.T), axis=0)\n",
    "        batch_y = np.array(y.T) if batch_y is None else np.concatenate( (batch_y,y.T), axis=0)\n",
    "        \n",
    "#         print('batch_x',batch_x.shape)\n",
    "#         print('batch_y',batch_y.shape)\n",
    "        \n",
    "        if e>0 and (e+1)%10==0:\n",
    "            temp_x, batch_x = batch_x, None\n",
    "            temp_y, batch_y = batch_y, None\n",
    "            \n",
    "            temp_x = temp_x.reshape((-1,Max_RNN,513))\n",
    "            temp_y = temp_y.reshape((-1,Max_RNN,513))\n",
    "\n",
    "#             print('temp_x',temp_x.shape)\n",
    "#             print('temp_y',temp_y.shape)\n",
    "        \n",
    "            yield temp_x,temp_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_9 (Bidirection (None, 5, 10)             20760     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 5, 10)             0         \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 5, 10)             640       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5, 513)            5643      \n",
      "=================================================================\n",
      "Total params: 27,043\n",
      "Trainable params: 27,043\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 130 samples, validate on 130 samples\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 3s 22ms/step - loss: 0.6931 - acc: 0.5045 - val_loss: 0.6929 - val_acc: 0.5286\n",
      "Train on 260 samples, validate on 260 samples\n",
      "Epoch 1/1\n",
      "260/260 [==============================] - 0s 615us/step - loss: 0.6927 - acc: 0.5407 - val_loss: 0.6926 - val_acc: 0.5294\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 679us/step - loss: 0.6920 - acc: 0.5727 - val_loss: 0.6920 - val_acc: 0.5427\n",
      "Train on 138 samples, validate on 138 samples\n",
      "Epoch 1/1\n",
      "138/138 [==============================] - 0s 729us/step - loss: 0.6916 - acc: 0.5698 - val_loss: 0.6921 - val_acc: 0.5219\n",
      "Train on 184 samples, validate on 184 samples\n",
      "Epoch 1/1\n",
      "184/184 [==============================] - 0s 589us/step - loss: 0.6912 - acc: 0.5456 - val_loss: 0.6911 - val_acc: 0.5452\n",
      "Train on 206 samples, validate on 206 samples\n",
      "Epoch 1/1\n",
      "206/206 [==============================] - 0s 759us/step - loss: 0.6895 - acc: 0.6045 - val_loss: 0.6913 - val_acc: 0.5310\n",
      "Train on 174 samples, validate on 174 samples\n",
      "Epoch 1/1\n",
      "174/174 [==============================] - 0s 613us/step - loss: 0.6898 - acc: 0.5655 - val_loss: 0.6885 - val_acc: 0.5915\n",
      "Train on 278 samples, validate on 278 samples\n",
      "Epoch 1/1\n",
      "278/278 [==============================] - 0s 586us/step - loss: 0.6893 - acc: 0.5703 - val_loss: 0.6898 - val_acc: 0.5420\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 681us/step - loss: 0.6883 - acc: 0.5832 - val_loss: 0.6889 - val_acc: 0.5561\n",
      "Train on 160 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 649us/step - loss: 0.6891 - acc: 0.5608 - val_loss: 0.6931 - val_acc: 0.4854\n",
      "Train on 282 samples, validate on 282 samples\n",
      "Epoch 1/1\n",
      "282/282 [==============================] - 0s 579us/step - loss: 0.6924 - acc: 0.5034 - val_loss: 0.6938 - val_acc: 0.4787\n",
      "Train on 138 samples, validate on 138 samples\n",
      "Epoch 1/1\n",
      "138/138 [==============================] - 0s 734us/step - loss: 0.6843 - acc: 0.6064 - val_loss: 0.6897 - val_acc: 0.5263\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 660us/step - loss: 0.6860 - acc: 0.5855 - val_loss: 0.6864 - val_acc: 0.5635\n",
      "Train on 154 samples, validate on 154 samples\n",
      "Epoch 1/1\n",
      "154/154 [==============================] - 0s 685us/step - loss: 0.6824 - acc: 0.6231 - val_loss: 0.6858 - val_acc: 0.5595\n",
      "Train on 262 samples, validate on 262 samples\n",
      "Epoch 1/1\n",
      "262/262 [==============================] - 0s 649us/step - loss: 0.6892 - acc: 0.5434 - val_loss: 0.6943 - val_acc: 0.4828\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 689us/step - loss: 0.6810 - acc: 0.6053 - val_loss: 0.6818 - val_acc: 0.5773\n",
      "Train on 158 samples, validate on 158 samples\n",
      "Epoch 1/1\n",
      "158/158 [==============================] - 0s 658us/step - loss: 0.6876 - acc: 0.5463 - val_loss: 0.7012 - val_acc: 0.4209\n",
      "Train on 188 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 599us/step - loss: 0.6891 - acc: 0.5274 - val_loss: 0.6853 - val_acc: 0.5548\n",
      "Train on 216 samples, validate on 216 samples\n",
      "Epoch 1/1\n",
      "216/216 [==============================] - 0s 696us/step - loss: 0.6822 - acc: 0.5796 - val_loss: 0.6850 - val_acc: 0.5376\n",
      "Train on 176 samples, validate on 176 samples\n",
      "Epoch 1/1\n",
      "176/176 [==============================] - 0s 598us/step - loss: 0.6810 - acc: 0.5770 - val_loss: 0.6856 - val_acc: 0.5394\n",
      "Train on 212 samples, validate on 212 samples\n",
      "Epoch 1/1\n",
      "212/212 [==============================] - 0s 742us/step - loss: 0.6832 - acc: 0.5742 - val_loss: 0.6806 - val_acc: 0.5727\n",
      "Train on 330 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "330/330 [==============================] - 0s 649us/step - loss: 0.6869 - acc: 0.5568 - val_loss: 0.6878 - val_acc: 0.5274\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 589us/step - loss: 0.6812 - acc: 0.5962 - val_loss: 0.6660 - val_acc: 0.6582\n",
      "Train on 256 samples, validate on 256 samples\n",
      "Epoch 1/1\n",
      "256/256 [==============================] - 0s 629us/step - loss: 0.6725 - acc: 0.6365 - val_loss: 0.6762 - val_acc: 0.5889\n",
      "Train on 264 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "264/264 [==============================] - 0s 639us/step - loss: 0.6779 - acc: 0.5979 - val_loss: 0.6707 - val_acc: 0.6025\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 565us/step - loss: 0.6765 - acc: 0.6126 - val_loss: 0.6750 - val_acc: 0.5816\n",
      "Train on 206 samples, validate on 206 samples\n",
      "Epoch 1/1\n",
      "206/206 [==============================] - 0s 747us/step - loss: 0.6781 - acc: 0.5743 - val_loss: 0.6850 - val_acc: 0.5323\n",
      "Train on 288 samples, validate on 288 samples\n",
      "Epoch 1/1\n",
      "288/288 [==============================] - 0s 577us/step - loss: 0.6731 - acc: 0.6184 - val_loss: 0.6748 - val_acc: 0.5954\n",
      "Train on 222 samples, validate on 222 samples\n",
      "Epoch 1/1\n",
      "222/222 [==============================] - 0s 716us/step - loss: 0.6740 - acc: 0.6067 - val_loss: 0.6744 - val_acc: 0.5821\n",
      "Train on 288 samples, validate on 288 samples\n",
      "Epoch 1/1\n",
      "288/288 [==============================] - 0s 577us/step - loss: 0.6626 - acc: 0.6237 - val_loss: 0.6648 - val_acc: 0.6105\n",
      "Train on 78 samples, validate on 78 samples\n",
      "Epoch 1/1\n",
      "78/78 [==============================] - 0s 686us/step - loss: 0.6634 - acc: 0.6388 - val_loss: 0.6730 - val_acc: 0.5917\n",
      "Train on 164 samples, validate on 164 samples\n",
      "Epoch 1/1\n",
      "164/164 [==============================] - 0s 662us/step - loss: 0.6763 - acc: 0.5815 - val_loss: 0.6765 - val_acc: 0.5737\n",
      "Train on 204 samples, validate on 204 samples\n",
      "Epoch 1/1\n",
      "204/204 [==============================] - 0s 748us/step - loss: 0.6720 - acc: 0.5976 - val_loss: 0.6653 - val_acc: 0.6103\n",
      "Train on 220 samples, validate on 220 samples\n",
      "Epoch 1/1\n",
      "220/220 [==============================] - 0s 717us/step - loss: 0.6687 - acc: 0.5837 - val_loss: 0.6730 - val_acc: 0.5703\n",
      "Train on 194 samples, validate on 194 samples\n",
      "Epoch 1/1\n",
      "194/194 [==============================] - 0s 564us/step - loss: 0.6951 - acc: 0.5090 - val_loss: 0.6889 - val_acc: 0.5137\n",
      "Train on 196 samples, validate on 196 samples\n",
      "Epoch 1/1\n",
      "196/196 [==============================] - 0s 560us/step - loss: 0.6576 - acc: 0.6352 - val_loss: 0.6607 - val_acc: 0.5963\n",
      "Train on 114 samples, validate on 114 samples\n",
      "Epoch 1/1\n",
      "114/114 [==============================] - 0s 868us/step - loss: 0.6595 - acc: 0.5955 - val_loss: 0.6551 - val_acc: 0.6170\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 784us/step - loss: 0.6604 - acc: 0.5973 - val_loss: 0.6619 - val_acc: 0.5804\n",
      "Train on 134 samples, validate on 134 samples\n",
      "Epoch 1/1\n",
      "134/134 [==============================] - 0s 781us/step - loss: 0.6693 - acc: 0.5618 - val_loss: 0.6600 - val_acc: 0.5740\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 671us/step - loss: 0.6530 - acc: 0.6641 - val_loss: 0.6355 - val_acc: 0.6706\n",
      "Train on 212 samples, validate on 212 samples\n",
      "Epoch 1/1\n",
      "212/212 [==============================] - 0s 735us/step - loss: 0.6717 - acc: 0.5651 - val_loss: 0.6914 - val_acc: 0.5012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 210 samples, validate on 210 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 0s 740us/step - loss: 0.6770 - acc: 0.5557 - val_loss: 0.6715 - val_acc: 0.5570\n",
      "Train on 158 samples, validate on 158 samples\n",
      "Epoch 1/1\n",
      "158/158 [==============================] - 0s 668us/step - loss: 0.6558 - acc: 0.6350 - val_loss: 0.6667 - val_acc: 0.5627\n",
      "Train on 198 samples, validate on 198 samples\n",
      "Epoch 1/1\n",
      "198/198 [==============================] - 0s 601us/step - loss: 0.6563 - acc: 0.6127 - val_loss: 0.6635 - val_acc: 0.5837\n",
      "Train on 206 samples, validate on 206 samples\n",
      "Epoch 1/1\n",
      "206/206 [==============================] - 0s 744us/step - loss: 0.6847 - acc: 0.5008 - val_loss: 0.6723 - val_acc: 0.5474\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 646us/step - loss: 0.6611 - acc: 0.5799 - val_loss: 0.6523 - val_acc: 0.6077\n",
      "Train on 214 samples, validate on 214 samples\n",
      "Epoch 1/1\n",
      "214/214 [==============================] - 0s 747us/step - loss: 0.6627 - acc: 0.5845 - val_loss: 0.6645 - val_acc: 0.5787\n",
      "Train on 186 samples, validate on 186 samples\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 596us/step - loss: 0.6488 - acc: 0.6225 - val_loss: 0.6522 - val_acc: 0.6007\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 604us/step - loss: 0.6562 - acc: 0.6026 - val_loss: 0.6547 - val_acc: 0.5767\n",
      "Train on 222 samples, validate on 222 samples\n",
      "Epoch 1/1\n",
      "222/222 [==============================] - 0s 724us/step - loss: 0.6612 - acc: 0.5733 - val_loss: 0.6523 - val_acc: 0.6052\n",
      "Train on 296 samples, validate on 296 samples\n",
      "Epoch 1/1\n",
      "296/296 [==============================] - 0s 603us/step - loss: 0.7122 - acc: 0.4742 - val_loss: 0.7129 - val_acc: 0.4622\n",
      "Train on 230 samples, validate on 230 samples\n",
      "Epoch 1/1\n",
      "230/230 [==============================] - 0s 679us/step - loss: 0.6889 - acc: 0.5016 - val_loss: 0.6862 - val_acc: 0.5141\n",
      "Train on 190 samples, validate on 190 samples\n",
      "Epoch 1/1\n",
      "190/190 [==============================] - 0s 562us/step - loss: 0.6806 - acc: 0.5641 - val_loss: 0.6897 - val_acc: 0.5354\n",
      "Train on 186 samples, validate on 186 samples\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 588us/step - loss: 0.6802 - acc: 0.5600 - val_loss: 0.6805 - val_acc: 0.5457\n",
      "Train on 218 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "218/218 [==============================] - 0s 718us/step - loss: 0.6791 - acc: 0.5595 - val_loss: 0.6855 - val_acc: 0.5487\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 914us/step - loss: 0.6816 - acc: 0.5694 - val_loss: 0.6797 - val_acc: 0.5530\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 718us/step - loss: 0.6769 - acc: 0.5960 - val_loss: 0.6855 - val_acc: 0.5658\n",
      "Train on 252 samples, validate on 252 samples\n",
      "Epoch 1/1\n",
      "252/252 [==============================] - 0s 638us/step - loss: 0.6754 - acc: 0.5775 - val_loss: 0.6648 - val_acc: 0.5811\n",
      "Train on 152 samples, validate on 152 samples\n",
      "Epoch 1/1\n",
      "152/152 [==============================] - 0s 691us/step - loss: 0.6643 - acc: 0.5798 - val_loss: 0.6814 - val_acc: 0.5264\n",
      "Train on 118 samples, validate on 118 samples\n",
      "Epoch 1/1\n",
      "118/118 [==============================] - 0s 879us/step - loss: 0.6697 - acc: 0.5683 - val_loss: 0.6730 - val_acc: 0.5667\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/1\n",
      "232/232 [==============================] - 0s 751us/step - loss: 0.6610 - acc: 0.5879 - val_loss: 0.6823 - val_acc: 0.5123\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 748us/step - loss: 0.6348 - acc: 0.6509 - val_loss: 0.6484 - val_acc: 0.5826\n",
      "Train on 144 samples, validate on 144 samples\n",
      "Epoch 1/1\n",
      "144/144 [==============================] - 0s 855us/step - loss: 0.6764 - acc: 0.5592 - val_loss: 0.6889 - val_acc: 0.5114\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 721us/step - loss: 0.6477 - acc: 0.6166 - val_loss: 0.6549 - val_acc: 0.5908\n",
      "Train on 212 samples, validate on 212 samples\n",
      "Epoch 1/1\n",
      "212/212 [==============================] - 0s 793us/step - loss: 0.6754 - acc: 0.5323 - val_loss: 0.6448 - val_acc: 0.6184\n",
      "Train on 194 samples, validate on 194 samples\n",
      "Epoch 1/1\n",
      "194/194 [==============================] - 0s 598us/step - loss: 0.6513 - acc: 0.5919 - val_loss: 0.6566 - val_acc: 0.5798\n",
      "Train on 164 samples, validate on 164 samples\n",
      "Epoch 1/1\n",
      "164/164 [==============================] - 0s 665us/step - loss: 0.6728 - acc: 0.5694 - val_loss: 0.6656 - val_acc: 0.5779\n",
      "Train on 346 samples, validate on 346 samples\n",
      "Epoch 1/1\n",
      "346/346 [==============================] - 0s 629us/step - loss: 0.6624 - acc: 0.5860 - val_loss: 0.6505 - val_acc: 0.5923\n",
      "Train on 172 samples, validate on 172 samples\n",
      "Epoch 1/1\n",
      "172/172 [==============================] - 0s 627us/step - loss: 0.6731 - acc: 0.5657 - val_loss: 0.6741 - val_acc: 0.5728\n",
      "Train on 286 samples, validate on 286 samples\n",
      "Epoch 1/1\n",
      "286/286 [==============================] - 0s 592us/step - loss: 0.6326 - acc: 0.6498 - val_loss: 0.6336 - val_acc: 0.6718\n",
      "Train on 186 samples, validate on 186 samples\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 582us/step - loss: 0.6527 - acc: 0.6411 - val_loss: 0.6395 - val_acc: 0.6691\n",
      "Train on 216 samples, validate on 216 samples\n",
      "Epoch 1/1\n",
      "216/216 [==============================] - 0s 751us/step - loss: 0.6491 - acc: 0.6295 - val_loss: 0.6530 - val_acc: 0.6320\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 682us/step - loss: 0.6399 - acc: 0.6445 - val_loss: 0.6427 - val_acc: 0.6282\n",
      "Train on 132 samples, validate on 132 samples\n",
      "Epoch 1/1\n",
      "132/132 [==============================] - 0s 767us/step - loss: 0.6266 - acc: 0.6640 - val_loss: 0.6327 - val_acc: 0.6561\n",
      "Train on 280 samples, validate on 280 samples\n",
      "Epoch 1/1\n",
      "280/280 [==============================] - 0s 599us/step - loss: 0.6131 - acc: 0.6752 - val_loss: 0.6334 - val_acc: 0.6342\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 817us/step - loss: 0.6897 - acc: 0.5279 - val_loss: 0.6501 - val_acc: 0.6007\n",
      "Train on 190 samples, validate on 190 samples\n",
      "Epoch 1/1\n",
      "190/190 [==============================] - 0s 588us/step - loss: 0.6518 - acc: 0.6108 - val_loss: 0.6371 - val_acc: 0.6527\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 695us/step - loss: 0.6259 - acc: 0.6715 - val_loss: 0.6464 - val_acc: 0.6504\n",
      "Train on 140 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "140/140 [==============================] - 0s 737us/step - loss: 0.6164 - acc: 0.6770 - val_loss: 0.6352 - val_acc: 0.6442\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 599us/step - loss: 0.6408 - acc: 0.6181 - val_loss: 0.6580 - val_acc: 0.6037\n",
      "Train on 142 samples, validate on 142 samples\n",
      "Epoch 1/1\n",
      "142/142 [==============================] - 0s 721us/step - loss: 0.6892 - acc: 0.5600 - val_loss: 0.6904 - val_acc: 0.5754\n",
      "Train on 108 samples, validate on 108 samples\n",
      "Epoch 1/1\n",
      "108/108 [==============================] - 0s 987us/step - loss: 0.6678 - acc: 0.5713 - val_loss: 0.6421 - val_acc: 0.6323\n",
      "Train on 184 samples, validate on 184 samples\n",
      "Epoch 1/1\n",
      "184/184 [==============================] - 0s 610us/step - loss: 0.6639 - acc: 0.6062 - val_loss: 0.6532 - val_acc: 0.6360\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 774us/step - loss: 0.6625 - acc: 0.6264 - val_loss: 0.6691 - val_acc: 0.5800\n",
      "Train on 152 samples, validate on 152 samples\n",
      "Epoch 1/1\n",
      "152/152 [==============================] - 0s 678us/step - loss: 0.6732 - acc: 0.5747 - val_loss: 0.6632 - val_acc: 0.5704\n",
      "Train on 174 samples, validate on 174 samples\n",
      "Epoch 1/1\n",
      "174/174 [==============================] - 0s 642us/step - loss: 0.6703 - acc: 0.5981 - val_loss: 0.6592 - val_acc: 0.6035\n",
      "Train on 140 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "140/140 [==============================] - 0s 776us/step - loss: 0.6665 - acc: 0.5992 - val_loss: 0.6461 - val_acc: 0.6044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 228 samples, validate on 228 samples\n",
      "Epoch 1/1\n",
      "228/228 [==============================] - 0s 687us/step - loss: 0.6811 - acc: 0.5430 - val_loss: 0.6607 - val_acc: 0.6203\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 757us/step - loss: 0.6468 - acc: 0.6444 - val_loss: 0.6447 - val_acc: 0.6357\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 598us/step - loss: 0.6570 - acc: 0.5971 - val_loss: 0.6479 - val_acc: 0.6475\n",
      "Train on 242 samples, validate on 242 samples\n",
      "Epoch 1/1\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.7030 - acc: 0.5378 - val_loss: 0.6988 - val_acc: 0.5683\n",
      "Train on 144 samples, validate on 144 samples\n",
      "Epoch 1/1\n",
      "144/144 [==============================] - 0s 695us/step - loss: 0.6511 - acc: 0.6339 - val_loss: 0.6502 - val_acc: 0.6327\n",
      "Train on 356 samples, validate on 356 samples\n",
      "Epoch 1/1\n",
      "356/356 [==============================] - 0s 617us/step - loss: 0.6601 - acc: 0.6273 - val_loss: 0.6571 - val_acc: 0.6207\n",
      "Train on 234 samples, validate on 234 samples\n",
      "Epoch 1/1\n",
      "234/234 [==============================] - 0s 671us/step - loss: 0.6591 - acc: 0.6681 - val_loss: 0.6471 - val_acc: 0.7082\n",
      "Train on 184 samples, validate on 184 samples\n",
      "Epoch 1/1\n",
      "184/184 [==============================] - 0s 606us/step - loss: 0.6607 - acc: 0.6363 - val_loss: 0.6551 - val_acc: 0.6514\n",
      "Train on 238 samples, validate on 238 samples\n",
      "Epoch 1/1\n",
      "238/238 [==============================] - 0s 683us/step - loss: 0.6567 - acc: 0.6269 - val_loss: 0.6442 - val_acc: 0.6782\n",
      "Train on 340 samples, validate on 340 samples\n",
      "Epoch 1/1\n",
      "340/340 [==============================] - 0s 623us/step - loss: 0.6510 - acc: 0.6470 - val_loss: 0.6346 - val_acc: 0.6875\n",
      "Train on 220 samples, validate on 220 samples\n",
      "Epoch 1/1\n",
      "220/220 [==============================] - 0s 717us/step - loss: 0.6498 - acc: 0.6830 - val_loss: 0.6472 - val_acc: 0.6633\n",
      "Train on 260 samples, validate on 260 samples\n",
      "Epoch 1/1\n",
      "260/260 [==============================] - 0s 617us/step - loss: 0.6458 - acc: 0.6579 - val_loss: 0.6376 - val_acc: 0.7064\n",
      "Train on 216 samples, validate on 216 samples\n",
      "Epoch 1/1\n",
      "216/216 [==============================] - 0s 725us/step - loss: 0.6404 - acc: 0.6937 - val_loss: 0.6225 - val_acc: 0.7189\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 545us/step - loss: 0.6441 - acc: 0.6336 - val_loss: 0.6222 - val_acc: 0.6572\n",
      "Train on 124 samples, validate on 124 samples\n",
      "Epoch 1/1\n",
      "124/124 [==============================] - 0s 829us/step - loss: 0.6361 - acc: 0.6418 - val_loss: 0.6463 - val_acc: 0.6102\n",
      "Train on 192 samples, validate on 192 samples\n",
      "Epoch 1/1\n",
      "192/192 [==============================] - 0s 572us/step - loss: 0.6435 - acc: 0.6192 - val_loss: 0.6231 - val_acc: 0.6581\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 625us/step - loss: 0.6314 - acc: 0.6509 - val_loss: 0.6319 - val_acc: 0.6516\n",
      "Train on 282 samples, validate on 282 samples\n",
      "Epoch 1/1\n",
      "282/282 [==============================] - 0s 578us/step - loss: 0.6257 - acc: 0.6809 - val_loss: 0.6227 - val_acc: 0.6653\n",
      "Train on 220 samples, validate on 220 samples\n",
      "Epoch 1/1\n",
      "220/220 [==============================] - 0s 728us/step - loss: 0.6186 - acc: 0.6760 - val_loss: 0.6251 - val_acc: 0.6821\n",
      "Train on 194 samples, validate on 194 samples\n",
      "Epoch 1/1\n",
      "194/194 [==============================] - 0s 579us/step - loss: 0.6313 - acc: 0.6431 - val_loss: 0.6314 - val_acc: 0.6465\n",
      "Train on 302 samples, validate on 302 samples\n",
      "Epoch 1/1\n",
      "302/302 [==============================] - 0s 701us/step - loss: 0.6835 - acc: 0.5779 - val_loss: 0.6593 - val_acc: 0.6240\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 577us/step - loss: 0.6254 - acc: 0.6683 - val_loss: 0.6181 - val_acc: 0.6964\n",
      "Train on 160 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 657us/step - loss: 0.6303 - acc: 0.6483 - val_loss: 0.5978 - val_acc: 0.7242\n",
      "Train on 158 samples, validate on 158 samples\n",
      "Epoch 1/1\n",
      "158/158 [==============================] - 0s 669us/step - loss: 0.6318 - acc: 0.6216 - val_loss: 0.6045 - val_acc: 0.7028\n",
      "Train on 154 samples, validate on 154 samples\n",
      "Epoch 1/1\n",
      "154/154 [==============================] - 0s 668us/step - loss: 0.6461 - acc: 0.6184 - val_loss: 0.6280 - val_acc: 0.6474\n",
      "Train on 166 samples, validate on 166 samples\n",
      "Epoch 1/1\n",
      "166/166 [==============================] - 0s 647us/step - loss: 0.6291 - acc: 0.6566 - val_loss: 0.6079 - val_acc: 0.6864\n",
      "Train on 224 samples, validate on 224 samples\n",
      "Epoch 1/1\n",
      "224/224 [==============================] - 0s 708us/step - loss: 0.6149 - acc: 0.6749 - val_loss: 0.5952 - val_acc: 0.6857\n",
      "Train on 294 samples, validate on 294 samples\n",
      "Epoch 1/1\n",
      "294/294 [==============================] - 0s 579us/step - loss: 0.6081 - acc: 0.6884 - val_loss: 0.6228 - val_acc: 0.6636\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 684us/step - loss: 0.5899 - acc: 0.7032 - val_loss: 0.5824 - val_acc: 0.7144\n",
      "Train on 196 samples, validate on 196 samples\n",
      "Epoch 1/1\n",
      "196/196 [==============================] - 0s 545us/step - loss: 0.6366 - acc: 0.6471 - val_loss: 0.6119 - val_acc: 0.6782\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 684us/step - loss: 0.6616 - acc: 0.6299 - val_loss: 0.6424 - val_acc: 0.6555\n",
      "Train on 234 samples, validate on 234 samples\n",
      "Epoch 1/1\n",
      "234/234 [==============================] - 0s 677us/step - loss: 0.6073 - acc: 0.6763 - val_loss: 0.6030 - val_acc: 0.6654\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 595us/step - loss: 0.6455 - acc: 0.6193 - val_loss: 0.6267 - val_acc: 0.6676\n",
      "Train on 130 samples, validate on 130 samples\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 766us/step - loss: 0.5586 - acc: 0.7341 - val_loss: 0.5971 - val_acc: 0.6861\n",
      "Train on 260 samples, validate on 260 samples\n",
      "Epoch 1/1\n",
      "260/260 [==============================] - 0s 653us/step - loss: 0.6191 - acc: 0.6646 - val_loss: 0.6032 - val_acc: 0.7089\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 679us/step - loss: 0.5974 - acc: 0.6902 - val_loss: 0.6120 - val_acc: 0.6653\n",
      "Train on 138 samples, validate on 138 samples\n",
      "Epoch 1/1\n",
      "138/138 [==============================] - 0s 815us/step - loss: 0.5926 - acc: 0.6925 - val_loss: 0.6022 - val_acc: 0.6952\n",
      "Train on 184 samples, validate on 184 samples\n",
      "Epoch 1/1\n",
      "184/184 [==============================] - 0s 597us/step - loss: 0.5847 - acc: 0.7030 - val_loss: 0.6082 - val_acc: 0.6727\n",
      "Train on 206 samples, validate on 206 samples\n",
      "Epoch 1/1\n",
      "206/206 [==============================] - 0s 760us/step - loss: 0.5899 - acc: 0.6968 - val_loss: 0.6201 - val_acc: 0.6584\n",
      "Train on 174 samples, validate on 174 samples\n",
      "Epoch 1/1\n",
      "174/174 [==============================] - 0s 609us/step - loss: 0.6134 - acc: 0.6706 - val_loss: 0.5975 - val_acc: 0.6906\n",
      "Train on 278 samples, validate on 278 samples\n",
      "Epoch 1/1\n",
      "278/278 [==============================] - 0s 606us/step - loss: 0.6226 - acc: 0.6507 - val_loss: 0.6102 - val_acc: 0.6812\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 682us/step - loss: 0.6162 - acc: 0.6403 - val_loss: 0.6158 - val_acc: 0.6637\n",
      "Train on 160 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 666us/step - loss: 0.6291 - acc: 0.6694 - val_loss: 0.6251 - val_acc: 0.6987\n",
      "Train on 282 samples, validate on 282 samples\n",
      "Epoch 1/1\n",
      "282/282 [==============================] - 0s 591us/step - loss: 0.6763 - acc: 0.5862 - val_loss: 0.6357 - val_acc: 0.6594\n",
      "Train on 138 samples, validate on 138 samples\n",
      "Epoch 1/1\n",
      "138/138 [==============================] - 0s 744us/step - loss: 0.6254 - acc: 0.6630 - val_loss: 0.6325 - val_acc: 0.6558\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 677us/step - loss: 0.6178 - acc: 0.6723 - val_loss: 0.5988 - val_acc: 0.6904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 154 samples, validate on 154 samples\n",
      "Epoch 1/1\n",
      "154/154 [==============================] - 0s 687us/step - loss: 0.6252 - acc: 0.6312 - val_loss: 0.6186 - val_acc: 0.6789\n",
      "Train on 262 samples, validate on 262 samples\n",
      "Epoch 1/1\n",
      "262/262 [==============================] - 0s 662us/step - loss: 0.6648 - acc: 0.5967 - val_loss: 0.6442 - val_acc: 0.6556\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 678us/step - loss: 0.6021 - acc: 0.7044 - val_loss: 0.5761 - val_acc: 0.7391\n",
      "Train on 158 samples, validate on 158 samples\n",
      "Epoch 1/1\n",
      "158/158 [==============================] - 0s 670us/step - loss: 0.6434 - acc: 0.6392 - val_loss: 0.6698 - val_acc: 0.6402\n",
      "Train on 188 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 617us/step - loss: 0.6164 - acc: 0.6907 - val_loss: 0.6093 - val_acc: 0.6994\n",
      "Train on 216 samples, validate on 216 samples\n",
      "Epoch 1/1\n",
      "100/216 [============>.................] - ETA: 0s - loss: 0.6110 - acc: 0.6713"
     ]
    }
   ],
   "source": [
    "Max_RNN=5\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Bidirectional(LSTM(Max_RNN, return_sequences=True), input_shape=(Max_RNN,513)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(Max_RNN, return_sequences=True)))\n",
    "# model.add(GRU(output_dim = 513, input_length = 5, input_dim = 513, return_sequences=True))\n",
    "\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(TimeDistributed(Dense(513, activation='sigmoid')))\n",
    "model.add(Dense(513, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "for e in range(10):\n",
    "    for (b_x,b_y), (v_x,v_y) in zip(next_batch(data_train_x, data_train_M), next_batch(data_val_x, data_val_M)):\n",
    "        model.fit(b_x, b_y, validation_data=(v_x,v_y), shuffle=True, batch_size=100)\n",
    "\n",
    "#     model.fit( , epochs=20, steps_per_epoch=700, validation_data=next_batch_mb(DATA_val_x, DATA_val_x,10), validation_steps=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75%\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for v_x,v_y in next_batch(data_val_x, data_val_M):\n",
    "    scores.append( model.evaluate(v_x, v_y,verbose=0)[1] )\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % np.mean(scores*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130, 5, 513)\n",
      "(130, 5, 513)\n",
      "(130, 5, 513)\n",
      "v_s_cmplx (650, 513)\n",
      "S.shape (513, 650)\n",
      "S_hat.shape (513, 650)\n",
      "(260, 5, 513)\n",
      "(260, 5, 513)\n",
      "(260, 5, 513)\n",
      "v_s_cmplx (1300, 513)\n",
      "S.shape (513, 1300)\n",
      "S_hat.shape (513, 1300)\n",
      "(150, 5, 513)\n",
      "(150, 5, 513)\n",
      "(150, 5, 513)\n",
      "v_s_cmplx (750, 513)\n",
      "S.shape (513, 750)\n",
      "S_hat.shape (513, 750)\n",
      "(138, 5, 513)\n",
      "(138, 5, 513)\n",
      "(138, 5, 513)\n",
      "v_s_cmplx (690, 513)\n",
      "S.shape (513, 690)\n",
      "S_hat.shape (513, 690)\n",
      "(184, 5, 513)\n",
      "(184, 5, 513)\n",
      "(184, 5, 513)\n",
      "v_s_cmplx (920, 513)\n",
      "S.shape (513, 920)\n",
      "S_hat.shape (513, 920)\n",
      "(206, 5, 513)\n",
      "(206, 5, 513)\n",
      "(206, 5, 513)\n",
      "v_s_cmplx (1030, 513)\n",
      "S.shape (513, 1030)\n",
      "S_hat.shape (513, 1030)\n",
      "(174, 5, 513)\n",
      "(174, 5, 513)\n",
      "(174, 5, 513)\n",
      "v_s_cmplx (870, 513)\n",
      "S.shape (513, 870)\n",
      "S_hat.shape (513, 870)\n",
      "(278, 5, 513)\n",
      "(278, 5, 513)\n",
      "(278, 5, 513)\n",
      "v_s_cmplx (1390, 513)\n",
      "S.shape (513, 1390)\n",
      "S_hat.shape (513, 1390)\n",
      "(156, 5, 513)\n",
      "(156, 5, 513)\n",
      "(156, 5, 513)\n",
      "v_s_cmplx (780, 513)\n",
      "S.shape (513, 780)\n",
      "S_hat.shape (513, 780)\n",
      "(160, 5, 513)\n",
      "(160, 5, 513)\n",
      "(160, 5, 513)\n",
      "v_s_cmplx (800, 513)\n",
      "S.shape (513, 800)\n",
      "S_hat.shape (513, 800)\n",
      "(282, 5, 513)\n",
      "(282, 5, 513)\n",
      "(282, 5, 513)\n",
      "v_s_cmplx (1410, 513)\n",
      "S.shape (513, 1410)\n",
      "S_hat.shape (513, 1410)\n",
      "(138, 5, 513)\n",
      "(138, 5, 513)\n",
      "(138, 5, 513)\n",
      "v_s_cmplx (690, 513)\n",
      "S.shape (513, 690)\n",
      "S_hat.shape (513, 690)\n",
      "(156, 5, 513)\n",
      "(156, 5, 513)\n",
      "(156, 5, 513)\n",
      "v_s_cmplx (780, 513)\n",
      "S.shape (513, 780)\n",
      "S_hat.shape (513, 780)\n",
      "(154, 5, 513)\n",
      "(154, 5, 513)\n",
      "(154, 5, 513)\n",
      "v_s_cmplx (770, 513)\n",
      "S.shape (513, 770)\n",
      "S_hat.shape (513, 770)\n",
      "(262, 5, 513)\n",
      "(262, 5, 513)\n",
      "(262, 5, 513)\n",
      "v_s_cmplx (1310, 513)\n",
      "S.shape (513, 1310)\n",
      "S_hat.shape (513, 1310)\n",
      "(156, 5, 513)\n",
      "(156, 5, 513)\n",
      "(156, 5, 513)\n",
      "v_s_cmplx (780, 513)\n",
      "S.shape (513, 780)\n",
      "S_hat.shape (513, 780)\n",
      "(158, 5, 513)\n",
      "(158, 5, 513)\n",
      "(158, 5, 513)\n",
      "v_s_cmplx (790, 513)\n",
      "S.shape (513, 790)\n",
      "S_hat.shape (513, 790)\n",
      "(188, 5, 513)\n",
      "(188, 5, 513)\n",
      "(188, 5, 513)\n",
      "v_s_cmplx (940, 513)\n",
      "S.shape (513, 940)\n",
      "S_hat.shape (513, 940)\n",
      "(216, 5, 513)\n",
      "(216, 5, 513)\n",
      "(216, 5, 513)\n",
      "v_s_cmplx (1080, 513)\n",
      "S.shape (513, 1080)\n",
      "S_hat.shape (513, 1080)\n",
      "(176, 5, 513)\n",
      "(176, 5, 513)\n",
      "(176, 5, 513)\n",
      "v_s_cmplx (880, 513)\n",
      "S.shape (513, 880)\n",
      "S_hat.shape (513, 880)\n",
      "(212, 5, 513)\n",
      "(212, 5, 513)\n",
      "(212, 5, 513)\n",
      "v_s_cmplx (1060, 513)\n",
      "S.shape (513, 1060)\n",
      "S_hat.shape (513, 1060)\n",
      "(330, 5, 513)\n",
      "(330, 5, 513)\n",
      "(330, 5, 513)\n",
      "v_s_cmplx (1650, 513)\n",
      "S.shape (513, 1650)\n",
      "S_hat.shape (513, 1650)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "v_s_cmplx (900, 513)\n",
      "S.shape (513, 900)\n",
      "S_hat.shape (513, 900)\n",
      "(256, 5, 513)\n",
      "(256, 5, 513)\n",
      "(256, 5, 513)\n",
      "v_s_cmplx (1280, 513)\n",
      "S.shape (513, 1280)\n",
      "S_hat.shape (513, 1280)\n",
      "(264, 5, 513)\n",
      "(264, 5, 513)\n",
      "(264, 5, 513)\n",
      "v_s_cmplx (1320, 513)\n",
      "S.shape (513, 1320)\n",
      "S_hat.shape (513, 1320)\n",
      "(200, 5, 513)\n",
      "(200, 5, 513)\n",
      "(200, 5, 513)\n",
      "v_s_cmplx (1000, 513)\n",
      "S.shape (513, 1000)\n",
      "S_hat.shape (513, 1000)\n",
      "(206, 5, 513)\n",
      "(206, 5, 513)\n",
      "(206, 5, 513)\n",
      "v_s_cmplx (1030, 513)\n",
      "S.shape (513, 1030)\n",
      "S_hat.shape (513, 1030)\n",
      "(288, 5, 513)\n",
      "(288, 5, 513)\n",
      "(288, 5, 513)\n",
      "v_s_cmplx (1440, 513)\n",
      "S.shape (513, 1440)\n",
      "S_hat.shape (513, 1440)\n",
      "(222, 5, 513)\n",
      "(222, 5, 513)\n",
      "(222, 5, 513)\n",
      "v_s_cmplx (1110, 513)\n",
      "S.shape (513, 1110)\n",
      "S_hat.shape (513, 1110)\n",
      "(288, 5, 513)\n",
      "(288, 5, 513)\n",
      "(288, 5, 513)\n",
      "v_s_cmplx (1440, 513)\n",
      "S.shape (513, 1440)\n",
      "S_hat.shape (513, 1440)\n",
      "(78, 5, 513)\n",
      "(78, 5, 513)\n",
      "(78, 5, 513)\n",
      "v_s_cmplx (390, 513)\n",
      "S.shape (513, 390)\n",
      "S_hat.shape (513, 390)\n",
      "(164, 5, 513)\n",
      "(164, 5, 513)\n",
      "(164, 5, 513)\n",
      "v_s_cmplx (820, 513)\n",
      "S.shape (513, 820)\n",
      "S_hat.shape (513, 820)\n",
      "(204, 5, 513)\n",
      "(204, 5, 513)\n",
      "(204, 5, 513)\n",
      "v_s_cmplx (1020, 513)\n",
      "S.shape (513, 1020)\n",
      "S_hat.shape (513, 1020)\n",
      "(220, 5, 513)\n",
      "(220, 5, 513)\n",
      "(220, 5, 513)\n",
      "v_s_cmplx (1100, 513)\n",
      "S.shape (513, 1100)\n",
      "S_hat.shape (513, 1100)\n",
      "(194, 5, 513)\n",
      "(194, 5, 513)\n",
      "(194, 5, 513)\n",
      "v_s_cmplx (970, 513)\n",
      "S.shape (513, 970)\n",
      "S_hat.shape (513, 970)\n",
      "(196, 5, 513)\n",
      "(196, 5, 513)\n",
      "(196, 5, 513)\n",
      "v_s_cmplx (980, 513)\n",
      "S.shape (513, 980)\n",
      "S_hat.shape (513, 980)\n",
      "(114, 5, 513)\n",
      "(114, 5, 513)\n",
      "(114, 5, 513)\n",
      "v_s_cmplx (570, 513)\n",
      "S.shape (513, 570)\n",
      "S_hat.shape (513, 570)\n",
      "(202, 5, 513)\n",
      "(202, 5, 513)\n",
      "(202, 5, 513)\n",
      "v_s_cmplx (1010, 513)\n",
      "S.shape (513, 1010)\n",
      "S_hat.shape (513, 1010)\n",
      "(134, 5, 513)\n",
      "(134, 5, 513)\n",
      "(134, 5, 513)\n",
      "v_s_cmplx (670, 513)\n",
      "S.shape (513, 670)\n",
      "S_hat.shape (513, 670)\n",
      "(148, 5, 513)\n",
      "(148, 5, 513)\n",
      "(148, 5, 513)\n",
      "v_s_cmplx (740, 513)\n",
      "S.shape (513, 740)\n",
      "S_hat.shape (513, 740)\n",
      "(212, 5, 513)\n",
      "(212, 5, 513)\n",
      "(212, 5, 513)\n",
      "v_s_cmplx (1060, 513)\n",
      "S.shape (513, 1060)\n",
      "S_hat.shape (513, 1060)\n",
      "(210, 5, 513)\n",
      "(210, 5, 513)\n",
      "(210, 5, 513)\n",
      "v_s_cmplx (1050, 513)\n",
      "S.shape (513, 1050)\n",
      "S_hat.shape (513, 1050)\n",
      "(158, 5, 513)\n",
      "(158, 5, 513)\n",
      "(158, 5, 513)\n",
      "v_s_cmplx (790, 513)\n",
      "S.shape (513, 790)\n",
      "S_hat.shape (513, 790)\n",
      "(198, 5, 513)\n",
      "(198, 5, 513)\n",
      "(198, 5, 513)\n",
      "v_s_cmplx (990, 513)\n",
      "S.shape (513, 990)\n",
      "S_hat.shape (513, 990)\n",
      "(206, 5, 513)\n",
      "(206, 5, 513)\n",
      "(206, 5, 513)\n",
      "v_s_cmplx (1030, 513)\n",
      "S.shape (513, 1030)\n",
      "S_hat.shape (513, 1030)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "v_s_cmplx (900, 513)\n",
      "S.shape (513, 900)\n",
      "S_hat.shape (513, 900)\n",
      "(214, 5, 513)\n",
      "(214, 5, 513)\n",
      "(214, 5, 513)\n",
      "v_s_cmplx (1070, 513)\n",
      "S.shape (513, 1070)\n",
      "S_hat.shape (513, 1070)\n",
      "(186, 5, 513)\n",
      "(186, 5, 513)\n",
      "(186, 5, 513)\n",
      "v_s_cmplx (930, 513)\n",
      "S.shape (513, 930)\n",
      "S_hat.shape (513, 930)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "v_s_cmplx (900, 513)\n",
      "S.shape (513, 900)\n",
      "S_hat.shape (513, 900)\n",
      "(222, 5, 513)\n",
      "(222, 5, 513)\n",
      "(222, 5, 513)\n",
      "v_s_cmplx (1110, 513)\n",
      "S.shape (513, 1110)\n",
      "S_hat.shape (513, 1110)\n",
      "(296, 5, 513)\n",
      "(296, 5, 513)\n",
      "(296, 5, 513)\n",
      "v_s_cmplx (1480, 513)\n",
      "S.shape (513, 1480)\n",
      "S_hat.shape (513, 1480)\n",
      "(230, 5, 513)\n",
      "(230, 5, 513)\n",
      "(230, 5, 513)\n",
      "v_s_cmplx (1150, 513)\n",
      "S.shape (513, 1150)\n",
      "S_hat.shape (513, 1150)\n",
      "(190, 5, 513)\n",
      "(190, 5, 513)\n",
      "(190, 5, 513)\n",
      "v_s_cmplx (950, 513)\n",
      "S.shape (513, 950)\n",
      "S_hat.shape (513, 950)\n",
      "(186, 5, 513)\n",
      "(186, 5, 513)\n",
      "(186, 5, 513)\n",
      "v_s_cmplx (930, 513)\n",
      "S.shape (513, 930)\n",
      "S_hat.shape (513, 930)\n",
      "(218, 5, 513)\n",
      "(218, 5, 513)\n",
      "(218, 5, 513)\n",
      "v_s_cmplx (1090, 513)\n",
      "S.shape (513, 1090)\n",
      "S_hat.shape (513, 1090)\n",
      "(202, 5, 513)\n",
      "(202, 5, 513)\n",
      "(202, 5, 513)\n",
      "v_s_cmplx (1010, 513)\n",
      "S.shape (513, 1010)\n",
      "S_hat.shape (513, 1010)\n",
      "(148, 5, 513)\n",
      "(148, 5, 513)\n",
      "(148, 5, 513)\n",
      "v_s_cmplx (740, 513)\n",
      "S.shape (513, 740)\n",
      "S_hat.shape (513, 740)\n",
      "(252, 5, 513)\n",
      "(252, 5, 513)\n",
      "(252, 5, 513)\n",
      "v_s_cmplx (1260, 513)\n",
      "S.shape (513, 1260)\n",
      "S_hat.shape (513, 1260)\n",
      "(152, 5, 513)\n",
      "(152, 5, 513)\n",
      "(152, 5, 513)\n",
      "v_s_cmplx (760, 513)\n",
      "S.shape (513, 760)\n",
      "S_hat.shape (513, 760)\n",
      "(118, 5, 513)\n",
      "(118, 5, 513)\n",
      "(118, 5, 513)\n",
      "v_s_cmplx (590, 513)\n",
      "S.shape (513, 590)\n",
      "S_hat.shape (513, 590)\n",
      "(232, 5, 513)\n",
      "(232, 5, 513)\n",
      "(232, 5, 513)\n",
      "v_s_cmplx (1160, 513)\n",
      "S.shape (513, 1160)\n",
      "S_hat.shape (513, 1160)\n",
      "(150, 5, 513)\n",
      "(150, 5, 513)\n",
      "(150, 5, 513)\n",
      "v_s_cmplx (750, 513)\n",
      "S.shape (513, 750)\n",
      "S_hat.shape (513, 750)\n",
      "(144, 5, 513)\n",
      "(144, 5, 513)\n",
      "(144, 5, 513)\n",
      "v_s_cmplx (720, 513)\n",
      "S.shape (513, 720)\n",
      "S_hat.shape (513, 720)\n",
      "(156, 5, 513)\n",
      "(156, 5, 513)\n",
      "(156, 5, 513)\n",
      "v_s_cmplx (780, 513)\n",
      "S.shape (513, 780)\n",
      "S_hat.shape (513, 780)\n",
      "(212, 5, 513)\n",
      "(212, 5, 513)\n",
      "(212, 5, 513)\n",
      "v_s_cmplx (1060, 513)\n",
      "S.shape (513, 1060)\n",
      "S_hat.shape (513, 1060)\n",
      "(194, 5, 513)\n",
      "(194, 5, 513)\n",
      "(194, 5, 513)\n",
      "v_s_cmplx (970, 513)\n",
      "S.shape (513, 970)\n",
      "S_hat.shape (513, 970)\n",
      "(164, 5, 513)\n",
      "(164, 5, 513)\n",
      "(164, 5, 513)\n",
      "v_s_cmplx (820, 513)\n",
      "S.shape (513, 820)\n",
      "S_hat.shape (513, 820)\n",
      "(346, 5, 513)\n",
      "(346, 5, 513)\n",
      "(346, 5, 513)\n",
      "v_s_cmplx (1730, 513)\n",
      "S.shape (513, 1730)\n",
      "S_hat.shape (513, 1730)\n",
      "(172, 5, 513)\n",
      "(172, 5, 513)\n",
      "(172, 5, 513)\n",
      "v_s_cmplx (860, 513)\n",
      "S.shape (513, 860)\n",
      "S_hat.shape (513, 860)\n",
      "(286, 5, 513)\n",
      "(286, 5, 513)\n",
      "(286, 5, 513)\n",
      "v_s_cmplx (1430, 513)\n",
      "S.shape (513, 1430)\n",
      "S_hat.shape (513, 1430)\n",
      "(186, 5, 513)\n",
      "(186, 5, 513)\n",
      "(186, 5, 513)\n",
      "v_s_cmplx (930, 513)\n",
      "S.shape (513, 930)\n",
      "S_hat.shape (513, 930)\n",
      "(216, 5, 513)\n",
      "(216, 5, 513)\n",
      "(216, 5, 513)\n",
      "v_s_cmplx (1080, 513)\n",
      "S.shape (513, 1080)\n",
      "S_hat.shape (513, 1080)\n",
      "(150, 5, 513)\n",
      "(150, 5, 513)\n",
      "(150, 5, 513)\n",
      "v_s_cmplx (750, 513)\n",
      "S.shape (513, 750)\n",
      "S_hat.shape (513, 750)\n",
      "(132, 5, 513)\n",
      "(132, 5, 513)\n",
      "(132, 5, 513)\n",
      "v_s_cmplx (660, 513)\n",
      "S.shape (513, 660)\n",
      "S_hat.shape (513, 660)\n",
      "(280, 5, 513)\n",
      "(280, 5, 513)\n",
      "(280, 5, 513)\n",
      "v_s_cmplx (1400, 513)\n",
      "S.shape (513, 1400)\n",
      "S_hat.shape (513, 1400)\n",
      "(148, 5, 513)\n",
      "(148, 5, 513)\n",
      "(148, 5, 513)\n",
      "v_s_cmplx (740, 513)\n",
      "S.shape (513, 740)\n",
      "S_hat.shape (513, 740)\n",
      "(190, 5, 513)\n",
      "(190, 5, 513)\n",
      "(190, 5, 513)\n",
      "v_s_cmplx (950, 513)\n",
      "S.shape (513, 950)\n",
      "S_hat.shape (513, 950)\n",
      "(148, 5, 513)\n",
      "(148, 5, 513)\n",
      "(148, 5, 513)\n",
      "v_s_cmplx (740, 513)\n",
      "S.shape (513, 740)\n",
      "S_hat.shape (513, 740)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140, 5, 513)\n",
      "(140, 5, 513)\n",
      "(140, 5, 513)\n",
      "v_s_cmplx (700, 513)\n",
      "S.shape (513, 700)\n",
      "S_hat.shape (513, 700)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "v_s_cmplx (900, 513)\n",
      "S.shape (513, 900)\n",
      "S_hat.shape (513, 900)\n",
      "(142, 5, 513)\n",
      "(142, 5, 513)\n",
      "(142, 5, 513)\n",
      "v_s_cmplx (710, 513)\n",
      "S.shape (513, 710)\n",
      "S_hat.shape (513, 710)\n",
      "(108, 5, 513)\n",
      "(108, 5, 513)\n",
      "(108, 5, 513)\n",
      "v_s_cmplx (540, 513)\n",
      "S.shape (513, 540)\n",
      "S_hat.shape (513, 540)\n",
      "(184, 5, 513)\n",
      "(184, 5, 513)\n",
      "(184, 5, 513)\n",
      "v_s_cmplx (920, 513)\n",
      "S.shape (513, 920)\n",
      "S_hat.shape (513, 920)\n",
      "(202, 5, 513)\n",
      "(202, 5, 513)\n",
      "(202, 5, 513)\n",
      "v_s_cmplx (1010, 513)\n",
      "S.shape (513, 1010)\n",
      "S_hat.shape (513, 1010)\n",
      "(152, 5, 513)\n",
      "(152, 5, 513)\n",
      "(152, 5, 513)\n",
      "v_s_cmplx (760, 513)\n",
      "S.shape (513, 760)\n",
      "S_hat.shape (513, 760)\n",
      "(174, 5, 513)\n",
      "(174, 5, 513)\n",
      "(174, 5, 513)\n",
      "v_s_cmplx (870, 513)\n",
      "S.shape (513, 870)\n",
      "S_hat.shape (513, 870)\n",
      "(140, 5, 513)\n",
      "(140, 5, 513)\n",
      "(140, 5, 513)\n",
      "v_s_cmplx (700, 513)\n",
      "S.shape (513, 700)\n",
      "S_hat.shape (513, 700)\n",
      "(228, 5, 513)\n",
      "(228, 5, 513)\n",
      "(228, 5, 513)\n",
      "v_s_cmplx (1140, 513)\n",
      "S.shape (513, 1140)\n",
      "S_hat.shape (513, 1140)\n",
      "(202, 5, 513)\n",
      "(202, 5, 513)\n",
      "(202, 5, 513)\n",
      "v_s_cmplx (1010, 513)\n",
      "S.shape (513, 1010)\n",
      "S_hat.shape (513, 1010)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "v_s_cmplx (900, 513)\n",
      "S.shape (513, 900)\n",
      "S_hat.shape (513, 900)\n",
      "(242, 5, 513)\n",
      "(242, 5, 513)\n",
      "(242, 5, 513)\n",
      "v_s_cmplx (1210, 513)\n",
      "S.shape (513, 1210)\n",
      "S_hat.shape (513, 1210)\n",
      "(144, 5, 513)\n",
      "(144, 5, 513)\n",
      "(144, 5, 513)\n",
      "v_s_cmplx (720, 513)\n",
      "S.shape (513, 720)\n",
      "S_hat.shape (513, 720)\n",
      "(356, 5, 513)\n",
      "(356, 5, 513)\n",
      "(356, 5, 513)\n",
      "v_s_cmplx (1780, 513)\n",
      "S.shape (513, 1780)\n",
      "S_hat.shape (513, 1780)\n",
      "(234, 5, 513)\n",
      "(234, 5, 513)\n",
      "(234, 5, 513)\n",
      "v_s_cmplx (1170, 513)\n",
      "S.shape (513, 1170)\n",
      "S_hat.shape (513, 1170)\n",
      "(184, 5, 513)\n",
      "(184, 5, 513)\n",
      "(184, 5, 513)\n",
      "v_s_cmplx (920, 513)\n",
      "S.shape (513, 920)\n",
      "S_hat.shape (513, 920)\n",
      "(238, 5, 513)\n",
      "(238, 5, 513)\n",
      "(238, 5, 513)\n",
      "v_s_cmplx (1190, 513)\n",
      "S.shape (513, 1190)\n",
      "S_hat.shape (513, 1190)\n",
      "(340, 5, 513)\n",
      "(340, 5, 513)\n",
      "(340, 5, 513)\n",
      "v_s_cmplx (1700, 513)\n",
      "S.shape (513, 1700)\n",
      "S_hat.shape (513, 1700)\n",
      "(220, 5, 513)\n",
      "(220, 5, 513)\n",
      "(220, 5, 513)\n",
      "v_s_cmplx (1100, 513)\n",
      "S.shape (513, 1100)\n",
      "S_hat.shape (513, 1100)\n",
      "(260, 5, 513)\n",
      "(260, 5, 513)\n",
      "(260, 5, 513)\n",
      "v_s_cmplx (1300, 513)\n",
      "S.shape (513, 1300)\n",
      "S_hat.shape (513, 1300)\n",
      "(216, 5, 513)\n",
      "(216, 5, 513)\n",
      "(216, 5, 513)\n",
      "v_s_cmplx (1080, 513)\n",
      "S.shape (513, 1080)\n",
      "S_hat.shape (513, 1080)\n",
      "(200, 5, 513)\n",
      "(200, 5, 513)\n",
      "(200, 5, 513)\n",
      "v_s_cmplx (1000, 513)\n",
      "S.shape (513, 1000)\n",
      "S_hat.shape (513, 1000)\n",
      "(124, 5, 513)\n",
      "(124, 5, 513)\n",
      "(124, 5, 513)\n",
      "v_s_cmplx (620, 513)\n",
      "S.shape (513, 620)\n",
      "S_hat.shape (513, 620)\n",
      "(192, 5, 513)\n",
      "(192, 5, 513)\n",
      "(192, 5, 513)\n",
      "v_s_cmplx (960, 513)\n",
      "S.shape (513, 960)\n",
      "S_hat.shape (513, 960)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "v_s_cmplx (900, 513)\n",
      "S.shape (513, 900)\n",
      "S_hat.shape (513, 900)\n",
      "(282, 5, 513)\n",
      "(282, 5, 513)\n",
      "(282, 5, 513)\n",
      "v_s_cmplx (1410, 513)\n",
      "S.shape (513, 1410)\n",
      "S_hat.shape (513, 1410)\n",
      "(220, 5, 513)\n",
      "(220, 5, 513)\n",
      "(220, 5, 513)\n",
      "v_s_cmplx (1100, 513)\n",
      "S.shape (513, 1100)\n",
      "S_hat.shape (513, 1100)\n",
      "(194, 5, 513)\n",
      "(194, 5, 513)\n",
      "(194, 5, 513)\n",
      "v_s_cmplx (970, 513)\n",
      "S.shape (513, 970)\n",
      "S_hat.shape (513, 970)\n",
      "(302, 5, 513)\n",
      "(302, 5, 513)\n",
      "(302, 5, 513)\n",
      "v_s_cmplx (1510, 513)\n",
      "S.shape (513, 1510)\n",
      "S_hat.shape (513, 1510)\n",
      "(200, 5, 513)\n",
      "(200, 5, 513)\n",
      "(200, 5, 513)\n",
      "v_s_cmplx (1000, 513)\n",
      "S.shape (513, 1000)\n",
      "S_hat.shape (513, 1000)\n",
      "(160, 5, 513)\n",
      "(160, 5, 513)\n",
      "(160, 5, 513)\n",
      "v_s_cmplx (800, 513)\n",
      "S.shape (513, 800)\n",
      "S_hat.shape (513, 800)\n",
      "(158, 5, 513)\n",
      "(158, 5, 513)\n",
      "(158, 5, 513)\n",
      "v_s_cmplx (790, 513)\n",
      "S.shape (513, 790)\n",
      "S_hat.shape (513, 790)\n",
      "(154, 5, 513)\n",
      "(154, 5, 513)\n",
      "(154, 5, 513)\n",
      "v_s_cmplx (770, 513)\n",
      "S.shape (513, 770)\n",
      "S_hat.shape (513, 770)\n",
      "(166, 5, 513)\n",
      "(166, 5, 513)\n",
      "(166, 5, 513)\n",
      "v_s_cmplx (830, 513)\n",
      "S.shape (513, 830)\n",
      "S_hat.shape (513, 830)\n",
      "(224, 5, 513)\n",
      "(224, 5, 513)\n",
      "(224, 5, 513)\n",
      "v_s_cmplx (1120, 513)\n",
      "S.shape (513, 1120)\n",
      "S_hat.shape (513, 1120)\n",
      "(294, 5, 513)\n",
      "(294, 5, 513)\n",
      "(294, 5, 513)\n",
      "v_s_cmplx (1470, 513)\n",
      "S.shape (513, 1470)\n",
      "S_hat.shape (513, 1470)\n",
      "(150, 5, 513)\n",
      "(150, 5, 513)\n",
      "(150, 5, 513)\n",
      "v_s_cmplx (750, 513)\n",
      "S.shape (513, 750)\n",
      "S_hat.shape (513, 750)\n",
      "(196, 5, 513)\n",
      "(196, 5, 513)\n",
      "(196, 5, 513)\n",
      "v_s_cmplx (980, 513)\n",
      "S.shape (513, 980)\n",
      "S_hat.shape (513, 980)\n",
      "(148, 5, 513)\n",
      "(148, 5, 513)\n",
      "(148, 5, 513)\n",
      "v_s_cmplx (740, 513)\n",
      "S.shape (513, 740)\n",
      "S_hat.shape (513, 740)\n",
      "(234, 5, 513)\n",
      "(234, 5, 513)\n",
      "(234, 5, 513)\n",
      "v_s_cmplx (1170, 513)\n",
      "S.shape (513, 1170)\n",
      "S_hat.shape (513, 1170)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "v_s_cmplx (900, 513)\n",
      "S.shape (513, 900)\n",
      "S_hat.shape (513, 900)\n",
      "3.5695887005693674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.526181781952143"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_s = 0.0\n",
    "sum_s_diff = 0.0\n",
    "\n",
    "for v_s,v_x,v_x_cmplx,v_s_cmplx in next_batchXSCmplx(data_val_s,data_val_x,val_complx_X,val_complx_S):\n",
    "    \n",
    "    print(v_s.shape)\n",
    "    print(v_x.shape)\n",
    "    print(v_x_cmplx.shape)\n",
    "    print('v_s_cmplx',v_s_cmplx.shape)\n",
    "    \n",
    "    mask = model.predict(v_x)\n",
    "    S_hat = (mask) * v_x_cmplx\n",
    "    S_hat = S_hat.reshape(-1,513).T\n",
    "    S = v_s_cmplx.T\n",
    "#     S=S.reshape(-1,513)\n",
    "    \n",
    "    print('S.shape',S.shape)\n",
    "    print('S_hat.shape',S_hat.shape)\n",
    "\n",
    "    S_org = librosa.istft(S, hop_length=512)\n",
    "    S_pred = librosa.istft(S_hat, hop_length=512)\n",
    "\n",
    "    sum_s += np.sum(S_org*S_org)\n",
    "    sum_s_diff += np.sum((S_org-S_pred)*(S_org-S_pred))\n",
    "    \n",
    "acc = sum_s/ sum_s_diff\n",
    "print(acc)\n",
    "\n",
    "10*np.log10(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write into audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_audio(file_name):\n",
    "    for i in range(len(file_name)):\n",
    "        audio_fname=file_name[i].replace(\"/opt/e533/timit-homework/te/\", \"\").replace(\".wav\",\"\")\n",
    "        print(audio_fname)\n",
    "        sn, sr=librosa.load(file_name[i], sr=None)\n",
    "        Sn=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "        mag_Sn=np.abs(Sn)\n",
    "        print(mag_Sn.shape)\n",
    "#         mag_Sn=mag_Sn.reshape(-1, 5, 513)\n",
    "        Stest_hat=model.predict(mag_Sn.reshape(-1, 5, 513))\n",
    "        Stest_hat=Stest_hat.reshape(-1,513)\n",
    "        S_hat=(Sn/mag_Sn)*Stest_hat.T\n",
    "\n",
    "        S_time=librosa.istft(S_hat, hop_length=512)\n",
    "        audio_fname=audio_fname + \"_recons.wav\"\n",
    "        print(audio_fname)\n",
    "        librosa.output.write_wav(audio_fname, S_time, sr)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tex0000\n",
      "(513, 110)\n",
      "tex0000_recons.wav\n",
      "tex0001\n",
      "(513, 110)\n",
      "tex0001_recons.wav\n",
      "tex0002\n",
      "(513, 110)\n",
      "tex0002_recons.wav\n",
      "tex0003\n",
      "(513, 110)\n",
      "tex0003_recons.wav\n",
      "tex0004\n",
      "(513, 110)\n",
      "tex0004_recons.wav\n",
      "tex0005\n",
      "(513, 110)\n",
      "tex0005_recons.wav\n",
      "tex0006\n",
      "(513, 110)\n",
      "tex0006_recons.wav\n",
      "tex0007\n",
      "(513, 110)\n",
      "tex0007_recons.wav\n",
      "tex0008\n",
      "(513, 110)\n",
      "tex0008_recons.wav\n",
      "tex0009\n",
      "(513, 110)\n",
      "tex0009_recons.wav\n"
     ]
    }
   ],
   "source": [
    "write_audio(fname_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513, 65)\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "sn, sr=librosa.load(fname_trn[0], sr=None)\n",
    "Sn=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "trn_arr=np.abs(Sn)\n",
    "print(trn_arr.shape)\n",
    "sx, sr=librosa.load(fname_trx[0], sr=None)\n",
    "Sx=librosa.stft(sx, n_fft=1024, hop_length=512)\n",
    "trx_arr=np.abs(Sx)\n",
    "\n",
    "ss, sr=librosa.load(fname_trs[0], sr=None)\n",
    "Ss=librosa.stft(ss, n_fft=1024, hop_length=512)\n",
    "trs_arr=np.abs(Ss)\n",
    "for i in range(1,1200):\n",
    "\n",
    "    sn, sr=librosa.load(fname_trn[i], sr=None)\n",
    "    Sn=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "    mag_Sn=np.abs(Sn)\n",
    "    trn_arr=np.concatenate((trn_arr, mag_Sn), axis=1)\n",
    "#     print(Sn.shape)\n",
    "\n",
    "    sx, sr=librosa.load(fname_trx[i], sr=None)\n",
    "    Sx=librosa.stft(sx, n_fft=1024, hop_length=512)\n",
    "    mag_Sx=np.abs(Sx)\n",
    "    trx_arr=np.concatenate((trx_arr, mag_Sx), axis=1)\n",
    "    \n",
    "    ss, sr=librosa.load(fname_trs[i], sr=None)\n",
    "    Ss=librosa.stft(ss, n_fft=1024, hop_length=512)\n",
    "    mag_Ss=np.abs(Ss)\n",
    "    trs_arr=np.concatenate((trs_arr, mag_Ss), axis=1)\n",
    "# print(Sn.shape)\n",
    "\n",
    "# sn, sr=librosa.load(fname_trn[129], sr=None)\n",
    "# Sn=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "\n",
    "# print(Sn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(513, 118550)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_train_M = 1*(trs_arr>trn_arr)\n",
    "DATA_train_M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((513, 10, 11855), (513, 10, 11855), (513, 10, 11855), (513, 10, 11855))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_arr = trn_arr.reshape( (513,10,-1))\n",
    "trx_arr = trx_arr.reshape( (513,10,-1))\n",
    "trs_arr = trs_arr.reshape( (513,10,-1))\n",
    "DATA_train_M = DATA_train_M.reshape( (513,10,-1))\n",
    "\n",
    "trn_arr.shape, trx_arr.shape, trs_arr.shape, DATA_train_M.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "sn, sr=librosa.load(fname_val_n[0], sr=None)\n",
    "Sn=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "val_n_arr=np.abs(Sn)\n",
    "\n",
    "# print(trn_arr.shape)\n",
    "sx, sr=librosa.load(fname_val_x[0], sr=None)\n",
    "Sx=librosa.stft(sx, n_fft=1024, hop_length=512)\n",
    "val_x_arr=np.abs(Sx)\n",
    "\n",
    "ss, sr=librosa.load(fname_val_s[0], sr=None)\n",
    "Ss=librosa.stft(ss, n_fft=1024, hop_length=512)\n",
    "val_s_arr=np.abs(Ss)\n",
    "for i in range(1,len(fname_val_n)):\n",
    "\n",
    "    sn, sr=librosa.load(fname_val_n[i], sr=None)\n",
    "    Sn=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "    mag_Sn=np.abs(Sn)\n",
    "    val_n_arr=np.concatenate((val_n_arr, mag_Sn), axis=1)\n",
    "#     print(Sn.shape)\n",
    "\n",
    "    sx, sr=librosa.load(fname_val_x[i], sr=None)\n",
    "    Sx=librosa.stft(sx, n_fft=1024, hop_length=512)\n",
    "    mag_Sx=np.abs(Sx)\n",
    "    val_x_arr=np.concatenate((val_x_arr, mag_Sx), axis=1)\n",
    "    \n",
    "    ss, sr=librosa.load(fname_val_s[i], sr=None)\n",
    "    Ss=librosa.stft(ss, n_fft=1024, hop_length=512)\n",
    "    mag_Ss=np.abs(Ss)\n",
    "    val_s_arr=np.concatenate((val_s_arr, mag_Ss), axis=1)\n",
    "# print(Sn.shape)\n",
    "\n",
    "# sn, sr=librosa.load(fname_trn[129], sr=None)\n",
    "# Sn=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "\n",
    "# print(Sn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(513, 118550)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_val_M = 1*(val_s_arr>val_n_arr)\n",
    "DATA_val_M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((513, 10, 11855), (513, 10, 11855), (513, 10, 11855), (513, 10, 11855))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_n_arr = val_n_arr.reshape( (513,10,-1))\n",
    "val_x_arr = val_x_arr.reshape( (513,10,-1))\n",
    "val_s_arr = val_s_arr.reshape( (513,10,-1))\n",
    "DATA_val_M = DATA_val_M.reshape( (513,10,-1))\n",
    "\n",
    "val_n_arr.shape, val_x_arr.shape, val_s_arr.shape, DATA_val_M.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn, sr=librosa.load(fname_test[0], sr=None)\n",
    "Sn=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "test_arr=np.abs(Sn)\n",
    "\n",
    "# print(trn_arr.shape)\n",
    "# sx, sr=librosa.load(fname_val_x[0], sr=None)\n",
    "# Sx=librosa.stft(sx, n_fft=1024, hop_length=512)\n",
    "# val_x_arr=np.abs(Sx)\n",
    "\n",
    "# ss, sr=librosa.load(fname_val_s[0], sr=None)\n",
    "# Ss=librosa.stft(ss, n_fft=1024, hop_length=512)\n",
    "# val_s_arr=np.abs(Ss)\n",
    "for i in range(1,len(fname_test)):\n",
    "\n",
    "    sn, sr=librosa.load(fname_test[i], sr=None)\n",
    "    Sn=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "    mag_Sn=np.abs(Sn)\n",
    "    test_arr=np.concatenate((test_arr, mag_Sn), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(513, 10, 4415)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_arr.shape\n",
    "test_arr = test_arr.reshape( (513,10,-1))\n",
    "test_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('DATA_train_s.npy', trs_arr)\n",
    "np.save('DATA_train_n.npy', trn_arr)\n",
    "np.save('DATA_train_x.npy', trx_arr)\n",
    "np.save('DATA_train_M.npy', DATA_train_M)\n",
    "\n",
    "np.save('DATA_val_s.npy', val_s_arr)\n",
    "np.save('DATA_val_n.npy', val_n_arr)\n",
    "np.save('DATA_val_x.npy', val_x_arr)\n",
    "np.save('DATA_val_M.npy', DATA_val_M)\n",
    "\n",
    "np.save('DATA_test_x.npy', test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_train_s_f = np.load('DATA_train_s.npy')\n",
    "DATA_train_n_f = np.load('DATA_train_n.npy')\n",
    "DATA_train_x_f = np.load('DATA_train_x.npy')\n",
    "DATA_train_M_f = np.load('DATA_train_M.npy')\n",
    "\n",
    "DATA_val_s_f = np.load('DATA_val_s.npy')\n",
    "DATA_val_n_f = np.load('DATA_val_n.npy')\n",
    "DATA_val_x_f = np.load('DATA_val_x.npy')\n",
    "DATA_val_M_f = np.load('DATA_val_M.npy')\n",
    "\n",
    "DATA_test_x_f = np.load('DATA_test_x.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_val_M_f[:10, :3, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11855, 10, 513, 2)\n",
      "(11855, 10, 513, 2)\n"
     ]
    }
   ],
   "source": [
    "tr_categorical_labels = to_categorical(DATA_train_M_f.T, num_classes=2)\n",
    "print(tr_categorical_labels.shape)\n",
    "\n",
    "v_categorical_labels = to_categorical(DATA_val_M_f.T, num_classes=2)\n",
    "print(v_categorical_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 10, 10)            20960     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 10, 10)            0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10, 513)           5643      \n",
      "=================================================================\n",
      "Total params: 26,603\n",
      "Trainable params: 26,603\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/Anaconda3-5.0.1/envs/e533/lib/python3.6/site-packages/keras/models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11855 samples, validate on 11855 samples\n",
      "Epoch 1/25\n",
      "11855/11855 [==============================] - 20s 2ms/step - loss: 0.2133 - acc: 0.0017 - val_loss: 0.1920 - val_acc: 0.0018\n",
      "Epoch 2/25\n",
      "11855/11855 [==============================] - 20s 2ms/step - loss: 0.1944 - acc: 0.0026 - val_loss: 0.1836 - val_acc: 0.0022\n",
      "Epoch 3/25\n",
      "11855/11855 [==============================] - 19s 2ms/step - loss: 0.1891 - acc: 0.0033 - val_loss: 0.1782 - val_acc: 0.0022\n",
      "Epoch 4/25\n",
      "11855/11855 [==============================] - 20s 2ms/step - loss: 0.1859 - acc: 0.0049 - val_loss: 0.1751 - val_acc: 0.0035\n",
      "Epoch 5/25\n",
      "11855/11855 [==============================] - 19s 2ms/step - loss: 0.1838 - acc: 0.0076 - val_loss: 0.1735 - val_acc: 0.0071\n",
      "Epoch 6/25\n",
      "11855/11855 [==============================] - 19s 2ms/step - loss: 0.1824 - acc: 0.0124 - val_loss: 0.1718 - val_acc: 0.0091\n",
      "Epoch 7/25\n",
      "11855/11855 [==============================] - 19s 2ms/step - loss: 0.1812 - acc: 0.0162 - val_loss: 0.1712 - val_acc: 0.0165\n",
      "Epoch 8/25\n",
      "11855/11855 [==============================] - 19s 2ms/step - loss: 0.1803 - acc: 0.0190 - val_loss: 0.1704 - val_acc: 0.0194\n",
      "Epoch 9/25\n",
      "11855/11855 [==============================] - 19s 2ms/step - loss: 0.1796 - acc: 0.0208 - val_loss: 0.1697 - val_acc: 0.0236\n",
      "Epoch 10/25\n",
      "11855/11855 [==============================] - 19s 2ms/step - loss: 0.1790 - acc: 0.0223 - val_loss: 0.1690 - val_acc: 0.0182\n",
      "Epoch 11/25\n",
      "11855/11855 [==============================] - 19s 2ms/step - loss: 0.1782 - acc: 0.0229 - val_loss: 0.1679 - val_acc: 0.0211\n",
      "Epoch 12/25\n",
      "11855/11855 [==============================] - 19s 2ms/step - loss: 0.1779 - acc: 0.0235 - val_loss: 0.1677 - val_acc: 0.0211\n",
      "Epoch 13/25\n",
      "11855/11855 [==============================] - 19s 2ms/step - loss: 0.1775 - acc: 0.0231 - val_loss: 0.1679 - val_acc: 0.0221\n",
      "Epoch 14/25\n",
      "11855/11855 [==============================] - 19s 2ms/step - loss: 0.1771 - acc: 0.0236 - val_loss: 0.1678 - val_acc: 0.0223\n",
      "Epoch 15/25\n",
      "11855/11855 [==============================] - 19s 2ms/step - loss: 0.1768 - acc: 0.0252 - val_loss: 0.1677 - val_acc: 0.0235\n",
      "Epoch 16/25\n",
      "11855/11855 [==============================] - 19s 2ms/step - loss: 0.1763 - acc: 0.0242 - val_loss: 0.1674 - val_acc: 0.0237\n",
      "Epoch 17/25\n",
      "11855/11855 [==============================] - 19s 2ms/step - loss: 0.1761 - acc: 0.0249 - val_loss: 0.1668 - val_acc: 0.0276\n",
      "Epoch 18/25\n",
      "11855/11855 [==============================] - 19s 2ms/step - loss: 0.1761 - acc: 0.0245 - val_loss: 0.1669 - val_acc: 0.0220\n",
      "Epoch 19/25\n",
      "11855/11855 [==============================] - 25s 2ms/step - loss: 0.1755 - acc: 0.0248 - val_loss: 0.1661 - val_acc: 0.0241\n",
      "Epoch 20/25\n",
      "11855/11855 [==============================] - 26s 2ms/step - loss: 0.1753 - acc: 0.0241 - val_loss: 0.1657 - val_acc: 0.0220\n",
      "Epoch 21/25\n",
      "11855/11855 [==============================] - 24s 2ms/step - loss: 0.1752 - acc: 0.0244 - val_loss: 0.1653 - val_acc: 0.0241\n",
      "Epoch 22/25\n",
      "11855/11855 [==============================] - 24s 2ms/step - loss: 0.1751 - acc: 0.0244 - val_loss: 0.1659 - val_acc: 0.0257\n",
      "Epoch 23/25\n",
      "11855/11855 [==============================] - 25s 2ms/step - loss: 0.1747 - acc: 0.0247 - val_loss: 0.1653 - val_acc: 0.0217\n",
      "Epoch 24/25\n",
      "11855/11855 [==============================] - 25s 2ms/step - loss: 0.1744 - acc: 0.0237 - val_loss: 0.1660 - val_acc: 0.0235\n",
      "Epoch 25/25\n",
      "11855/11855 [==============================] - 24s 2ms/step - loss: 0.1744 - acc: 0.0237 - val_loss: 0.1658 - val_acc: 0.0265\n",
      "Accuracy: 2.65%\n"
     ]
    }
   ],
   "source": [
    "Max_RNN = 10\n",
    "# create the model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(Max_RNN, return_sequences=True, input_shape=(Max_RNN,513)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(513, activation='sigmoid'))\n",
    "          \n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# model.fit(DATA_train_x_f.T, tr_categorical_labels, validation_data=(DATA_val_x_f.T,v_categorical_labels), shuffle=True, nb_epoch=100, batch_size=10)\n",
    "# # Final evaluation of the model\n",
    "# scores = model.evaluate(DATA_val_x_f.T, v_categorical_labels, verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "model.fit(DATA_train_x_f.T, DATA_train_M_f.T, validation_data=(DATA_val_x_f.T,DATA_val_M_f.T), shuffle=True, nb_epoch=25, batch_size=10)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(DATA_val_x_f.T, DATA_val_M_f.T, verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "sx, sr=librosa.load(fname_trx[0], sr=None)\n",
    "Sx=librosa.stft(sx, n_fft=1024, hop_length=512)\n",
    "X_tr=Sx\n",
    "\n",
    "for i in range(1,len(fname_trx)):\n",
    "\n",
    "    sx, sr=librosa.load(fname_trx[i], sr=None)\n",
    "    Sx=librosa.stft(sx, n_fft=1024, hop_length=512)\n",
    "#     mag_Sx=np.abs(Sx)\n",
    "    X_tr=np.concatenate((X_tr, Sx), axis=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "sx, sr=librosa.load(fname_val_x[0], sr=None)\n",
    "Sx=librosa.stft(sx, n_fft=1024, hop_length=512)\n",
    "X_val=Sx\n",
    "\n",
    "for i in range(1,len(fname_val_x)):\n",
    "\n",
    "    sx, sr=librosa.load(fname_val_x[i], sr=None)\n",
    "    Sx=librosa.stft(sx, n_fft=1024, hop_length=512)\n",
    "#     mag_Sx=np.abs(Sx)\n",
    "    X_val=np.concatenate((X_val, Sx), axis=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11855, 10, 513)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = model.predict(DATA_val_x_f.T)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_hat = (labels.T) * DATA_val_x_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_org = (DATA_val_s_f).flatten()\n",
    "S_pred = S_hat.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.074747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.101006865501404"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = np.sum(S_org*S_org)/ np.sum((S_org-S_pred)*(S_org-S_pred))\n",
    "print(acc)\n",
    "\n",
    "10*np.log10(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
