{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENGR-E 533: Deep Learning Systems\n",
    "## Homework 2\n",
    "\n",
    "### Khandokar Md. Nayem (knayem@iu.edu)\n",
    "### Apr 3, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary files and set environment parameters\n",
    "My assigned Node is `r-005` and GPU `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Dropout, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, TimeDistributed\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables\n",
    "\n",
    "Directory names, file formates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_directory = '/N/u/knayem/data/timit-homework/'\n",
    "\n",
    "PATH_train = 'tr/'\n",
    "PATH_val = 'v/'\n",
    "PATH_test = 'te/'\n",
    "PATH_denoise = 'te/'\n",
    "\n",
    "CLEAN_format_train = 'trs*.wav'\n",
    "NOISE_format_train = 'trn*.wav'\n",
    "MIX_format_train = 'trx*.wav'\n",
    "\n",
    "CLEAN_format_val = 'vs*.wav'\n",
    "NOISE_format_val = 'vn*.wav'\n",
    "MIX_format_val = 'vx*.wav'\n",
    "\n",
    "MIX_format_test = 'tex*.wav'\n",
    "\n",
    "Max_RNN = 5 # Number of Time Stamps in a RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Names\n",
    "\n",
    "Since there are huge number of files for both training, validation and testing, for convinience we write them in files. These are the name of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_S = 'TRAIN_S.txt'\n",
    "TRAIN_N = 'TRAIN_N.txt'\n",
    "TRAIN_X_cmplx = 'TRAIN_X_cmplx.txt'\n",
    "TRAIN_X = 'TRAIN_X.txt'\n",
    "\n",
    "VAL_S = 'VAL_S.txt'\n",
    "VAL_S_cmplx = 'VAL_S_cmplx.txt'\n",
    "VAL_N = 'VAL_N.txt'\n",
    "VAL_X_cmplx = 'VAL_X_cmplx.txt'\n",
    "VAL_X = 'VAL_X.txt'\n",
    "\n",
    "\n",
    "TEST_S = 'TEST_S.txt'\n",
    "TEST_N = 'TEST_N.txt'\n",
    "TEST_X_cmplx = 'TEST_X_cmplx.txt'\n",
    "TEST_X = 'TEST_X.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a .wav file\n",
    "A function to get the Magnitude Spectum of a file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprossed_data(file_name):\n",
    "    \n",
    "    sn, sr=librosa.load(file_name, sr=None)\n",
    "    X=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "    X_mag = np.abs(X)\n",
    "#     print('X_mag',X_mag.shape)\n",
    "    return X, X_mag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Train Data  \n",
    "\n",
    "Here we open the files, and read the Train Dataset. We need to run this porion for the first time only. After we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_train_s = None\n",
    "# DATA_train_n = None\n",
    "# DATA_train_x = None\n",
    "\n",
    "with open(TRAIN_S,'wb') as fs, open(TRAIN_N,'wb') as fn, open(TRAIN_X_cmplx,'wb') as fx_cmplx, open(TRAIN_X,'wb') as fx: \n",
    "    \n",
    "    for file_s, file_n, file_x in zip(sorted(glob.glob(PATH_directory+PATH_train+CLEAN_format_train)),sorted(glob.glob(PATH_directory+PATH_train+NOISE_format_train)),sorted(glob.glob(PATH_directory+PATH_train+MIX_format_train))):\n",
    "        _,s = preprossed_data(file_s)\n",
    "        np.savetxt(fs, s, fmt='%.5f')\n",
    "        fs.write(b'\\n')\n",
    "        \n",
    "        _,n = preprossed_data(file_n)\n",
    "        np.savetxt(fn, n, fmt='%.5f')\n",
    "        fn.write(b'\\n')\n",
    "        \n",
    "        x_cmplx,x = preprossed_data(file_x)\n",
    "        np.savetxt(fx, x, fmt='%.5f')\n",
    "        fx.write(b'\\n')\n",
    "        \n",
    "        np.savetxt(fx_cmplx, x_cmplx, fmt='%.5f')\n",
    "        fx_cmplx.write(b'\\n')\n",
    "        \n",
    "#         DATA_train_s = np.array(preprossed_data(file_s)) if DATA_train_s is None else np.concatenate( (DATA_train_s,preprossed_data(file_s)),axis=1)\n",
    "#         DATA_train_n = np.array(preprossed_data(file_n)) if DATA_train_n is None else np.concatenate( (DATA_train_n,preprossed_data(file_n)),axis=1)\n",
    "#         DATA_train_x = np.array(preprossed_data(file_x)) if DATA_train_x is None else np.concatenate( (DATA_train_x,preprossed_data(file_x)),axis=1)\n",
    "\n",
    "fs.close()\n",
    "fn.close()\n",
    "fx_cmplx.close()\n",
    "fx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Validate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ...\n",
      "2 ...\n",
      "3 ...\n",
      "4 ...\n",
      "5 ...\n",
      "6 ...\n",
      "7 ...\n",
      "8 ...\n",
      "9 ...\n",
      "10 ...\n",
      "11 ...\n",
      "12 ...\n",
      "13 ...\n",
      "14 ...\n",
      "15 ...\n",
      "16 ...\n",
      "17 ...\n",
      "18 ...\n",
      "19 ...\n",
      "20 ...\n",
      "21 ...\n",
      "22 ...\n",
      "23 ...\n",
      "24 ...\n",
      "25 ...\n",
      "26 ...\n",
      "27 ...\n",
      "28 ...\n",
      "29 ...\n",
      "30 ...\n",
      "31 ...\n",
      "32 ...\n",
      "33 ...\n",
      "34 ...\n",
      "35 ...\n",
      "36 ...\n",
      "37 ...\n",
      "38 ...\n",
      "39 ...\n",
      "40 ...\n",
      "41 ...\n",
      "42 ...\n",
      "43 ...\n",
      "44 ...\n",
      "45 ...\n",
      "46 ...\n",
      "47 ...\n",
      "48 ...\n",
      "49 ...\n",
      "50 ...\n",
      "51 ...\n",
      "52 ...\n",
      "53 ...\n",
      "54 ...\n",
      "55 ...\n",
      "56 ...\n",
      "57 ...\n",
      "58 ...\n",
      "59 ...\n",
      "60 ...\n",
      "61 ...\n",
      "62 ...\n",
      "63 ...\n",
      "64 ...\n",
      "65 ...\n",
      "66 ...\n",
      "67 ...\n",
      "68 ...\n",
      "69 ...\n",
      "70 ...\n",
      "71 ...\n",
      "72 ...\n",
      "73 ...\n",
      "74 ...\n",
      "75 ...\n",
      "76 ...\n",
      "77 ...\n",
      "78 ...\n",
      "79 ...\n",
      "80 ...\n",
      "81 ...\n",
      "82 ...\n",
      "83 ...\n",
      "84 ...\n",
      "85 ...\n",
      "86 ...\n",
      "87 ...\n",
      "88 ...\n",
      "89 ...\n",
      "90 ...\n",
      "91 ...\n",
      "92 ...\n",
      "93 ...\n",
      "94 ...\n",
      "95 ...\n",
      "96 ...\n",
      "97 ...\n",
      "98 ...\n",
      "99 ...\n",
      "100 ...\n",
      "101 ...\n",
      "102 ...\n",
      "103 ...\n",
      "104 ...\n",
      "105 ...\n",
      "106 ...\n",
      "107 ...\n",
      "108 ...\n",
      "109 ...\n",
      "110 ...\n",
      "111 ...\n",
      "112 ...\n",
      "113 ...\n",
      "114 ...\n",
      "115 ...\n",
      "116 ...\n",
      "117 ...\n",
      "118 ...\n",
      "119 ...\n",
      "120 ...\n",
      "121 ...\n",
      "122 ...\n",
      "123 ...\n",
      "124 ...\n",
      "125 ...\n",
      "126 ...\n",
      "127 ...\n",
      "128 ...\n",
      "129 ...\n",
      "130 ...\n",
      "131 ...\n",
      "132 ...\n",
      "133 ...\n",
      "134 ...\n",
      "135 ...\n",
      "136 ...\n",
      "137 ...\n",
      "138 ...\n",
      "139 ...\n",
      "140 ...\n",
      "141 ...\n",
      "142 ...\n",
      "143 ...\n",
      "144 ...\n",
      "145 ...\n",
      "146 ...\n",
      "147 ...\n",
      "148 ...\n",
      "149 ...\n",
      "150 ...\n",
      "151 ...\n",
      "152 ...\n",
      "153 ...\n",
      "154 ...\n",
      "155 ...\n",
      "156 ...\n",
      "157 ...\n",
      "158 ...\n",
      "159 ...\n",
      "160 ...\n",
      "161 ...\n",
      "162 ...\n",
      "163 ...\n",
      "164 ...\n",
      "165 ...\n",
      "166 ...\n",
      "167 ...\n",
      "168 ...\n",
      "169 ...\n",
      "170 ...\n",
      "171 ...\n",
      "172 ...\n",
      "173 ...\n",
      "174 ...\n",
      "175 ...\n",
      "176 ...\n",
      "177 ...\n",
      "178 ...\n",
      "179 ...\n",
      "180 ...\n",
      "181 ...\n",
      "182 ...\n",
      "183 ...\n",
      "184 ...\n",
      "185 ...\n",
      "186 ...\n",
      "187 ...\n",
      "188 ...\n",
      "189 ...\n",
      "190 ...\n",
      "191 ...\n",
      "192 ...\n",
      "193 ...\n",
      "194 ...\n",
      "195 ...\n",
      "196 ...\n",
      "197 ...\n",
      "198 ...\n",
      "199 ...\n",
      "200 ...\n",
      "201 ...\n",
      "202 ...\n",
      "203 ...\n",
      "204 ...\n",
      "205 ...\n",
      "206 ...\n",
      "207 ...\n",
      "208 ...\n",
      "209 ...\n",
      "210 ...\n",
      "211 ...\n",
      "212 ...\n",
      "213 ...\n",
      "214 ...\n",
      "215 ...\n",
      "216 ...\n",
      "217 ...\n",
      "218 ...\n",
      "219 ...\n",
      "220 ...\n",
      "221 ...\n",
      "222 ...\n",
      "223 ...\n",
      "224 ...\n",
      "225 ...\n",
      "226 ...\n",
      "227 ...\n",
      "228 ...\n",
      "229 ...\n",
      "230 ...\n",
      "231 ...\n",
      "232 ...\n",
      "233 ...\n",
      "234 ...\n",
      "235 ...\n",
      "236 ...\n",
      "237 ...\n",
      "238 ...\n",
      "239 ...\n",
      "240 ...\n",
      "241 ...\n",
      "242 ...\n",
      "243 ...\n",
      "244 ...\n",
      "245 ...\n",
      "246 ...\n",
      "247 ...\n",
      "248 ...\n",
      "249 ...\n",
      "250 ...\n",
      "251 ...\n",
      "252 ...\n",
      "253 ...\n",
      "254 ...\n",
      "255 ...\n",
      "256 ...\n",
      "257 ...\n",
      "258 ...\n",
      "259 ...\n",
      "260 ...\n",
      "261 ...\n",
      "262 ...\n",
      "263 ...\n",
      "264 ...\n",
      "265 ...\n",
      "266 ...\n",
      "267 ...\n",
      "268 ...\n",
      "269 ...\n",
      "270 ...\n",
      "271 ...\n",
      "272 ...\n",
      "273 ...\n",
      "274 ...\n",
      "275 ...\n",
      "276 ...\n",
      "277 ...\n",
      "278 ...\n",
      "279 ...\n",
      "280 ...\n",
      "281 ...\n",
      "282 ...\n",
      "283 ...\n",
      "284 ...\n",
      "285 ...\n",
      "286 ...\n",
      "287 ...\n",
      "288 ...\n",
      "289 ...\n",
      "290 ...\n",
      "291 ...\n",
      "292 ...\n",
      "293 ...\n",
      "294 ...\n",
      "295 ...\n",
      "296 ...\n",
      "297 ...\n",
      "298 ...\n",
      "299 ...\n",
      "300 ...\n",
      "301 ...\n",
      "302 ...\n",
      "303 ...\n",
      "304 ...\n",
      "305 ...\n",
      "306 ...\n",
      "307 ...\n",
      "308 ...\n",
      "309 ...\n",
      "310 ...\n",
      "311 ...\n",
      "312 ...\n",
      "313 ...\n",
      "314 ...\n",
      "315 ...\n",
      "316 ...\n",
      "317 ...\n",
      "318 ...\n",
      "319 ...\n",
      "320 ...\n",
      "321 ...\n",
      "322 ...\n",
      "323 ...\n",
      "324 ...\n",
      "325 ...\n",
      "326 ...\n",
      "327 ...\n",
      "328 ...\n",
      "329 ...\n",
      "330 ...\n",
      "331 ...\n",
      "332 ...\n",
      "333 ...\n",
      "334 ...\n",
      "335 ...\n",
      "336 ...\n",
      "337 ...\n",
      "338 ...\n",
      "339 ...\n",
      "340 ...\n",
      "341 ...\n",
      "342 ...\n",
      "343 ...\n",
      "344 ...\n",
      "345 ...\n",
      "346 ...\n",
      "347 ...\n",
      "348 ...\n",
      "349 ...\n",
      "350 ...\n",
      "351 ...\n",
      "352 ...\n",
      "353 ...\n",
      "354 ...\n",
      "355 ...\n",
      "356 ...\n",
      "357 ...\n",
      "358 ...\n",
      "359 ...\n",
      "360 ...\n",
      "361 ...\n",
      "362 ...\n",
      "363 ...\n",
      "364 ...\n",
      "365 ...\n",
      "366 ...\n",
      "367 ...\n",
      "368 ...\n",
      "369 ...\n",
      "370 ...\n",
      "371 ...\n",
      "372 ...\n",
      "373 ...\n",
      "374 ...\n",
      "375 ...\n",
      "376 ...\n",
      "377 ...\n",
      "378 ...\n",
      "379 ...\n",
      "380 ...\n",
      "381 ...\n",
      "382 ...\n",
      "383 ...\n",
      "384 ...\n",
      "385 ...\n",
      "386 ...\n",
      "387 ...\n",
      "388 ...\n",
      "389 ...\n",
      "390 ...\n",
      "391 ...\n",
      "392 ...\n",
      "393 ...\n",
      "394 ...\n",
      "395 ...\n",
      "396 ...\n",
      "397 ...\n",
      "398 ...\n",
      "399 ...\n",
      "400 ...\n",
      "401 ...\n",
      "402 ...\n",
      "403 ...\n",
      "404 ...\n",
      "405 ...\n",
      "406 ...\n",
      "407 ...\n",
      "408 ...\n",
      "409 ...\n",
      "410 ...\n",
      "411 ...\n",
      "412 ...\n",
      "413 ...\n",
      "414 ...\n",
      "415 ...\n",
      "416 ...\n",
      "417 ...\n",
      "418 ...\n",
      "419 ...\n",
      "420 ...\n",
      "421 ...\n",
      "422 ...\n",
      "423 ...\n",
      "424 ...\n",
      "425 ...\n",
      "426 ...\n",
      "427 ...\n",
      "428 ...\n",
      "429 ...\n",
      "430 ...\n",
      "431 ...\n",
      "432 ...\n",
      "433 ...\n",
      "434 ...\n",
      "435 ...\n",
      "436 ...\n",
      "437 ...\n",
      "438 ...\n",
      "439 ...\n",
      "440 ...\n",
      "441 ...\n",
      "442 ...\n",
      "443 ...\n",
      "444 ...\n",
      "445 ...\n",
      "446 ...\n",
      "447 ...\n",
      "448 ...\n",
      "449 ...\n",
      "450 ...\n",
      "451 ...\n",
      "452 ...\n",
      "453 ...\n",
      "454 ...\n",
      "455 ...\n",
      "456 ...\n",
      "457 ...\n",
      "458 ...\n",
      "459 ...\n",
      "460 ...\n",
      "461 ...\n",
      "462 ...\n",
      "463 ...\n",
      "464 ...\n",
      "465 ...\n",
      "466 ...\n",
      "467 ...\n",
      "468 ...\n",
      "469 ...\n",
      "470 ...\n",
      "471 ...\n",
      "472 ...\n",
      "473 ...\n",
      "474 ...\n",
      "475 ...\n",
      "476 ...\n",
      "477 ...\n",
      "478 ...\n",
      "479 ...\n",
      "480 ...\n",
      "481 ...\n",
      "482 ...\n",
      "483 ...\n",
      "484 ...\n",
      "485 ...\n",
      "486 ...\n",
      "487 ...\n",
      "488 ...\n",
      "489 ...\n",
      "490 ...\n",
      "491 ...\n",
      "492 ...\n",
      "493 ...\n",
      "494 ...\n",
      "495 ...\n",
      "496 ...\n",
      "497 ...\n",
      "498 ...\n",
      "499 ...\n",
      "500 ...\n",
      "501 ...\n",
      "502 ...\n",
      "503 ...\n",
      "504 ...\n",
      "505 ...\n",
      "506 ...\n",
      "507 ...\n",
      "508 ...\n",
      "509 ...\n",
      "510 ...\n",
      "511 ...\n",
      "512 ...\n",
      "513 ...\n",
      "514 ...\n",
      "515 ...\n",
      "516 ...\n",
      "517 ...\n"
     ]
    }
   ],
   "source": [
    "DATA_val_s = []\n",
    "DATA_val_s_cmplx = []\n",
    "DATA_val_n = []\n",
    "DATA_val_x_cmplx = []\n",
    "DATA_val_x = []\n",
    "\n",
    "with open(VAL_S,'wb') as fs, open(VAL_N,'wb') as fn, open(VAL_X_cmplx,'wb') as fx_cmplx, open(VAL_X,'wb') as fx, open(VAL_S_cmplx,'wb') as fs_cmplx: \n",
    "    count = 1\n",
    "    for file_s, file_n, file_x in zip(sorted(glob.glob(PATH_directory+PATH_val+CLEAN_format_val)),sorted(glob.glob(PATH_directory+PATH_val+NOISE_format_val)),sorted(glob.glob(PATH_directory+PATH_val+MIX_format_val))):\n",
    "        s_cmplx,s = preprossed_data(file_s)\n",
    "        DATA_val_s.append(np.array(s))\n",
    "        np.savetxt(fs, s, fmt='%.5f')\n",
    "        fs.write(b'\\n')\n",
    "        \n",
    "        DATA_val_s_cmplx.append(np.array(s_cmplx))\n",
    "        np.savetxt(fs_cmplx, s_cmplx, fmt='%.5f')\n",
    "        fs_cmplx.write(b'\\n')\n",
    "        \n",
    "        _,n = preprossed_data(file_n)\n",
    "        DATA_val_n.append(np.array(n))\n",
    "        np.savetxt(fn, n, fmt='%.5f')\n",
    "        fn.write(b'\\n')\n",
    "        \n",
    "        x_cmplx,x = preprossed_data(file_x)\n",
    "        DATA_val_x.append(np.array(x))\n",
    "        np.savetxt(fx, x, fmt='%.5f')\n",
    "        fx.write(b'\\n')\n",
    "        \n",
    "        DATA_val_x_cmplx.append(np.array(x_cmplx))\n",
    "        np.savetxt(fx_cmplx, x_cmplx, fmt='%.5f')\n",
    "        fx_cmplx.write(b'\\n')\n",
    "        \n",
    "        print(count, '...')\n",
    "        count += 1\n",
    "        \n",
    "fs.close()\n",
    "fs_cmplx.close()\n",
    "fn.close()\n",
    "fx_cmplx.close()\n",
    "fx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ...\n",
      "2 ...\n",
      "3 ...\n",
      "4 ...\n",
      "5 ...\n",
      "6 ...\n",
      "7 ...\n",
      "8 ...\n",
      "9 ...\n",
      "10 ...\n",
      "11 ...\n",
      "12 ...\n",
      "13 ...\n",
      "14 ...\n",
      "15 ...\n",
      "16 ...\n",
      "17 ...\n",
      "18 ...\n",
      "19 ...\n",
      "20 ...\n",
      "21 ...\n",
      "22 ...\n",
      "23 ...\n",
      "24 ...\n",
      "25 ...\n",
      "26 ...\n",
      "27 ...\n",
      "28 ...\n",
      "29 ...\n",
      "30 ...\n",
      "31 ...\n",
      "32 ...\n",
      "33 ...\n",
      "34 ...\n",
      "35 ...\n",
      "36 ...\n",
      "37 ...\n",
      "38 ...\n",
      "39 ...\n",
      "40 ...\n",
      "41 ...\n",
      "42 ...\n",
      "43 ...\n",
      "44 ...\n",
      "45 ...\n",
      "46 ...\n",
      "47 ...\n",
      "48 ...\n",
      "49 ...\n",
      "50 ...\n",
      "51 ...\n",
      "52 ...\n",
      "53 ...\n",
      "54 ...\n",
      "55 ...\n",
      "56 ...\n",
      "57 ...\n",
      "58 ...\n",
      "59 ...\n",
      "60 ...\n",
      "61 ...\n",
      "62 ...\n",
      "63 ...\n",
      "64 ...\n",
      "65 ...\n",
      "66 ...\n",
      "67 ...\n",
      "68 ...\n",
      "69 ...\n",
      "70 ...\n",
      "71 ...\n",
      "72 ...\n",
      "73 ...\n",
      "74 ...\n",
      "75 ...\n",
      "76 ...\n",
      "77 ...\n",
      "78 ...\n",
      "79 ...\n",
      "80 ...\n",
      "81 ...\n",
      "82 ...\n",
      "83 ...\n",
      "84 ...\n",
      "85 ...\n",
      "86 ...\n",
      "87 ...\n",
      "88 ...\n",
      "89 ...\n",
      "90 ...\n",
      "91 ...\n",
      "92 ...\n",
      "93 ...\n",
      "94 ...\n",
      "95 ...\n",
      "96 ...\n",
      "97 ...\n",
      "98 ...\n",
      "99 ...\n",
      "100 ...\n",
      "101 ...\n",
      "102 ...\n",
      "103 ...\n",
      "104 ...\n",
      "105 ...\n",
      "106 ...\n",
      "107 ...\n",
      "108 ...\n",
      "109 ...\n",
      "110 ...\n",
      "111 ...\n",
      "112 ...\n",
      "113 ...\n",
      "114 ...\n",
      "115 ...\n",
      "116 ...\n",
      "117 ...\n",
      "118 ...\n",
      "119 ...\n",
      "120 ...\n",
      "121 ...\n",
      "122 ...\n",
      "123 ...\n",
      "124 ...\n",
      "125 ...\n",
      "126 ...\n",
      "127 ...\n",
      "128 ...\n",
      "129 ...\n",
      "130 ...\n",
      "131 ...\n",
      "132 ...\n",
      "133 ...\n",
      "134 ...\n",
      "135 ...\n",
      "136 ...\n",
      "137 ...\n",
      "138 ...\n",
      "139 ...\n",
      "140 ...\n",
      "141 ...\n",
      "142 ...\n",
      "143 ...\n",
      "144 ...\n",
      "145 ...\n",
      "146 ...\n",
      "147 ...\n",
      "148 ...\n",
      "149 ...\n",
      "150 ...\n",
      "151 ...\n",
      "152 ...\n",
      "153 ...\n",
      "154 ...\n",
      "155 ...\n",
      "156 ...\n",
      "157 ...\n",
      "158 ...\n",
      "159 ...\n",
      "160 ...\n",
      "161 ...\n",
      "162 ...\n",
      "163 ...\n",
      "164 ...\n",
      "165 ...\n",
      "166 ...\n",
      "167 ...\n",
      "168 ...\n",
      "169 ...\n",
      "170 ...\n",
      "171 ...\n",
      "172 ...\n",
      "173 ...\n",
      "174 ...\n",
      "175 ...\n",
      "176 ...\n",
      "177 ...\n",
      "178 ...\n",
      "179 ...\n",
      "180 ...\n",
      "181 ...\n",
      "182 ...\n",
      "183 ...\n",
      "184 ...\n",
      "185 ...\n",
      "186 ...\n",
      "187 ...\n",
      "188 ...\n",
      "189 ...\n",
      "190 ...\n",
      "191 ...\n",
      "192 ...\n",
      "193 ...\n",
      "194 ...\n",
      "195 ...\n",
      "196 ...\n",
      "197 ...\n",
      "198 ...\n",
      "199 ...\n",
      "200 ...\n",
      "201 ...\n",
      "202 ...\n",
      "203 ...\n",
      "204 ...\n",
      "205 ...\n",
      "206 ...\n",
      "207 ...\n",
      "208 ...\n",
      "209 ...\n",
      "210 ...\n",
      "211 ...\n",
      "212 ...\n",
      "213 ...\n",
      "214 ...\n",
      "215 ...\n",
      "216 ...\n",
      "217 ...\n",
      "218 ...\n",
      "219 ...\n",
      "220 ...\n",
      "221 ...\n",
      "222 ...\n",
      "223 ...\n",
      "224 ...\n",
      "225 ...\n",
      "226 ...\n",
      "227 ...\n",
      "228 ...\n",
      "229 ...\n",
      "230 ...\n",
      "231 ...\n",
      "232 ...\n",
      "233 ...\n",
      "234 ...\n",
      "235 ...\n",
      "236 ...\n",
      "237 ...\n",
      "238 ...\n",
      "239 ...\n",
      "240 ...\n",
      "241 ...\n",
      "242 ...\n",
      "243 ...\n",
      "244 ...\n",
      "245 ...\n",
      "246 ...\n",
      "247 ...\n",
      "248 ...\n",
      "249 ...\n",
      "250 ...\n",
      "251 ...\n",
      "252 ...\n",
      "253 ...\n",
      "254 ...\n",
      "255 ...\n",
      "256 ...\n",
      "257 ...\n",
      "258 ...\n",
      "259 ...\n",
      "260 ...\n",
      "261 ...\n",
      "262 ...\n",
      "263 ...\n",
      "264 ...\n",
      "265 ...\n",
      "266 ...\n",
      "267 ...\n",
      "268 ...\n",
      "269 ...\n",
      "270 ...\n",
      "271 ...\n",
      "272 ...\n",
      "273 ...\n",
      "274 ...\n",
      "275 ...\n",
      "276 ...\n",
      "277 ...\n",
      "278 ...\n",
      "279 ...\n",
      "280 ...\n",
      "281 ...\n",
      "282 ...\n",
      "283 ...\n",
      "284 ...\n",
      "285 ...\n",
      "286 ...\n",
      "287 ...\n",
      "288 ...\n",
      "289 ...\n",
      "290 ...\n",
      "291 ...\n",
      "292 ...\n",
      "293 ...\n",
      "294 ...\n",
      "295 ...\n",
      "296 ...\n",
      "297 ...\n",
      "298 ...\n",
      "299 ...\n",
      "300 ...\n",
      "301 ...\n",
      "302 ...\n",
      "303 ...\n",
      "304 ...\n",
      "305 ...\n",
      "306 ...\n",
      "307 ...\n",
      "308 ...\n",
      "309 ...\n",
      "310 ...\n",
      "311 ...\n",
      "312 ...\n",
      "313 ...\n",
      "314 ...\n",
      "315 ...\n",
      "316 ...\n",
      "317 ...\n",
      "318 ...\n",
      "319 ...\n",
      "320 ...\n",
      "321 ...\n",
      "322 ...\n",
      "323 ...\n",
      "324 ...\n",
      "325 ...\n",
      "326 ...\n",
      "327 ...\n",
      "328 ...\n",
      "329 ...\n",
      "330 ...\n",
      "331 ...\n",
      "332 ...\n",
      "333 ...\n",
      "334 ...\n",
      "335 ...\n",
      "336 ...\n",
      "337 ...\n",
      "338 ...\n",
      "339 ...\n",
      "340 ...\n",
      "341 ...\n",
      "342 ...\n",
      "343 ...\n",
      "344 ...\n",
      "345 ...\n",
      "346 ...\n",
      "347 ...\n",
      "348 ...\n",
      "349 ...\n",
      "350 ...\n",
      "351 ...\n",
      "352 ...\n",
      "353 ...\n",
      "354 ...\n",
      "355 ...\n",
      "356 ...\n",
      "357 ...\n",
      "358 ...\n",
      "359 ...\n",
      "360 ...\n",
      "361 ...\n",
      "362 ...\n",
      "363 ...\n",
      "364 ...\n",
      "365 ...\n",
      "366 ...\n",
      "367 ...\n",
      "368 ...\n",
      "369 ...\n",
      "370 ...\n",
      "371 ...\n",
      "372 ...\n",
      "373 ...\n",
      "374 ...\n",
      "375 ...\n",
      "376 ...\n",
      "377 ...\n",
      "378 ...\n",
      "379 ...\n",
      "380 ...\n",
      "381 ...\n",
      "382 ...\n",
      "383 ...\n",
      "384 ...\n",
      "385 ...\n",
      "386 ...\n",
      "387 ...\n",
      "388 ...\n",
      "389 ...\n",
      "390 ...\n",
      "391 ...\n",
      "392 ...\n",
      "393 ...\n",
      "394 ...\n",
      "395 ...\n",
      "396 ...\n",
      "397 ...\n",
      "398 ...\n",
      "399 ...\n",
      "400 ...\n"
     ]
    }
   ],
   "source": [
    "DATA_test_x_cmplx = []\n",
    "DATA_test_x = []\n",
    "\n",
    "with open(TEST_S,'wb') as fs, open(TEST_N,'wb') as fn, open(TEST_X_cmplx,'wb') as fx_cmplx, open(TEST_X,'wb') as fx: \n",
    "    count = 1\n",
    "    for file_x in sorted(glob.glob(PATH_directory+PATH_test+MIX_format_test)):\n",
    "        x_cmplx,x = preprossed_data(file_x)\n",
    "        DATA_test_x.append(np.array(x))\n",
    "        np.savetxt(fx, x, fmt='%.5f')\n",
    "        fx.write(b'\\n')\n",
    "        \n",
    "        DATA_test_x_cmplx.append(np.array(x_cmplx))\n",
    "        np.savetxt(fx_cmplx, x_cmplx, fmt='%.5f')\n",
    "        fx_cmplx.write(b'\\n')\n",
    "        \n",
    "        print(count,'...')\n",
    "        count += 1\n",
    "        \n",
    "fx_cmplx.close()\n",
    "fx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_name):\n",
    "    \n",
    "    with open(file_name) as f:\n",
    "        lines=f.readlines()\n",
    "        print(len(lines))\n",
    "        sentence_full=[]\n",
    "        count = 0\n",
    "        sentence=[]\n",
    "        for line in lines:\n",
    "\n",
    "            if count < 513:\n",
    "                if count ==0:\n",
    "                    sentence=np.array(np.fromstring(line, dtype=float, sep=' '), ndmin=2)\n",
    "                    count+=1\n",
    "                else:\n",
    "                    myarray = np.array(np.fromstring(line, dtype=float, sep=' '), ndmin=2)\n",
    "                    sentence=np.concatenate((sentence, myarray), axis=0)\n",
    "                    count+=1\n",
    "            else:\n",
    "                sentence_full.append(sentence) \n",
    "                count=0\n",
    "                sentence=[]\n",
    "                \n",
    "        return sentence_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616800\n",
      "616800\n",
      "616800\n",
      "616800\n"
     ]
    }
   ],
   "source": [
    "DATA_train_s = load_file(TRAIN_S)\n",
    "DATA_train_n = load_file(TRAIN_N)\n",
    "DATA_train_x_cmplx = load_file(TRAIN_X_cmplx)\n",
    "DATA_train_x = load_file(TRAIN_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(513, 65)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_train_s[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616800\n",
      "616800\n",
      "616800\n",
      "616800\n"
     ]
    }
   ],
   "source": [
    "DATA_val_s = load_file(VAL_S)\n",
    "DATA_val_n = load_file(VAL_N)\n",
    "DATA_val_x_cmplx = load_file(VAL_X_cmplx)\n",
    "DATA_val_x = load_file(VAL_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205600\n",
      "205600\n"
     ]
    }
   ],
   "source": [
    "DATA_test_x_cmplx = load_file(TEST_X_cmplx)\n",
    "DATA_test_x = load_file(TEST_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_train_M=[ (1.0*(DATA_train_s[i]>DATA_train_n[i])) for i in range(len(DATA_train_s)) ]\n",
    "\n",
    "DATA_val_M=[ (1.0*(DATA_val_s[i]>DATA_val_n[i])) for i in range(len(DATA_val_s)) ]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch_mb(X_,Y_,mini_batch):\n",
    "    \n",
    "    batch_x, batch_y = None, None\n",
    "    \n",
    "    for e,(x,y) in enumerate(zip(X_,Y_)):\n",
    "#         print(e)\n",
    "        \n",
    "        batch_x = np.array(x.T) if batch_x is None else np.concatenate( (batch_x,x.T), axis=0)\n",
    "        batch_y = np.array(y.T) if batch_y is None else np.concatenate( (batch_y,y.T), axis=0)\n",
    "        \n",
    "#         print('e',e,'batch_x',batch_x.shape,'batch_y',batch_y.shape)\n",
    "#         print('batch_y',batch_y.shape)\n",
    "        \n",
    "        if e>0 and (e+1)%10==0:\n",
    "#             print('ZZZ--> e',e,'batch_x',batch_x.shape,'batch_y',batch_y.shape)\n",
    "            \n",
    "            batch_x, temp_x = None, batch_x\n",
    "            batch_y, temp_y = None, batch_y\n",
    "            \n",
    "            temp_x = temp_x.reshape((-1,Max_RNN,513))\n",
    "            temp_y = temp_y.reshape((-1,Max_RNN,513))\n",
    "            \n",
    "            total_mini_batch = temp_x.shape[0]//mini_batch\n",
    "#             print('temp_x.shape[0]',temp_x.shape[0], 'total_mini_batch',total_mini_batch)\n",
    "            \n",
    "            for mb in range(total_mini_batch):\n",
    "                start_b = (mb*mini_batch)\n",
    "                end_b = ((mb+1)*mini_batch)\n",
    "\n",
    "                yield temp_x[start_b:end_b],temp_y[start_b:end_b]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(X_,Y_):\n",
    "    \n",
    "    batch_x, batch_y = None, None\n",
    "    \n",
    "    for e,(x,y) in enumerate(zip(X_,Y_)):\n",
    "#         print(e)\n",
    "        \n",
    "        batch_x = np.array(x.T) if batch_x is None else np.concatenate( (batch_x,x.T), axis=0)\n",
    "        batch_y = np.array(y.T) if batch_y is None else np.concatenate( (batch_y,y.T), axis=0)\n",
    "        \n",
    "#         print('batch_x',batch_x.shape)\n",
    "#         print('batch_y',batch_y.shape)\n",
    "        \n",
    "        if e>0 and (e+1)%10==0:\n",
    "            temp_x, batch_x = batch_x, None\n",
    "            temp_y, batch_y = batch_y, None\n",
    "            \n",
    "            temp_x = temp_x.reshape((-1,Max_RNN,513))\n",
    "            temp_y = temp_y.reshape((-1,Max_RNN,513))\n",
    "\n",
    "#             print('temp_x',temp_x.shape)\n",
    "#             print('temp_y',temp_y.shape)\n",
    "        \n",
    "            yield temp_x,temp_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(DATA_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "don't know how to read character strings with that array type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-020f6d9f6fc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mfXX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfXX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: don't know how to read character strings with that array type"
     ]
    }
   ],
   "source": [
    "# fXX = np.loadtxt(TEST_X_cmplx, dtype = np.complex128)\n",
    "\n",
    "\n",
    "with open(TEST_X_cmplx) as f:\n",
    "        lines=f.readlines()\n",
    "        for line in lines:\n",
    "            fXX = np.fromstring(line, dtype=np.complex128, sep=' ')\n",
    "            print(fXX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_x (65, 513)\n",
      "batch_y (65, 513)\n",
      "batch_x (130, 513)\n",
      "batch_y (130, 513)\n",
      "batch_x (195, 513)\n",
      "batch_y (195, 513)\n",
      "batch_x (260, 513)\n",
      "batch_y (260, 513)\n",
      "batch_x (325, 513)\n",
      "batch_y (325, 513)\n",
      "batch_x (390, 513)\n",
      "batch_y (390, 513)\n",
      "batch_x (455, 513)\n",
      "batch_y (455, 513)\n",
      "batch_x (520, 513)\n",
      "batch_y (520, 513)\n",
      "batch_x (585, 513)\n",
      "batch_y (585, 513)\n",
      "batch_x (650, 513)\n",
      "batch_y (650, 513)\n",
      "p (130, 5, 513)\n",
      "q (130, 5, 513)\n",
      "batch_x (130, 513)\n",
      "batch_y (130, 513)\n",
      "batch_x (260, 513)\n",
      "batch_y (260, 513)\n",
      "batch_x (390, 513)\n",
      "batch_y (390, 513)\n",
      "batch_x (520, 513)\n",
      "batch_y (520, 513)\n",
      "batch_x (650, 513)\n",
      "batch_y (650, 513)\n",
      "batch_x (780, 513)\n",
      "batch_y (780, 513)\n",
      "batch_x (910, 513)\n",
      "batch_y (910, 513)\n",
      "batch_x (1040, 513)\n",
      "batch_y (1040, 513)\n",
      "batch_x (1170, 513)\n",
      "batch_y (1170, 513)\n",
      "batch_x (1300, 513)\n",
      "batch_y (1300, 513)\n",
      "p (260, 5, 513)\n",
      "q (260, 5, 513)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for p,q in next_batch(DATA_train_x, DATA_train_M):\n",
    "    print('p',p.shape)\n",
    "#     print(p)\n",
    "    print('q',q.shape)\n",
    "#     print(q)\n",
    "    c+=1\n",
    "    \n",
    "    if c == 2:\n",
    "        break\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(513, 130)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DATA_train_x)\n",
    "DATA_train_x[12].shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(GRU(Max_RNN,input_shape=(Max_RNN,513), return_sequences=True))\n",
    "# model.add(GRU(output_dim = 513, input_length = 5, input_dim = 513, return_sequences=True))\n",
    "\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(TimeDistributed(Dense(513, activation='sigmoid')))\n",
    "model.add(Dense(513, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit_generator( next_batch(DATA_train_x, DATA_train_M), epochs=20, steps_per_epoch=120, validation_data=next_batch(DATA_val_x, DATA_val_M), validation_steps=120, shuffle=True)\n",
    "\n",
    "# Final evaluation of the model\n",
    "\n",
    "scores = model.evaluate_generator(next_batch(DATA_val_x, DATA_val_M), verbose=0)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(GRU(Max_RNN,input_shape=(Max_RNN,513), return_sequences=True))\n",
    "# model.add(GRU(output_dim = 513, input_length = 5, input_dim = 513, return_sequences=True))\n",
    "\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(TimeDistributed(Dense(513, activation='sigmoid')))\n",
    "model.add(Dense(513, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit_generator( next_batch_mb(DATA_train_x, DATA_train_M,10), epochs=20, steps_per_epoch=700, validation_data=next_batch_mb(DATA_val_x, DATA_val_M,10), validation_steps=700)\n",
    "\n",
    "# Final evaluation of the model\n",
    "\n",
    "scores = model.evaluate_generator(next_batch(DATA_val_x, DATA_val_M), verbose=0)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 5, 10)             15570     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 10)             0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 5, 10)             480       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5, 513)            5643      \n",
      "=================================================================\n",
      "Total params: 21,693\n",
      "Trainable params: 21,693\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 130 samples, validate on 130 samples\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 3s 24ms/step - loss: 0.2500 - acc: 0.0031 - val_loss: 0.2499 - val_acc: 0.0031\n",
      "Train on 260 samples, validate on 260 samples\n",
      "Epoch 1/1\n",
      "260/260 [==============================] - 0s 361us/step - loss: 0.2499 - acc: 0.0023 - val_loss: 0.2497 - val_acc: 7.6923e-04\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 309us/step - loss: 0.2495 - acc: 0.0013 - val_loss: 0.2495 - val_acc: 0.0027\n",
      "Train on 138 samples, validate on 138 samples\n",
      "Epoch 1/1\n",
      "138/138 [==============================] - 0s 326us/step - loss: 0.2494 - acc: 0.0000e+00 - val_loss: 0.2495 - val_acc: 0.0014\n",
      "Train on 184 samples, validate on 184 samples\n",
      "Epoch 1/1\n",
      "184/184 [==============================] - 0s 257us/step - loss: 0.2494 - acc: 0.0000e+00 - val_loss: 0.2493 - val_acc: 0.0011\n",
      "Train on 206 samples, validate on 206 samples\n",
      "Epoch 1/1\n",
      "206/206 [==============================] - 0s 243us/step - loss: 0.2489 - acc: 9.7087e-04 - val_loss: 0.2494 - val_acc: 9.7087e-04\n",
      "Train on 174 samples, validate on 174 samples\n",
      "Epoch 1/1\n",
      "174/174 [==============================] - 0s 275us/step - loss: 0.2492 - acc: 0.0011 - val_loss: 0.2488 - val_acc: 0.0000e+00\n",
      "Train on 278 samples, validate on 278 samples\n",
      "Epoch 1/1\n",
      "278/278 [==============================] - 0s 334us/step - loss: 0.2491 - acc: 7.1942e-04 - val_loss: 0.2490 - val_acc: 0.0000e+00\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 284us/step - loss: 0.2487 - acc: 0.0064 - val_loss: 0.2488 - val_acc: 0.0013\n",
      "Train on 160 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 303us/step - loss: 0.2487 - acc: 0.0000e+00 - val_loss: 0.2497 - val_acc: 0.0012\n",
      "Train on 282 samples, validate on 282 samples\n",
      "Epoch 1/1\n",
      "282/282 [==============================] - 0s 358us/step - loss: 0.2497 - acc: 0.0000e+00 - val_loss: 0.2499 - val_acc: 0.0028\n",
      "Train on 138 samples, validate on 138 samples\n",
      "Epoch 1/1\n",
      "138/138 [==============================] - 0s 318us/step - loss: 0.2475 - acc: 0.0014 - val_loss: 0.2488 - val_acc: 0.0014\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 290us/step - loss: 0.2480 - acc: 0.0000e+00 - val_loss: 0.2479 - val_acc: 0.0026\n",
      "Train on 154 samples, validate on 154 samples\n",
      "Epoch 1/1\n",
      "154/154 [==============================] - 0s 302us/step - loss: 0.2471 - acc: 0.0000e+00 - val_loss: 0.2479 - val_acc: 0.0039\n",
      "Train on 262 samples, validate on 262 samples\n",
      "Epoch 1/1\n",
      "262/262 [==============================] - 0s 337us/step - loss: 0.2489 - acc: 0.0015 - val_loss: 0.2499 - val_acc: 0.0000e+00\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 299us/step - loss: 0.2466 - acc: 0.0038 - val_loss: 0.2466 - val_acc: 0.0064\n",
      "Train on 158 samples, validate on 158 samples\n",
      "Epoch 1/1\n",
      "158/158 [==============================] - 0s 335us/step - loss: 0.2482 - acc: 0.0000e+00 - val_loss: 0.2517 - val_acc: 0.0013\n",
      "Train on 188 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 253us/step - loss: 0.2485 - acc: 0.0011 - val_loss: 0.2476 - val_acc: 0.0000e+00\n",
      "Train on 216 samples, validate on 216 samples\n",
      "Epoch 1/1\n",
      "216/216 [==============================] - 0s 222us/step - loss: 0.2469 - acc: 0.0000e+00 - val_loss: 0.2474 - val_acc: 0.0028\n",
      "Train on 176 samples, validate on 176 samples\n",
      "Epoch 1/1\n",
      "176/176 [==============================] - 0s 272us/step - loss: 0.2467 - acc: 0.0034 - val_loss: 0.2477 - val_acc: 0.0011\n",
      "Train on 212 samples, validate on 212 samples\n",
      "Epoch 1/1\n",
      "212/212 [==============================] - 0s 234us/step - loss: 0.2471 - acc: 0.0028 - val_loss: 0.2465 - val_acc: 9.4340e-04\n",
      "Train on 330 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "330/330 [==============================] - 0s 310us/step - loss: 0.2482 - acc: 0.0000e+00 - val_loss: 0.2484 - val_acc: 0.0000e+00\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 275us/step - loss: 0.2467 - acc: 0.0000e+00 - val_loss: 0.2426 - val_acc: 0.0011\n",
      "Train on 256 samples, validate on 256 samples\n",
      "Epoch 1/1\n",
      "256/256 [==============================] - 0s 210us/step - loss: 0.2444 - acc: 0.0000e+00 - val_loss: 0.2454 - val_acc: 7.8125e-04\n",
      "Train on 264 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "264/264 [==============================] - 0s 342us/step - loss: 0.2459 - acc: 0.0023 - val_loss: 0.2437 - val_acc: 0.0015\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 249us/step - loss: 0.2455 - acc: 0.0010 - val_loss: 0.2451 - val_acc: 0.0000e+00\n",
      "Train on 206 samples, validate on 206 samples\n",
      "Epoch 1/1\n",
      "206/206 [==============================] - 0s 240us/step - loss: 0.2459 - acc: 0.0000e+00 - val_loss: 0.2476 - val_acc: 0.0019\n",
      "Train on 288 samples, validate on 288 samples\n",
      "Epoch 1/1\n",
      "288/288 [==============================] - 0s 309us/step - loss: 0.2449 - acc: 0.0021 - val_loss: 0.2448 - val_acc: 0.0021\n",
      "Train on 222 samples, validate on 222 samples\n",
      "Epoch 1/1\n",
      "222/222 [==============================] - 0s 224us/step - loss: 0.2450 - acc: 0.0000e+00 - val_loss: 0.2448 - val_acc: 9.0090e-04\n",
      "Train on 288 samples, validate on 288 samples\n",
      "Epoch 1/1\n",
      "288/288 [==============================] - 0s 334us/step - loss: 0.2422 - acc: 6.9444e-04 - val_loss: 0.2423 - val_acc: 6.9444e-04\n",
      "Train on 78 samples, validate on 78 samples\n",
      "Epoch 1/1\n",
      "78/78 [==============================] - 0s 516us/step - loss: 0.2420 - acc: 0.0026 - val_loss: 0.2439 - val_acc: 0.0026\n",
      "Train on 164 samples, validate on 164 samples\n",
      "Epoch 1/1\n",
      "164/164 [==============================] - 0s 283us/step - loss: 0.2447 - acc: 0.0000e+00 - val_loss: 0.2448 - val_acc: 0.0012\n",
      "Train on 204 samples, validate on 204 samples\n",
      "Epoch 1/1\n",
      "204/204 [==============================] - 0s 239us/step - loss: 0.2440 - acc: 9.8039e-04 - val_loss: 0.2425 - val_acc: 9.8039e-04\n",
      "Train on 220 samples, validate on 220 samples\n",
      "Epoch 1/1\n",
      "220/220 [==============================] - 0s 214us/step - loss: 0.2431 - acc: 0.0000e+00 - val_loss: 0.2442 - val_acc: 0.0027\n",
      "Train on 194 samples, validate on 194 samples\n",
      "Epoch 1/1\n",
      "194/194 [==============================] - 0s 241us/step - loss: 0.2497 - acc: 0.0000e+00 - val_loss: 0.2487 - val_acc: 0.0000e+00\n",
      "Train on 196 samples, validate on 196 samples\n",
      "Epoch 1/1\n",
      "196/196 [==============================] - 0s 268us/step - loss: 0.2404 - acc: 0.0041 - val_loss: 0.2423 - val_acc: 0.0010\n",
      "Train on 114 samples, validate on 114 samples\n",
      "Epoch 1/1\n",
      "114/114 [==============================] - 0s 369us/step - loss: 0.2421 - acc: 0.0053 - val_loss: 0.2399 - val_acc: 0.0053\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 237us/step - loss: 0.2418 - acc: 9.9010e-04 - val_loss: 0.2429 - val_acc: 0.0020\n",
      "Train on 134 samples, validate on 134 samples\n",
      "Epoch 1/1\n",
      "134/134 [==============================] - 0s 331us/step - loss: 0.2452 - acc: 0.0000e+00 - val_loss: 0.2432 - val_acc: 0.0000e+00\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 305us/step - loss: 0.2387 - acc: 0.0041 - val_loss: 0.2328 - val_acc: 0.0081\n",
      "Train on 212 samples, validate on 212 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212/212 [==============================] - 0s 250us/step - loss: 0.2441 - acc: 0.0028 - val_loss: 0.2502 - val_acc: 0.0019\n",
      "Train on 210 samples, validate on 210 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 0s 232us/step - loss: 0.2456 - acc: 0.0019 - val_loss: 0.2445 - val_acc: 0.0029\n",
      "Train on 158 samples, validate on 158 samples\n",
      "Epoch 1/1\n",
      "158/158 [==============================] - 0s 302us/step - loss: 0.2382 - acc: 0.0013 - val_loss: 0.2437 - val_acc: 0.0025\n",
      "Train on 198 samples, validate on 198 samples\n",
      "Epoch 1/1\n",
      "198/198 [==============================] - 0s 253us/step - loss: 0.2399 - acc: 0.0061 - val_loss: 0.2417 - val_acc: 0.0010\n",
      "Train on 206 samples, validate on 206 samples\n",
      "Epoch 1/1\n",
      "206/206 [==============================] - 0s 245us/step - loss: 0.2494 - acc: 0.0019 - val_loss: 0.2450 - val_acc: 9.7087e-04\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 262us/step - loss: 0.2416 - acc: 0.0044 - val_loss: 0.2393 - val_acc: 0.0089\n",
      "Train on 214 samples, validate on 214 samples\n",
      "Epoch 1/1\n",
      "214/214 [==============================] - 0s 240us/step - loss: 0.2426 - acc: 0.0047 - val_loss: 0.2410 - val_acc: 0.0028\n",
      "Train on 186 samples, validate on 186 samples\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 269us/step - loss: 0.2388 - acc: 0.0000e+00 - val_loss: 0.2400 - val_acc: 0.0000e+00\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 267us/step - loss: 0.2394 - acc: 0.0011 - val_loss: 0.2417 - val_acc: 0.0044\n",
      "Train on 222 samples, validate on 222 samples\n",
      "Epoch 1/1\n",
      "222/222 [==============================] - 0s 226us/step - loss: 0.2435 - acc: 9.0090e-04 - val_loss: 0.2385 - val_acc: 9.0090e-04\n",
      "Train on 296 samples, validate on 296 samples\n",
      "Epoch 1/1\n",
      "296/296 [==============================] - 0s 307us/step - loss: 0.2573 - acc: 0.0014 - val_loss: 0.2595 - val_acc: 6.7568e-04\n",
      "Train on 230 samples, validate on 230 samples\n",
      "Epoch 1/1\n",
      "230/230 [==============================] - 0s 227us/step - loss: 0.2537 - acc: 0.0043 - val_loss: 0.2577 - val_acc: 0.0026\n",
      "Train on 190 samples, validate on 190 samples\n",
      "Epoch 1/1\n",
      "190/190 [==============================] - 0s 256us/step - loss: 0.2482 - acc: 0.0032 - val_loss: 0.2529 - val_acc: 0.0000e+00\n",
      "Train on 186 samples, validate on 186 samples\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 253us/step - loss: 0.2425 - acc: 0.0000e+00 - val_loss: 0.2491 - val_acc: 0.0000e+00\n",
      "Train on 218 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "218/218 [==============================] - 0s 265us/step - loss: 0.2501 - acc: 0.0000e+00 - val_loss: 0.2564 - val_acc: 0.0000e+00\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 288us/step - loss: 0.2558 - acc: 0.0000e+00 - val_loss: 0.2515 - val_acc: 0.0030\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 321us/step - loss: 0.2516 - acc: 0.0014 - val_loss: 0.2534 - val_acc: 0.0000e+00\n",
      "Train on 252 samples, validate on 252 samples\n",
      "Epoch 1/1\n",
      "252/252 [==============================] - 0s 210us/step - loss: 0.2463 - acc: 0.0016 - val_loss: 0.2449 - val_acc: 7.9365e-04\n",
      "Train on 152 samples, validate on 152 samples\n",
      "Epoch 1/1\n",
      "152/152 [==============================] - 0s 297us/step - loss: 0.2436 - acc: 0.0000e+00 - val_loss: 0.2500 - val_acc: 0.0013\n",
      "Train on 118 samples, validate on 118 samples\n",
      "Epoch 1/1\n",
      "118/118 [==============================] - 0s 360us/step - loss: 0.2449 - acc: 0.0017 - val_loss: 0.2442 - val_acc: 0.0034\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/1\n",
      "232/232 [==============================] - 0s 219us/step - loss: 0.2420 - acc: 0.0017 - val_loss: 0.2473 - val_acc: 8.6207e-04\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 295us/step - loss: 0.2373 - acc: 0.0000e+00 - val_loss: 0.2399 - val_acc: 0.0027\n",
      "Train on 144 samples, validate on 144 samples\n",
      "Epoch 1/1\n",
      "144/144 [==============================] - 0s 300us/step - loss: 0.2445 - acc: 0.0000e+00 - val_loss: 0.2472 - val_acc: 0.0000e+00\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 293us/step - loss: 0.2398 - acc: 0.0013 - val_loss: 0.2416 - val_acc: 0.0026\n",
      "Train on 212 samples, validate on 212 samples\n",
      "Epoch 1/1\n",
      "212/212 [==============================] - 0s 229us/step - loss: 0.2460 - acc: 9.4340e-04 - val_loss: 0.2385 - val_acc: 9.4340e-04\n",
      "Train on 194 samples, validate on 194 samples\n",
      "Epoch 1/1\n",
      "194/194 [==============================] - 0s 258us/step - loss: 0.2408 - acc: 0.0000e+00 - val_loss: 0.2427 - val_acc: 0.0021\n",
      "Train on 164 samples, validate on 164 samples\n",
      "Epoch 1/1\n",
      "164/164 [==============================] - 0s 408us/step - loss: 0.2445 - acc: 0.0000e+00 - val_loss: 0.2443 - val_acc: 0.0000e+00\n",
      "Train on 346 samples, validate on 346 samples\n",
      "Epoch 1/1\n",
      "346/346 [==============================] - 0s 284us/step - loss: 0.2432 - acc: 0.0012 - val_loss: 0.2412 - val_acc: 0.0012\n",
      "Train on 172 samples, validate on 172 samples\n",
      "Epoch 1/1\n",
      "172/172 [==============================] - 0s 274us/step - loss: 0.2461 - acc: 0.0047 - val_loss: 0.2454 - val_acc: 0.0035\n",
      "Train on 286 samples, validate on 286 samples\n",
      "Epoch 1/1\n",
      "286/286 [==============================] - 0s 323us/step - loss: 0.2366 - acc: 0.0049 - val_loss: 0.2354 - val_acc: 0.0042\n",
      "Train on 186 samples, validate on 186 samples\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 265us/step - loss: 0.2380 - acc: 0.0022 - val_loss: 0.2372 - val_acc: 0.0011\n",
      "Train on 216 samples, validate on 216 samples\n",
      "Epoch 1/1\n",
      "216/216 [==============================] - 0s 226us/step - loss: 0.2403 - acc: 0.0019 - val_loss: 0.2400 - val_acc: 0.0028\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 310us/step - loss: 0.2365 - acc: 0.0080 - val_loss: 0.2371 - val_acc: 0.0027\n",
      "Train on 132 samples, validate on 132 samples\n",
      "Epoch 1/1\n",
      "132/132 [==============================] - 0s 337us/step - loss: 0.2344 - acc: 0.0030 - val_loss: 0.2345 - val_acc: 0.0000e+00\n",
      "Train on 280 samples, validate on 280 samples\n",
      "Epoch 1/1\n",
      "280/280 [==============================] - 0s 403us/step - loss: 0.2319 - acc: 0.0014 - val_loss: 0.2346 - val_acc: 0.0036\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 309us/step - loss: 0.2468 - acc: 0.0027 - val_loss: 0.2368 - val_acc: 0.0027\n",
      "Train on 190 samples, validate on 190 samples\n",
      "Epoch 1/1\n",
      "190/190 [==============================] - 0s 255us/step - loss: 0.2404 - acc: 0.0021 - val_loss: 0.2355 - val_acc: 0.0011\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 298us/step - loss: 0.2323 - acc: 0.0027 - val_loss: 0.2405 - val_acc: 0.0014\n",
      "Train on 140 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "140/140 [==============================] - 0s 317us/step - loss: 0.2279 - acc: 0.0014 - val_loss: 0.2350 - val_acc: 0.0043\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 258us/step - loss: 0.2366 - acc: 0.0022 - val_loss: 0.2396 - val_acc: 0.0000e+00\n",
      "Train on 142 samples, validate on 142 samples\n",
      "Epoch 1/1\n",
      "142/142 [==============================] - 0s 308us/step - loss: 0.2481 - acc: 0.0042 - val_loss: 0.2498 - val_acc: 0.0000e+00\n",
      "Train on 108 samples, validate on 108 samples\n",
      "Epoch 1/1\n",
      "108/108 [==============================] - 0s 384us/step - loss: 0.2421 - acc: 0.0037 - val_loss: 0.2368 - val_acc: 0.0037\n",
      "Train on 184 samples, validate on 184 samples\n",
      "Epoch 1/1\n",
      "184/184 [==============================] - 0s 261us/step - loss: 0.2494 - acc: 0.0000e+00 - val_loss: 0.2444 - val_acc: 0.0011\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 244us/step - loss: 0.2512 - acc: 0.0000e+00 - val_loss: 0.2491 - val_acc: 0.0000e+00\n",
      "Train on 152 samples, validate on 152 samples\n",
      "Epoch 1/1\n",
      "152/152 [==============================] - 0s 292us/step - loss: 0.2448 - acc: 0.0000e+00 - val_loss: 0.2381 - val_acc: 0.0013\n",
      "Train on 174 samples, validate on 174 samples\n",
      "Epoch 1/1\n",
      "174/174 [==============================] - 0s 273us/step - loss: 0.2478 - acc: 0.0000e+00 - val_loss: 0.2420 - val_acc: 0.0011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "140/140 [==============================] - 0s 322us/step - loss: 0.2446 - acc: 0.0014 - val_loss: 0.2373 - val_acc: 0.0014\n",
      "Train on 228 samples, validate on 228 samples\n",
      "Epoch 1/1\n",
      "228/228 [==============================] - 0s 225us/step - loss: 0.2460 - acc: 8.7719e-04 - val_loss: 0.2446 - val_acc: 8.7719e-04\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 263us/step - loss: 0.2407 - acc: 0.0000e+00 - val_loss: 0.2381 - val_acc: 0.0020\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 260us/step - loss: 0.2378 - acc: 0.0033 - val_loss: 0.2371 - val_acc: 0.0022\n",
      "Train on 242 samples, validate on 242 samples\n",
      "Epoch 1/1\n",
      "242/242 [==============================] - 0s 211us/step - loss: 0.2537 - acc: 0.0041 - val_loss: 0.2533 - val_acc: 0.0025\n",
      "Train on 144 samples, validate on 144 samples\n",
      "Epoch 1/1\n",
      "144/144 [==============================] - 0s 308us/step - loss: 0.2418 - acc: 0.0042 - val_loss: 0.2397 - val_acc: 0.0042\n",
      "Train on 356 samples, validate on 356 samples\n",
      "Epoch 1/1\n",
      "356/356 [==============================] - 0s 274us/step - loss: 0.2458 - acc: 0.0056 - val_loss: 0.2445 - val_acc: 0.0107\n",
      "Train on 234 samples, validate on 234 samples\n",
      "Epoch 1/1\n",
      "234/234 [==============================] - 0s 268us/step - loss: 0.2510 - acc: 0.0171 - val_loss: 0.2476 - val_acc: 0.0359\n",
      "Train on 184 samples, validate on 184 samples\n",
      "Epoch 1/1\n",
      "184/184 [==============================] - 0s 300us/step - loss: 0.2455 - acc: 0.0293 - val_loss: 0.2442 - val_acc: 0.0554\n",
      "Train on 238 samples, validate on 238 samples\n",
      "Epoch 1/1\n",
      "238/238 [==============================] - 0s 244us/step - loss: 0.2424 - acc: 0.0269 - val_loss: 0.2412 - val_acc: 0.0336\n",
      "Train on 340 samples, validate on 340 samples\n",
      "Epoch 1/1\n",
      "340/340 [==============================] - 0s 302us/step - loss: 0.2418 - acc: 0.0541 - val_loss: 0.2394 - val_acc: 0.0635\n",
      "Train on 220 samples, validate on 220 samples\n",
      "Epoch 1/1\n",
      "220/220 [==============================] - 0s 240us/step - loss: 0.2396 - acc: 0.0864 - val_loss: 0.2384 - val_acc: 0.0955\n",
      "Train on 260 samples, validate on 260 samples\n",
      "Epoch 1/1\n",
      "260/260 [==============================] - 0s 377us/step - loss: 0.2414 - acc: 0.0538 - val_loss: 0.2401 - val_acc: 0.0815\n",
      "Train on 216 samples, validate on 216 samples\n",
      "Epoch 1/1\n",
      "216/216 [==============================] - 0s 254us/step - loss: 0.2388 - acc: 0.0870 - val_loss: 0.2330 - val_acc: 0.1083\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 262us/step - loss: 0.2417 - acc: 0.0330 - val_loss: 0.2404 - val_acc: 0.0100\n",
      "Train on 124 samples, validate on 124 samples\n",
      "Epoch 1/1\n",
      "124/124 [==============================] - 0s 381us/step - loss: 0.2464 - acc: 0.0371 - val_loss: 0.2486 - val_acc: 0.0177\n",
      "Train on 192 samples, validate on 192 samples\n",
      "Epoch 1/1\n",
      "192/192 [==============================] - 0s 264us/step - loss: 0.2456 - acc: 0.0229 - val_loss: 0.2452 - val_acc: 0.0198\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 287us/step - loss: 0.2443 - acc: 0.0156 - val_loss: 0.2433 - val_acc: 0.0078\n",
      "Train on 282 samples, validate on 282 samples\n",
      "Epoch 1/1\n",
      "282/282 [==============================] - 0s 339us/step - loss: 0.2405 - acc: 0.0106 - val_loss: 0.2400 - val_acc: 0.0057\n",
      "Train on 220 samples, validate on 220 samples\n",
      "Epoch 1/1\n",
      "220/220 [==============================] - 0s 262us/step - loss: 0.2386 - acc: 0.0309 - val_loss: 0.2372 - val_acc: 0.0236\n",
      "Train on 194 samples, validate on 194 samples\n",
      "Epoch 1/1\n",
      "194/194 [==============================] - 0s 259us/step - loss: 0.2390 - acc: 0.0052 - val_loss: 0.2355 - val_acc: 0.0103\n",
      "Train on 302 samples, validate on 302 samples\n",
      "Epoch 1/1\n",
      "302/302 [==============================] - 0s 351us/step - loss: 0.2411 - acc: 6.6225e-04 - val_loss: 0.2398 - val_acc: 6.6225e-04\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 241us/step - loss: 0.2348 - acc: 0.0030 - val_loss: 0.2338 - val_acc: 0.0090\n",
      "Train on 160 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 311us/step - loss: 0.2324 - acc: 0.0137 - val_loss: 0.2265 - val_acc: 0.0050\n",
      "Train on 158 samples, validate on 158 samples\n",
      "Epoch 1/1\n",
      "158/158 [==============================] - 0s 289us/step - loss: 0.2291 - acc: 0.0051 - val_loss: 0.2231 - val_acc: 0.0114\n",
      "Train on 154 samples, validate on 154 samples\n",
      "Epoch 1/1\n",
      "154/154 [==============================] - 0s 298us/step - loss: 0.2328 - acc: 0.0065 - val_loss: 0.2289 - val_acc: 0.0065\n",
      "Train on 166 samples, validate on 166 samples\n",
      "Epoch 1/1\n",
      "166/166 [==============================] - 0s 293us/step - loss: 0.2347 - acc: 0.0000e+00 - val_loss: 0.2286 - val_acc: 0.0000e+00\n",
      "Train on 224 samples, validate on 224 samples\n",
      "Epoch 1/1\n",
      "224/224 [==============================] - 0s 221us/step - loss: 0.2256 - acc: 0.0018 - val_loss: 0.2185 - val_acc: 8.9286e-04\n",
      "Train on 294 samples, validate on 294 samples\n",
      "Epoch 1/1\n",
      "294/294 [==============================] - 0s 325us/step - loss: 0.2273 - acc: 0.0020 - val_loss: 0.2291 - val_acc: 0.0014\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 292us/step - loss: 0.2208 - acc: 0.0013 - val_loss: 0.2148 - val_acc: 0.0040\n",
      "Train on 196 samples, validate on 196 samples\n",
      "Epoch 1/1\n",
      "196/196 [==============================] - 0s 240us/step - loss: 0.2331 - acc: 0.0031 - val_loss: 0.2273 - val_acc: 0.0041\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 303us/step - loss: 0.2354 - acc: 0.0041 - val_loss: 0.2292 - val_acc: 0.0014\n",
      "Train on 234 samples, validate on 234 samples\n",
      "Epoch 1/1\n",
      "234/234 [==============================] - 0s 220us/step - loss: 0.2217 - acc: 0.0068 - val_loss: 0.2226 - val_acc: 0.0060\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 262us/step - loss: 0.2318 - acc: 0.0056 - val_loss: 0.2335 - val_acc: 0.0100\n",
      "Train on 130 samples, validate on 130 samples\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 330us/step - loss: 0.2067 - acc: 0.0015 - val_loss: 0.2210 - val_acc: 0.0031\n",
      "Train on 260 samples, validate on 260 samples\n",
      "Epoch 1/1\n",
      "260/260 [==============================] - 0s 344us/step - loss: 0.2249 - acc: 0.0015 - val_loss: 0.2251 - val_acc: 0.0038\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 308us/step - loss: 0.2176 - acc: 0.0027 - val_loss: 0.2244 - val_acc: 0.0013\n",
      "Train on 138 samples, validate on 138 samples\n",
      "Epoch 1/1\n",
      "138/138 [==============================] - 0s 321us/step - loss: 0.2193 - acc: 0.0000e+00 - val_loss: 0.2289 - val_acc: 0.0000e+00\n",
      "Train on 184 samples, validate on 184 samples\n",
      "Epoch 1/1\n",
      "184/184 [==============================] - 0s 329us/step - loss: 0.2206 - acc: 0.0022 - val_loss: 0.2254 - val_acc: 0.0000e+00\n",
      "Train on 206 samples, validate on 206 samples\n",
      "Epoch 1/1\n",
      "206/206 [==============================] - 0s 247us/step - loss: 0.2137 - acc: 0.0029 - val_loss: 0.2312 - val_acc: 9.7087e-04\n",
      "Train on 174 samples, validate on 174 samples\n",
      "Epoch 1/1\n",
      "174/174 [==============================] - 0s 276us/step - loss: 0.2235 - acc: 0.0057 - val_loss: 0.2167 - val_acc: 0.0057\n",
      "Train on 278 samples, validate on 278 samples\n",
      "Epoch 1/1\n",
      "278/278 [==============================] - 0s 319us/step - loss: 0.2299 - acc: 0.0029 - val_loss: 0.2302 - val_acc: 7.1942e-04\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 302us/step - loss: 0.2233 - acc: 0.0128 - val_loss: 0.2260 - val_acc: 0.0026\n",
      "Train on 160 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 274us/step - loss: 0.2354 - acc: 0.0025 - val_loss: 0.2447 - val_acc: 0.0025\n",
      "Train on 282 samples, validate on 282 samples\n",
      "Epoch 1/1\n",
      "282/282 [==============================] - 0s 384us/step - loss: 0.2450 - acc: 0.0156 - val_loss: 0.2355 - val_acc: 0.0348\n",
      "Train on 138 samples, validate on 138 samples\n",
      "Epoch 1/1\n",
      "138/138 [==============================] - 0s 367us/step - loss: 0.2238 - acc: 0.0014 - val_loss: 0.2339 - val_acc: 0.0014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 377us/step - loss: 0.2236 - acc: 0.0205 - val_loss: 0.2175 - val_acc: 0.0231\n",
      "Train on 154 samples, validate on 154 samples\n",
      "Epoch 1/1\n",
      "154/154 [==============================] - 0s 333us/step - loss: 0.2213 - acc: 0.0117 - val_loss: 0.2234 - val_acc: 0.0117\n",
      "Train on 262 samples, validate on 262 samples\n",
      "Epoch 1/1\n",
      "262/262 [==============================] - 0s 342us/step - loss: 0.2415 - acc: 0.0374 - val_loss: 0.2387 - val_acc: 0.0267\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 304us/step - loss: 0.2191 - acc: 0.0064 - val_loss: 0.2187 - val_acc: 0.0218\n",
      "Train on 158 samples, validate on 158 samples\n",
      "Epoch 1/1\n",
      "158/158 [==============================] - 0s 292us/step - loss: 0.2378 - acc: 0.0038 - val_loss: 0.2541 - val_acc: 0.0025\n",
      "Train on 188 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 257us/step - loss: 0.2269 - acc: 0.0532 - val_loss: 0.2220 - val_acc: 0.0511\n",
      "Train on 216 samples, validate on 216 samples\n",
      "Epoch 1/1\n",
      "216/216 [==============================] - 0s 234us/step - loss: 0.2296 - acc: 0.0046 - val_loss: 0.2199 - val_acc: 0.0287\n",
      "Train on 176 samples, validate on 176 samples\n",
      "Epoch 1/1\n",
      "176/176 [==============================] - 0s 278us/step - loss: 0.2241 - acc: 0.0250 - val_loss: 0.2281 - val_acc: 0.0330\n",
      "Train on 212 samples, validate on 212 samples\n",
      "Epoch 1/1\n",
      "212/212 [==============================] - 0s 225us/step - loss: 0.2232 - acc: 0.0198 - val_loss: 0.2219 - val_acc: 0.0057\n",
      "Train on 330 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "330/330 [==============================] - 0s 287us/step - loss: 0.2345 - acc: 0.0097 - val_loss: 0.2349 - val_acc: 0.0103\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 265us/step - loss: 0.2295 - acc: 0.0000e+00 - val_loss: 0.2076 - val_acc: 0.0011\n",
      "Train on 256 samples, validate on 256 samples\n",
      "Epoch 1/1\n",
      "256/256 [==============================] - 0s 209us/step - loss: 0.2216 - acc: 0.0078 - val_loss: 0.2153 - val_acc: 0.0133\n",
      "Train on 264 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "264/264 [==============================] - 0s 336us/step - loss: 0.2249 - acc: 0.0098 - val_loss: 0.2097 - val_acc: 0.0068\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 244us/step - loss: 0.2238 - acc: 0.0080 - val_loss: 0.2127 - val_acc: 0.0190\n",
      "Train on 206 samples, validate on 206 samples\n",
      "Epoch 1/1\n",
      "206/206 [==============================] - 0s 242us/step - loss: 0.2189 - acc: 0.0117 - val_loss: 0.2203 - val_acc: 0.0214\n",
      "Train on 288 samples, validate on 288 samples\n",
      "Epoch 1/1\n",
      "288/288 [==============================] - 0s 357us/step - loss: 0.2284 - acc: 0.0132 - val_loss: 0.2197 - val_acc: 0.0125\n",
      "Train on 222 samples, validate on 222 samples\n",
      "Epoch 1/1\n",
      "222/222 [==============================] - 0s 269us/step - loss: 0.2219 - acc: 0.0180 - val_loss: 0.2156 - val_acc: 0.0108\n",
      "Train on 288 samples, validate on 288 samples\n",
      "Epoch 1/1\n",
      "288/288 [==============================] - 0s 327us/step - loss: 0.2142 - acc: 0.0062 - val_loss: 0.2190 - val_acc: 0.0069\n",
      "Train on 78 samples, validate on 78 samples\n",
      "Epoch 1/1\n",
      "78/78 [==============================] - 0s 518us/step - loss: 0.2161 - acc: 0.0103 - val_loss: 0.2150 - val_acc: 0.0077\n",
      "Train on 164 samples, validate on 164 samples\n",
      "Epoch 1/1\n",
      "164/164 [==============================] - 0s 287us/step - loss: 0.2266 - acc: 0.0256 - val_loss: 0.2194 - val_acc: 0.0183\n",
      "Train on 204 samples, validate on 204 samples\n",
      "Epoch 1/1\n",
      "204/204 [==============================] - 0s 251us/step - loss: 0.2143 - acc: 0.0216 - val_loss: 0.2089 - val_acc: 0.0157\n",
      "Train on 220 samples, validate on 220 samples\n",
      "Epoch 1/1\n",
      "220/220 [==============================] - 0s 241us/step - loss: 0.2220 - acc: 0.0100 - val_loss: 0.2226 - val_acc: 0.0191\n",
      "Train on 194 samples, validate on 194 samples\n",
      "Epoch 1/1\n",
      "194/194 [==============================] - 0s 259us/step - loss: 0.2287 - acc: 0.0206 - val_loss: 0.2216 - val_acc: 0.0278\n",
      "Train on 196 samples, validate on 196 samples\n",
      "Epoch 1/1\n",
      "196/196 [==============================] - 0s 261us/step - loss: 0.2131 - acc: 0.0194 - val_loss: 0.2065 - val_acc: 0.0276\n",
      "Train on 114 samples, validate on 114 samples\n",
      "Epoch 1/1\n",
      "114/114 [==============================] - 0s 374us/step - loss: 0.2213 - acc: 0.0105 - val_loss: 0.2148 - val_acc: 0.0070\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 249us/step - loss: 0.2143 - acc: 0.0168 - val_loss: 0.2119 - val_acc: 0.0277\n",
      "Train on 134 samples, validate on 134 samples\n",
      "Epoch 1/1\n",
      "134/134 [==============================] - 0s 336us/step - loss: 0.2266 - acc: 0.0164 - val_loss: 0.2181 - val_acc: 0.0104\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 322us/step - loss: 0.2149 - acc: 0.0068 - val_loss: 0.1997 - val_acc: 0.0122\n",
      "Train on 212 samples, validate on 212 samples\n",
      "Epoch 1/1\n",
      "212/212 [==============================] - 0s 238us/step - loss: 0.2246 - acc: 0.0085 - val_loss: 0.2429 - val_acc: 0.0208\n",
      "Train on 210 samples, validate on 210 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 0s 251us/step - loss: 0.2318 - acc: 0.0133 - val_loss: 0.2272 - val_acc: 0.0143\n",
      "Train on 158 samples, validate on 158 samples\n",
      "Epoch 1/1\n",
      "158/158 [==============================] - 0s 346us/step - loss: 0.2176 - acc: 0.0025 - val_loss: 0.2221 - val_acc: 0.0089\n",
      "Train on 198 samples, validate on 198 samples\n",
      "Epoch 1/1\n",
      "198/198 [==============================] - 0s 272us/step - loss: 0.2236 - acc: 0.0212 - val_loss: 0.2250 - val_acc: 0.0111\n",
      "Train on 206 samples, validate on 206 samples\n",
      "Epoch 1/1\n",
      "206/206 [==============================] - 0s 303us/step - loss: 0.2317 - acc: 0.0155 - val_loss: 0.2239 - val_acc: 0.0146\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 283us/step - loss: 0.2261 - acc: 0.0122 - val_loss: 0.2183 - val_acc: 0.0300\n",
      "Train on 214 samples, validate on 214 samples\n",
      "Epoch 1/1\n",
      "214/214 [==============================] - 0s 237us/step - loss: 0.2279 - acc: 0.0121 - val_loss: 0.2314 - val_acc: 0.0150\n",
      "Train on 186 samples, validate on 186 samples\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 255us/step - loss: 0.2203 - acc: 0.0065 - val_loss: 0.2172 - val_acc: 0.0237\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 338us/step - loss: 0.2268 - acc: 0.0033 - val_loss: 0.2240 - val_acc: 0.0122\n",
      "Train on 222 samples, validate on 222 samples\n",
      "Epoch 1/1\n",
      "222/222 [==============================] - 0s 282us/step - loss: 0.2231 - acc: 0.0162 - val_loss: 0.2222 - val_acc: 0.0117\n",
      "Train on 296 samples, validate on 296 samples\n",
      "Epoch 1/1\n",
      "296/296 [==============================] - 0s 343us/step - loss: 0.2394 - acc: 0.0243 - val_loss: 0.2380 - val_acc: 0.0439\n",
      "Train on 230 samples, validate on 230 samples\n",
      "Epoch 1/1\n",
      "230/230 [==============================] - 0s 215us/step - loss: 0.2254 - acc: 0.0374 - val_loss: 0.2267 - val_acc: 0.0330\n",
      "Train on 190 samples, validate on 190 samples\n",
      "Epoch 1/1\n",
      "190/190 [==============================] - 0s 254us/step - loss: 0.2273 - acc: 0.0189 - val_loss: 0.2319 - val_acc: 0.0211\n",
      "Train on 186 samples, validate on 186 samples\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 259us/step - loss: 0.2330 - acc: 0.0108 - val_loss: 0.2307 - val_acc: 0.0290\n",
      "Train on 218 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "218/218 [==============================] - 0s 224us/step - loss: 0.2281 - acc: 0.0266 - val_loss: 0.2273 - val_acc: 0.0459\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 242us/step - loss: 0.2227 - acc: 0.0446 - val_loss: 0.2266 - val_acc: 0.0366\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 371us/step - loss: 0.2250 - acc: 0.0338 - val_loss: 0.2323 - val_acc: 0.0351\n",
      "Train on 252 samples, validate on 252 samples\n",
      "Epoch 1/1\n",
      "252/252 [==============================] - 0s 227us/step - loss: 0.2311 - acc: 0.0190 - val_loss: 0.2224 - val_acc: 0.0198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 152 samples, validate on 152 samples\n",
      "Epoch 1/1\n",
      "152/152 [==============================] - 0s 369us/step - loss: 0.2242 - acc: 0.0316 - val_loss: 0.2279 - val_acc: 0.0408\n",
      "Train on 118 samples, validate on 118 samples\n",
      "Epoch 1/1\n",
      "118/118 [==============================] - 0s 362us/step - loss: 0.2243 - acc: 0.0085 - val_loss: 0.2267 - val_acc: 0.0136\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/1\n",
      "232/232 [==============================] - 0s 240us/step - loss: 0.2298 - acc: 0.0216 - val_loss: 0.2327 - val_acc: 0.0190\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 291us/step - loss: 0.2089 - acc: 0.0333 - val_loss: 0.2127 - val_acc: 0.0253\n",
      "Train on 144 samples, validate on 144 samples\n",
      "Epoch 1/1\n",
      "144/144 [==============================] - 0s 313us/step - loss: 0.2343 - acc: 0.0125 - val_loss: 0.2307 - val_acc: 0.0222\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 304us/step - loss: 0.2183 - acc: 0.0179 - val_loss: 0.2144 - val_acc: 0.0231\n",
      "Train on 212 samples, validate on 212 samples\n",
      "Epoch 1/1\n",
      "212/212 [==============================] - 0s 243us/step - loss: 0.2252 - acc: 0.0189 - val_loss: 0.2147 - val_acc: 0.0075\n",
      "Train on 194 samples, validate on 194 samples\n",
      "Epoch 1/1\n",
      "194/194 [==============================] - 0s 260us/step - loss: 0.2131 - acc: 0.0175 - val_loss: 0.2187 - val_acc: 0.0247\n",
      "Train on 164 samples, validate on 164 samples\n",
      "Epoch 1/1\n",
      "164/164 [==============================] - 0s 300us/step - loss: 0.2328 - acc: 0.0110 - val_loss: 0.2236 - val_acc: 0.0049\n",
      "Train on 346 samples, validate on 346 samples\n",
      "Epoch 1/1\n",
      "346/346 [==============================] - 0s 311us/step - loss: 0.2254 - acc: 0.0121 - val_loss: 0.2161 - val_acc: 0.0110\n",
      "Train on 172 samples, validate on 172 samples\n",
      "Epoch 1/1\n",
      "172/172 [==============================] - 0s 288us/step - loss: 0.2281 - acc: 0.0233 - val_loss: 0.2313 - val_acc: 0.0174\n",
      "Train on 286 samples, validate on 286 samples\n",
      "Epoch 1/1\n",
      "286/286 [==============================] - 0s 376us/step - loss: 0.2050 - acc: 0.0084 - val_loss: 0.2076 - val_acc: 0.0105\n",
      "Train on 186 samples, validate on 186 samples\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 279us/step - loss: 0.2191 - acc: 0.0043 - val_loss: 0.2076 - val_acc: 0.0226\n",
      "Train on 216 samples, validate on 216 samples\n",
      "Epoch 1/1\n",
      "216/216 [==============================] - 0s 239us/step - loss: 0.2131 - acc: 0.0139 - val_loss: 0.2190 - val_acc: 0.0213\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 297us/step - loss: 0.2114 - acc: 0.0147 - val_loss: 0.2159 - val_acc: 0.0133\n",
      "Train on 132 samples, validate on 132 samples\n",
      "Epoch 1/1\n",
      "132/132 [==============================] - 0s 351us/step - loss: 0.2048 - acc: 0.0061 - val_loss: 0.2108 - val_acc: 0.0030\n",
      "Train on 280 samples, validate on 280 samples\n",
      "Epoch 1/1\n",
      "280/280 [==============================] - 0s 342us/step - loss: 0.1991 - acc: 0.0107 - val_loss: 0.2103 - val_acc: 0.0100\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 309us/step - loss: 0.2412 - acc: 0.0068 - val_loss: 0.2241 - val_acc: 0.0041\n",
      "Train on 190 samples, validate on 190 samples\n",
      "Epoch 1/1\n",
      "190/190 [==============================] - 0s 276us/step - loss: 0.2213 - acc: 0.0168 - val_loss: 0.2158 - val_acc: 0.0032\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 349us/step - loss: 0.2113 - acc: 0.0027 - val_loss: 0.2197 - val_acc: 0.0243\n",
      "Train on 140 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "140/140 [==============================] - 0s 402us/step - loss: 0.2106 - acc: 0.0071 - val_loss: 0.2132 - val_acc: 0.0171\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 267us/step - loss: 0.2119 - acc: 0.0100 - val_loss: 0.2217 - val_acc: 0.0089\n",
      "Train on 142 samples, validate on 142 samples\n",
      "Epoch 1/1\n",
      "142/142 [==============================] - 0s 327us/step - loss: 0.2343 - acc: 0.0070 - val_loss: 0.2395 - val_acc: 0.0014\n",
      "Train on 108 samples, validate on 108 samples\n",
      "Epoch 1/1\n",
      "108/108 [==============================] - 0s 403us/step - loss: 0.2325 - acc: 0.0037 - val_loss: 0.2260 - val_acc: 0.0074\n",
      "Train on 184 samples, validate on 184 samples\n",
      "Epoch 1/1\n",
      "184/184 [==============================] - 0s 260us/step - loss: 0.2336 - acc: 0.0076 - val_loss: 0.2261 - val_acc: 0.0076\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 286us/step - loss: 0.2347 - acc: 0.0119 - val_loss: 0.2335 - val_acc: 0.0129\n",
      "Train on 152 samples, validate on 152 samples\n",
      "Epoch 1/1\n",
      "152/152 [==============================] - 0s 319us/step - loss: 0.2256 - acc: 0.0211 - val_loss: 0.2171 - val_acc: 0.0197\n",
      "Train on 174 samples, validate on 174 samples\n",
      "Epoch 1/1\n",
      "174/174 [==============================] - 0s 277us/step - loss: 0.2305 - acc: 0.0115 - val_loss: 0.2220 - val_acc: 0.0023\n",
      "Train on 140 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "140/140 [==============================] - 0s 334us/step - loss: 0.2308 - acc: 0.0114 - val_loss: 0.2148 - val_acc: 0.0157\n",
      "Train on 228 samples, validate on 228 samples\n",
      "Epoch 1/1\n",
      "228/228 [==============================] - 0s 232us/step - loss: 0.2347 - acc: 0.0246 - val_loss: 0.2220 - val_acc: 0.0342\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 266us/step - loss: 0.2175 - acc: 0.0366 - val_loss: 0.2177 - val_acc: 0.0485\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 312us/step - loss: 0.2264 - acc: 0.0400 - val_loss: 0.2197 - val_acc: 0.0444\n",
      "Train on 242 samples, validate on 242 samples\n",
      "Epoch 1/1\n",
      "242/242 [==============================] - 0s 223us/step - loss: 0.2272 - acc: 0.0901 - val_loss: 0.2270 - val_acc: 0.0950\n",
      "Train on 144 samples, validate on 144 samples\n",
      "Epoch 1/1\n",
      "144/144 [==============================] - 0s 327us/step - loss: 0.2198 - acc: 0.0500 - val_loss: 0.2230 - val_acc: 0.0417\n",
      "Train on 356 samples, validate on 356 samples\n",
      "Epoch 1/1\n",
      "356/356 [==============================] - 0s 281us/step - loss: 0.2196 - acc: 0.0865 - val_loss: 0.2225 - val_acc: 0.0618\n",
      "Train on 234 samples, validate on 234 samples\n",
      "Epoch 1/1\n",
      "234/234 [==============================] - 0s 228us/step - loss: 0.2140 - acc: 0.1162 - val_loss: 0.2067 - val_acc: 0.1376\n",
      "Train on 184 samples, validate on 184 samples\n",
      "Epoch 1/1\n",
      "184/184 [==============================] - 0s 288us/step - loss: 0.2183 - acc: 0.0826 - val_loss: 0.2187 - val_acc: 0.0696\n",
      "Train on 238 samples, validate on 238 samples\n",
      "Epoch 1/1\n",
      "238/238 [==============================] - 0s 248us/step - loss: 0.2192 - acc: 0.0496 - val_loss: 0.2107 - val_acc: 0.0563\n",
      "Train on 340 samples, validate on 340 samples\n",
      "Epoch 1/1\n",
      "340/340 [==============================] - 0s 298us/step - loss: 0.2163 - acc: 0.0518 - val_loss: 0.2078 - val_acc: 0.0471\n",
      "Train on 220 samples, validate on 220 samples\n",
      "Epoch 1/1\n",
      "220/220 [==============================] - 0s 234us/step - loss: 0.2121 - acc: 0.0909 - val_loss: 0.2129 - val_acc: 0.0845\n",
      "Train on 260 samples, validate on 260 samples\n",
      "Epoch 1/1\n",
      "260/260 [==============================] - 0s 347us/step - loss: 0.2173 - acc: 0.0262 - val_loss: 0.2092 - val_acc: 0.0608\n",
      "Train on 216 samples, validate on 216 samples\n",
      "Epoch 1/1\n",
      "216/216 [==============================] - 0s 237us/step - loss: 0.2080 - acc: 0.0815 - val_loss: 0.1964 - val_acc: 0.0963\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 264us/step - loss: 0.2141 - acc: 0.0370 - val_loss: 0.2048 - val_acc: 0.0180\n",
      "Train on 124 samples, validate on 124 samples\n",
      "Epoch 1/1\n",
      "124/124 [==============================] - 0s 351us/step - loss: 0.2192 - acc: 0.0258 - val_loss: 0.2295 - val_acc: 0.0177\n",
      "Train on 192 samples, validate on 192 samples\n",
      "Epoch 1/1\n",
      "192/192 [==============================] - 0s 244us/step - loss: 0.2249 - acc: 0.0198 - val_loss: 0.2179 - val_acc: 0.0250\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 334us/step - loss: 0.2200 - acc: 0.0256 - val_loss: 0.2180 - val_acc: 0.0156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 282 samples, validate on 282 samples\n",
      "Epoch 1/1\n",
      "282/282 [==============================] - 0s 376us/step - loss: 0.2137 - acc: 0.0191 - val_loss: 0.2139 - val_acc: 0.0149\n",
      "Train on 220 samples, validate on 220 samples\n",
      "Epoch 1/1\n",
      "220/220 [==============================] - 0s 242us/step - loss: 0.2106 - acc: 0.0355 - val_loss: 0.2072 - val_acc: 0.0255\n",
      "Train on 194 samples, validate on 194 samples\n",
      "Epoch 1/1\n",
      "194/194 [==============================] - 0s 260us/step - loss: 0.2176 - acc: 0.0103 - val_loss: 0.2143 - val_acc: 0.0155\n",
      "Train on 302 samples, validate on 302 samples\n",
      "Epoch 1/1\n",
      "302/302 [==============================] - 0s 353us/step - loss: 0.2264 - acc: 0.0033 - val_loss: 0.2272 - val_acc: 0.0033\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 252us/step - loss: 0.2114 - acc: 0.0090 - val_loss: 0.2075 - val_acc: 0.0120\n",
      "Train on 160 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 293us/step - loss: 0.2106 - acc: 0.0312 - val_loss: 0.1949 - val_acc: 0.0100\n",
      "Train on 158 samples, validate on 158 samples\n",
      "Epoch 1/1\n",
      "158/158 [==============================] - 0s 323us/step - loss: 0.2133 - acc: 0.0089 - val_loss: 0.2006 - val_acc: 0.0190\n",
      "Train on 154 samples, validate on 154 samples\n",
      "Epoch 1/1\n",
      "154/154 [==============================] - 0s 306us/step - loss: 0.2260 - acc: 0.0065 - val_loss: 0.2175 - val_acc: 0.0091\n",
      "Train on 166 samples, validate on 166 samples\n",
      "Epoch 1/1\n",
      "166/166 [==============================] - 0s 307us/step - loss: 0.2169 - acc: 0.0072 - val_loss: 0.2033 - val_acc: 0.0072\n",
      "Train on 224 samples, validate on 224 samples\n",
      "Epoch 1/1\n",
      "224/224 [==============================] - 0s 232us/step - loss: 0.2007 - acc: 0.0107 - val_loss: 0.1943 - val_acc: 0.0018\n",
      "Train on 294 samples, validate on 294 samples\n",
      "Epoch 1/1\n",
      "294/294 [==============================] - 0s 311us/step - loss: 0.2017 - acc: 0.0116 - val_loss: 0.2063 - val_acc: 0.0163\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 377us/step - loss: 0.1980 - acc: 0.0013 - val_loss: 0.1935 - val_acc: 0.0040\n",
      "Train on 196 samples, validate on 196 samples\n",
      "Epoch 1/1\n",
      "196/196 [==============================] - 0s 244us/step - loss: 0.2145 - acc: 0.0112 - val_loss: 0.2051 - val_acc: 0.0153\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 333us/step - loss: 0.2238 - acc: 0.0054 - val_loss: 0.2138 - val_acc: 0.0068\n",
      "Train on 234 samples, validate on 234 samples\n",
      "Epoch 1/1\n",
      "234/234 [==============================] - 0s 263us/step - loss: 0.2050 - acc: 0.0085 - val_loss: 0.2050 - val_acc: 0.0120\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 309us/step - loss: 0.2165 - acc: 0.0256 - val_loss: 0.2083 - val_acc: 0.0456\n",
      "Train on 130 samples, validate on 130 samples\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 331us/step - loss: 0.1857 - acc: 0.0169 - val_loss: 0.2008 - val_acc: 0.0354\n",
      "Train on 260 samples, validate on 260 samples\n",
      "Epoch 1/1\n",
      "260/260 [==============================] - 0s 327us/step - loss: 0.2109 - acc: 0.0162 - val_loss: 0.1988 - val_acc: 0.0438\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 309us/step - loss: 0.1970 - acc: 0.0080 - val_loss: 0.2021 - val_acc: 0.0067\n",
      "Train on 138 samples, validate on 138 samples\n",
      "Epoch 1/1\n",
      "138/138 [==============================] - 0s 324us/step - loss: 0.2005 - acc: 0.0000e+00 - val_loss: 0.2040 - val_acc: 0.0000e+00\n",
      "Train on 184 samples, validate on 184 samples\n",
      "Epoch 1/1\n",
      "184/184 [==============================] - 0s 258us/step - loss: 0.1984 - acc: 0.0022 - val_loss: 0.2043 - val_acc: 0.0000e+00\n",
      "Train on 206 samples, validate on 206 samples\n",
      "Epoch 1/1\n",
      "206/206 [==============================] - 0s 248us/step - loss: 0.1941 - acc: 0.0039 - val_loss: 0.2074 - val_acc: 0.0068\n",
      "Train on 174 samples, validate on 174 samples\n",
      "Epoch 1/1\n",
      "174/174 [==============================] - 0s 265us/step - loss: 0.2105 - acc: 0.0069 - val_loss: 0.1993 - val_acc: 0.0057\n",
      "Train on 278 samples, validate on 278 samples\n",
      "Epoch 1/1\n",
      "278/278 [==============================] - 0s 330us/step - loss: 0.2143 - acc: 0.0029 - val_loss: 0.2104 - val_acc: 0.0014\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 304us/step - loss: 0.2057 - acc: 0.0154 - val_loss: 0.2053 - val_acc: 0.0077\n",
      "Train on 160 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 293us/step - loss: 0.2209 - acc: 0.0025 - val_loss: 0.2253 - val_acc: 0.0137\n",
      "Train on 282 samples, validate on 282 samples\n",
      "Epoch 1/1\n",
      "282/282 [==============================] - 0s 320us/step - loss: 0.2408 - acc: 0.0191 - val_loss: 0.2221 - val_acc: 0.0489\n",
      "Train on 138 samples, validate on 138 samples\n",
      "Epoch 1/1\n",
      "138/138 [==============================] - 0s 346us/step - loss: 0.2107 - acc: 0.0043 - val_loss: 0.2138 - val_acc: 0.0043\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 313us/step - loss: 0.2087 - acc: 0.0256 - val_loss: 0.1946 - val_acc: 0.0244\n",
      "Train on 154 samples, validate on 154 samples\n",
      "Epoch 1/1\n",
      "154/154 [==============================] - 0s 298us/step - loss: 0.2045 - acc: 0.0156 - val_loss: 0.2052 - val_acc: 0.0143\n",
      "Train on 262 samples, validate on 262 samples\n",
      "Epoch 1/1\n",
      "262/262 [==============================] - 0s 336us/step - loss: 0.2368 - acc: 0.0328 - val_loss: 0.2253 - val_acc: 0.0198\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 292us/step - loss: 0.2005 - acc: 0.0205 - val_loss: 0.1958 - val_acc: 0.0154\n",
      "Train on 158 samples, validate on 158 samples\n",
      "Epoch 1/1\n",
      "158/158 [==============================] - 0s 294us/step - loss: 0.2299 - acc: 0.0063 - val_loss: 0.2424 - val_acc: 0.0063\n",
      "Train on 188 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 264us/step - loss: 0.2114 - acc: 0.0521 - val_loss: 0.2064 - val_acc: 0.0585\n",
      "Train on 216 samples, validate on 216 samples\n",
      "Epoch 1/1\n",
      "216/216 [==============================] - 0s 249us/step - loss: 0.2149 - acc: 0.0065 - val_loss: 0.1988 - val_acc: 0.0185\n",
      "Train on 176 samples, validate on 176 samples\n",
      "Epoch 1/1\n",
      "176/176 [==============================] - 0s 268us/step - loss: 0.2075 - acc: 0.0273 - val_loss: 0.2118 - val_acc: 0.0477\n",
      "Train on 212 samples, validate on 212 samples\n",
      "Epoch 1/1\n",
      "212/212 [==============================] - 0s 236us/step - loss: 0.2021 - acc: 0.0198 - val_loss: 0.2027 - val_acc: 0.0057\n",
      "Train on 330 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "330/330 [==============================] - 0s 291us/step - loss: 0.2243 - acc: 0.0036 - val_loss: 0.2210 - val_acc: 0.0048\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 276us/step - loss: 0.2255 - acc: 0.0011 - val_loss: 0.1913 - val_acc: 0.0033\n",
      "Train on 256 samples, validate on 256 samples\n",
      "Epoch 1/1\n",
      "256/256 [==============================] - 0s 209us/step - loss: 0.2135 - acc: 0.0094 - val_loss: 0.1942 - val_acc: 0.0234\n",
      "Train on 264 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "264/264 [==============================] - 0s 339us/step - loss: 0.2167 - acc: 0.0106 - val_loss: 0.1942 - val_acc: 0.0152\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 241us/step - loss: 0.2114 - acc: 0.0060 - val_loss: 0.1880 - val_acc: 0.0110\n",
      "Train on 206 samples, validate on 206 samples\n",
      "Epoch 1/1\n",
      "206/206 [==============================] - 0s 254us/step - loss: 0.2007 - acc: 0.0058 - val_loss: 0.1986 - val_acc: 0.0126\n",
      "Train on 288 samples, validate on 288 samples\n",
      "Epoch 1/1\n",
      "288/288 [==============================] - 0s 340us/step - loss: 0.2200 - acc: 0.0035 - val_loss: 0.2022 - val_acc: 0.0042\n",
      "Train on 222 samples, validate on 222 samples\n",
      "Epoch 1/1\n",
      "222/222 [==============================] - 0s 228us/step - loss: 0.2086 - acc: 0.0117 - val_loss: 0.1979 - val_acc: 0.0090\n",
      "Train on 288 samples, validate on 288 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288/288 [==============================] - 0s 338us/step - loss: 0.1980 - acc: 0.0028 - val_loss: 0.2005 - val_acc: 0.0021\n",
      "Train on 78 samples, validate on 78 samples\n",
      "Epoch 1/1\n",
      "78/78 [==============================] - 0s 512us/step - loss: 0.2108 - acc: 0.0026 - val_loss: 0.2065 - val_acc: 0.0000e+00\n",
      "Train on 164 samples, validate on 164 samples\n",
      "Epoch 1/1\n",
      "164/164 [==============================] - 0s 286us/step - loss: 0.2113 - acc: 0.0098 - val_loss: 0.2100 - val_acc: 0.0073\n",
      "Train on 204 samples, validate on 204 samples\n",
      "Epoch 1/1\n",
      "204/204 [==============================] - 0s 246us/step - loss: 0.1948 - acc: 0.0176 - val_loss: 0.1844 - val_acc: 0.0127\n",
      "Train on 220 samples, validate on 220 samples\n",
      "Epoch 1/1\n",
      "220/220 [==============================] - 0s 230us/step - loss: 0.2064 - acc: 0.0055 - val_loss: 0.2073 - val_acc: 0.0191\n",
      "Train on 194 samples, validate on 194 samples\n",
      "Epoch 1/1\n",
      "194/194 [==============================] - 0s 255us/step - loss: 0.2163 - acc: 0.0165 - val_loss: 0.2055 - val_acc: 0.0113\n",
      "Train on 196 samples, validate on 196 samples\n",
      "Epoch 1/1\n",
      "196/196 [==============================] - 0s 260us/step - loss: 0.1968 - acc: 0.0102 - val_loss: 0.1903 - val_acc: 0.0204\n",
      "Train on 114 samples, validate on 114 samples\n",
      "Epoch 1/1\n",
      "114/114 [==============================] - 0s 449us/step - loss: 0.2114 - acc: 0.0018 - val_loss: 0.2010 - val_acc: 0.0018\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 246us/step - loss: 0.1975 - acc: 0.0050 - val_loss: 0.1926 - val_acc: 0.0099\n",
      "Train on 134 samples, validate on 134 samples\n",
      "Epoch 1/1\n",
      "134/134 [==============================] - 0s 356us/step - loss: 0.2180 - acc: 0.0134 - val_loss: 0.2040 - val_acc: 0.0119\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 295us/step - loss: 0.1981 - acc: 0.0014 - val_loss: 0.1820 - val_acc: 0.0068\n",
      "Train on 212 samples, validate on 212 samples\n",
      "Epoch 1/1\n",
      "212/212 [==============================] - 0s 242us/step - loss: 0.2139 - acc: 0.0038 - val_loss: 0.2315 - val_acc: 0.0047\n",
      "Train on 210 samples, validate on 210 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 0s 237us/step - loss: 0.2228 - acc: 0.0000e+00 - val_loss: 0.2170 - val_acc: 0.0029\n",
      "Train on 158 samples, validate on 158 samples\n",
      "Epoch 1/1\n",
      "158/158 [==============================] - 0s 413us/step - loss: 0.2081 - acc: 0.0025 - val_loss: 0.2128 - val_acc: 0.0038\n",
      "Train on 198 samples, validate on 198 samples\n",
      "Epoch 1/1\n",
      "198/198 [==============================] - 0s 371us/step - loss: 0.2188 - acc: 0.0030 - val_loss: 0.2159 - val_acc: 0.0020\n",
      "Train on 206 samples, validate on 206 samples\n",
      "Epoch 1/1\n",
      "206/206 [==============================] - 0s 451us/step - loss: 0.2194 - acc: 0.0097 - val_loss: 0.2085 - val_acc: 0.0117\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 397us/step - loss: 0.2177 - acc: 0.0156 - val_loss: 0.2078 - val_acc: 0.0189\n",
      "Train on 214 samples, validate on 214 samples\n",
      "Epoch 1/1\n",
      "214/214 [==============================] - 0s 402us/step - loss: 0.2208 - acc: 0.0028 - val_loss: 0.2221 - val_acc: 0.0047\n",
      "Train on 186 samples, validate on 186 samples\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 453us/step - loss: 0.2179 - acc: 0.0065 - val_loss: 0.2066 - val_acc: 0.0075\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 543us/step - loss: 0.2166 - acc: 0.0056 - val_loss: 0.2102 - val_acc: 0.0022\n",
      "Train on 222 samples, validate on 222 samples\n",
      "Epoch 1/1\n",
      "222/222 [==============================] - 0s 247us/step - loss: 0.2130 - acc: 0.0000e+00 - val_loss: 0.2103 - val_acc: 0.0045\n",
      "Train on 296 samples, validate on 296 samples\n",
      "Epoch 1/1\n",
      "296/296 [==============================] - 0s 346us/step - loss: 0.2150 - acc: 0.0115 - val_loss: 0.2080 - val_acc: 0.0101\n",
      "Train on 230 samples, validate on 230 samples\n",
      "Epoch 1/1\n",
      "230/230 [==============================] - 0s 271us/step - loss: 0.2022 - acc: 0.0017 - val_loss: 0.2038 - val_acc: 0.0026\n",
      "Train on 190 samples, validate on 190 samples\n",
      "Epoch 1/1\n",
      "190/190 [==============================] - 0s 291us/step - loss: 0.2141 - acc: 0.0021 - val_loss: 0.2135 - val_acc: 0.0074\n",
      "Train on 186 samples, validate on 186 samples\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 280us/step - loss: 0.2211 - acc: 0.0011 - val_loss: 0.2161 - val_acc: 0.0022\n",
      "Train on 218 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "218/218 [==============================] - 0s 260us/step - loss: 0.2153 - acc: 0.0018 - val_loss: 0.2117 - val_acc: 0.0018\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 279us/step - loss: 0.2056 - acc: 9.9010e-04 - val_loss: 0.2100 - val_acc: 9.9010e-04\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 325us/step - loss: 0.2051 - acc: 0.0027 - val_loss: 0.2130 - val_acc: 0.0014\n",
      "Train on 252 samples, validate on 252 samples\n",
      "Epoch 1/1\n",
      "252/252 [==============================] - 0s 213us/step - loss: 0.2165 - acc: 0.0024 - val_loss: 0.2035 - val_acc: 0.0024\n",
      "Train on 152 samples, validate on 152 samples\n",
      "Epoch 1/1\n",
      "152/152 [==============================] - 0s 291us/step - loss: 0.2094 - acc: 0.0039 - val_loss: 0.2157 - val_acc: 0.0013\n",
      "Train on 118 samples, validate on 118 samples\n",
      "Epoch 1/1\n",
      "118/118 [==============================] - 0s 350us/step - loss: 0.2104 - acc: 0.0017 - val_loss: 0.2111 - val_acc: 0.0034\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/1\n",
      "232/232 [==============================] - 0s 269us/step - loss: 0.2117 - acc: 0.0043 - val_loss: 0.2150 - val_acc: 0.0017\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 320us/step - loss: 0.1895 - acc: 0.0027 - val_loss: 0.1958 - val_acc: 0.0027\n",
      "Train on 144 samples, validate on 144 samples\n",
      "Epoch 1/1\n",
      "144/144 [==============================] - 0s 352us/step - loss: 0.2215 - acc: 0.0000e+00 - val_loss: 0.2070 - val_acc: 0.0000e+00\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 336us/step - loss: 0.2045 - acc: 0.0038 - val_loss: 0.1918 - val_acc: 0.0064\n",
      "Train on 212 samples, validate on 212 samples\n",
      "Epoch 1/1\n",
      "212/212 [==============================] - 0s 254us/step - loss: 0.2109 - acc: 0.0019 - val_loss: 0.2010 - val_acc: 0.0019\n",
      "Train on 194 samples, validate on 194 samples\n",
      "Epoch 1/1\n",
      "194/194 [==============================] - 0s 305us/step - loss: 0.2009 - acc: 0.0031 - val_loss: 0.2024 - val_acc: 0.0041\n",
      "Train on 164 samples, validate on 164 samples\n",
      "Epoch 1/1\n",
      "164/164 [==============================] - 0s 373us/step - loss: 0.2217 - acc: 0.0000e+00 - val_loss: 0.2072 - val_acc: 0.0012\n",
      "Train on 346 samples, validate on 346 samples\n",
      "Epoch 1/1\n",
      "346/346 [==============================] - 0s 358us/step - loss: 0.2125 - acc: 0.0029 - val_loss: 0.2000 - val_acc: 0.0023\n",
      "Train on 172 samples, validate on 172 samples\n",
      "Epoch 1/1\n",
      "172/172 [==============================] - 0s 279us/step - loss: 0.2130 - acc: 0.0047 - val_loss: 0.2183 - val_acc: 0.0035\n",
      "Train on 286 samples, validate on 286 samples\n",
      "Epoch 1/1\n",
      "286/286 [==============================] - 0s 330us/step - loss: 0.1879 - acc: 0.0028 - val_loss: 0.1921 - val_acc: 0.0028\n",
      "Train on 186 samples, validate on 186 samples\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 263us/step - loss: 0.2067 - acc: 0.0032 - val_loss: 0.1927 - val_acc: 0.0022\n",
      "Train on 216 samples, validate on 216 samples\n",
      "Epoch 1/1\n",
      "216/216 [==============================] - 0s 268us/step - loss: 0.2021 - acc: 0.0028 - val_loss: 0.2048 - val_acc: 0.0028\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 310us/step - loss: 0.1954 - acc: 0.0053 - val_loss: 0.2010 - val_acc: 0.0067\n",
      "Train on 132 samples, validate on 132 samples\n",
      "Epoch 1/1\n",
      "132/132 [==============================] - 0s 349us/step - loss: 0.1901 - acc: 0.0030 - val_loss: 0.1981 - val_acc: 0.0000e+00\n",
      "Train on 280 samples, validate on 280 samples\n",
      "Epoch 1/1\n",
      "280/280 [==============================] - 0s 329us/step - loss: 0.1827 - acc: 0.0029 - val_loss: 0.1938 - val_acc: 0.0050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 338us/step - loss: 0.2284 - acc: 0.0027 - val_loss: 0.2092 - val_acc: 0.0000e+00\n",
      "Train on 190 samples, validate on 190 samples\n",
      "Epoch 1/1\n",
      "190/190 [==============================] - 0s 263us/step - loss: 0.2091 - acc: 0.0032 - val_loss: 0.2019 - val_acc: 0.0000e+00\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 320us/step - loss: 0.1998 - acc: 0.0027 - val_loss: 0.2078 - val_acc: 0.0027\n",
      "Train on 140 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "140/140 [==============================] - 0s 317us/step - loss: 0.2012 - acc: 0.0014 - val_loss: 0.2007 - val_acc: 0.0043\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 281us/step - loss: 0.2031 - acc: 0.0022 - val_loss: 0.2115 - val_acc: 0.0011\n",
      "Train on 142 samples, validate on 142 samples\n",
      "Epoch 1/1\n",
      "142/142 [==============================] - 0s 331us/step - loss: 0.2239 - acc: 0.0042 - val_loss: 0.2266 - val_acc: 0.0000e+00\n",
      "Train on 108 samples, validate on 108 samples\n",
      "Epoch 1/1\n",
      "108/108 [==============================] - 0s 431us/step - loss: 0.2130 - acc: 0.0037 - val_loss: 0.2109 - val_acc: 0.0000e+00\n",
      "Train on 184 samples, validate on 184 samples\n",
      "Epoch 1/1\n",
      "184/184 [==============================] - 0s 263us/step - loss: 0.2175 - acc: 0.0000e+00 - val_loss: 0.2109 - val_acc: 0.0011\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 263us/step - loss: 0.2183 - acc: 0.0000e+00 - val_loss: 0.2188 - val_acc: 0.0000e+00\n",
      "Train on 152 samples, validate on 152 samples\n",
      "Epoch 1/1\n",
      "152/152 [==============================] - 0s 293us/step - loss: 0.2117 - acc: 0.0013 - val_loss: 0.2026 - val_acc: 0.0013\n",
      "Train on 174 samples, validate on 174 samples\n",
      "Epoch 1/1\n",
      "174/174 [==============================] - 0s 339us/step - loss: 0.2099 - acc: 0.0000e+00 - val_loss: 0.2066 - val_acc: 0.0000e+00\n",
      "Train on 140 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "140/140 [==============================] - 0s 509us/step - loss: 0.2140 - acc: 0.0014 - val_loss: 0.1932 - val_acc: 0.0014\n",
      "Train on 228 samples, validate on 228 samples\n",
      "Epoch 1/1\n",
      "228/228 [==============================] - 0s 310us/step - loss: 0.2229 - acc: 0.0035 - val_loss: 0.2071 - val_acc: 8.7719e-04\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 389us/step - loss: 0.2002 - acc: 9.9010e-04 - val_loss: 0.2001 - val_acc: 9.9010e-04\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 416us/step - loss: 0.2143 - acc: 0.0033 - val_loss: 0.2043 - val_acc: 0.0033\n",
      "Train on 242 samples, validate on 242 samples\n",
      "Epoch 1/1\n",
      "242/242 [==============================] - 0s 331us/step - loss: 0.2110 - acc: 0.0099 - val_loss: 0.2106 - val_acc: 0.0099\n",
      "Train on 144 samples, validate on 144 samples\n",
      "Epoch 1/1\n",
      "144/144 [==============================] - 0s 481us/step - loss: 0.2043 - acc: 0.0083 - val_loss: 0.2051 - val_acc: 0.0042\n",
      "Train on 356 samples, validate on 356 samples\n",
      "Epoch 1/1\n",
      "356/356 [==============================] - 0s 426us/step - loss: 0.2045 - acc: 0.0022 - val_loss: 0.2059 - val_acc: 0.0022\n",
      "Train on 234 samples, validate on 234 samples\n",
      "Epoch 1/1\n",
      "234/234 [==============================] - 0s 274us/step - loss: 0.1973 - acc: 0.0068 - val_loss: 0.1849 - val_acc: 0.0043\n",
      "Train on 184 samples, validate on 184 samples\n",
      "Epoch 1/1\n",
      "184/184 [==============================] - 0s 333us/step - loss: 0.1986 - acc: 0.0065 - val_loss: 0.2009 - val_acc: 0.0054\n",
      "Train on 238 samples, validate on 238 samples\n",
      "Epoch 1/1\n",
      "238/238 [==============================] - 0s 248us/step - loss: 0.2045 - acc: 0.0042 - val_loss: 0.1944 - val_acc: 0.0034\n",
      "Train on 340 samples, validate on 340 samples\n",
      "Epoch 1/1\n",
      "340/340 [==============================] - 0s 329us/step - loss: 0.2012 - acc: 0.0035 - val_loss: 0.1905 - val_acc: 0.0018\n",
      "Train on 220 samples, validate on 220 samples\n",
      "Epoch 1/1\n",
      "220/220 [==============================] - 0s 245us/step - loss: 0.1977 - acc: 0.0000e+00 - val_loss: 0.1989 - val_acc: 0.0000e+00\n",
      "Train on 260 samples, validate on 260 samples\n",
      "Epoch 1/1\n",
      "260/260 [==============================] - 0s 350us/step - loss: 0.2057 - acc: 7.6923e-04 - val_loss: 0.1933 - val_acc: 0.0015\n",
      "Train on 216 samples, validate on 216 samples\n",
      "Epoch 1/1\n",
      "216/216 [==============================] - 0s 243us/step - loss: 0.1907 - acc: 0.0065 - val_loss: 0.1840 - val_acc: 9.2593e-04\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 254us/step - loss: 0.2059 - acc: 0.0050 - val_loss: 0.1935 - val_acc: 0.0060\n",
      "Train on 124 samples, validate on 124 samples\n",
      "Epoch 1/1\n",
      "124/124 [==============================] - 0s 340us/step - loss: 0.2068 - acc: 0.0048 - val_loss: 0.2235 - val_acc: 0.0016\n",
      "Train on 192 samples, validate on 192 samples\n",
      "Epoch 1/1\n",
      "192/192 [==============================] - 0s 241us/step - loss: 0.2208 - acc: 0.0021 - val_loss: 0.2104 - val_acc: 0.0021\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 277us/step - loss: 0.2091 - acc: 0.0044 - val_loss: 0.2099 - val_acc: 0.0011\n",
      "Train on 282 samples, validate on 282 samples\n",
      "Epoch 1/1\n",
      "282/282 [==============================] - 0s 333us/step - loss: 0.2063 - acc: 0.0028 - val_loss: 0.2042 - val_acc: 7.0922e-04\n",
      "Train on 220 samples, validate on 220 samples\n",
      "Epoch 1/1\n",
      "220/220 [==============================] - 0s 267us/step - loss: 0.1987 - acc: 0.0000e+00 - val_loss: 0.1965 - val_acc: 0.0000e+00\n",
      "Train on 194 samples, validate on 194 samples\n",
      "Epoch 1/1\n",
      "194/194 [==============================] - 0s 269us/step - loss: 0.2092 - acc: 0.0021 - val_loss: 0.2030 - val_acc: 0.0010\n",
      "Train on 302 samples, validate on 302 samples\n",
      "Epoch 1/1\n",
      "302/302 [==============================] - 0s 462us/step - loss: 0.2113 - acc: 0.0013 - val_loss: 0.2143 - val_acc: 0.0033\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 428us/step - loss: 0.1999 - acc: 0.0010 - val_loss: 0.1945 - val_acc: 0.0010\n",
      "Train on 160 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 443us/step - loss: 0.2039 - acc: 0.0037 - val_loss: 0.1835 - val_acc: 0.0025\n",
      "Train on 158 samples, validate on 158 samples\n",
      "Epoch 1/1\n",
      "158/158 [==============================] - 0s 491us/step - loss: 0.2113 - acc: 0.0063 - val_loss: 0.1968 - val_acc: 0.0051\n",
      "Train on 154 samples, validate on 154 samples\n",
      "Epoch 1/1\n",
      "154/154 [==============================] - 0s 494us/step - loss: 0.2203 - acc: 0.0013 - val_loss: 0.2121 - val_acc: 0.0026\n",
      "Train on 166 samples, validate on 166 samples\n",
      "Epoch 1/1\n",
      "166/166 [==============================] - 0s 600us/step - loss: 0.2061 - acc: 0.0012 - val_loss: 0.1945 - val_acc: 0.0024\n",
      "Train on 224 samples, validate on 224 samples\n",
      "Epoch 1/1\n",
      "224/224 [==============================] - 0s 216us/step - loss: 0.1927 - acc: 0.0027 - val_loss: 0.1840 - val_acc: 8.9286e-04\n",
      "Train on 294 samples, validate on 294 samples\n",
      "Epoch 1/1\n",
      "294/294 [==============================] - 0s 309us/step - loss: 0.1959 - acc: 0.0034 - val_loss: 0.1989 - val_acc: 0.0014\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 357us/step - loss: 0.1894 - acc: 0.0000e+00 - val_loss: 0.1845 - val_acc: 0.0053\n",
      "Train on 196 samples, validate on 196 samples\n",
      "Epoch 1/1\n",
      "196/196 [==============================] - 0s 237us/step - loss: 0.2065 - acc: 0.0020 - val_loss: 0.1950 - val_acc: 0.0031\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 321us/step - loss: 0.2149 - acc: 0.0041 - val_loss: 0.2060 - val_acc: 0.0054\n",
      "Train on 234 samples, validate on 234 samples\n",
      "Epoch 1/1\n",
      "234/234 [==============================] - 0s 226us/step - loss: 0.1992 - acc: 8.5470e-04 - val_loss: 0.1976 - val_acc: 0.0060\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 269us/step - loss: 0.2086 - acc: 0.0033 - val_loss: 0.2006 - val_acc: 0.0011\n",
      "Train on 130 samples, validate on 130 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 0s 341us/step - loss: 0.1774 - acc: 0.0000e+00 - val_loss: 0.1941 - val_acc: 0.0046\n",
      "Train on 260 samples, validate on 260 samples\n",
      "Epoch 1/1\n",
      "260/260 [==============================] - 0s 366us/step - loss: 0.2025 - acc: 0.0000e+00 - val_loss: 0.1913 - val_acc: 0.0046\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 308us/step - loss: 0.1884 - acc: 0.0013 - val_loss: 0.1927 - val_acc: 0.0000e+00\n",
      "Train on 138 samples, validate on 138 samples\n",
      "Epoch 1/1\n",
      "138/138 [==============================] - 0s 326us/step - loss: 0.1888 - acc: 0.0000e+00 - val_loss: 0.1923 - val_acc: 0.0029\n",
      "Train on 184 samples, validate on 184 samples\n",
      "Epoch 1/1\n",
      "184/184 [==============================] - 0s 271us/step - loss: 0.1864 - acc: 0.0022 - val_loss: 0.1921 - val_acc: 0.0000e+00\n",
      "Train on 206 samples, validate on 206 samples\n",
      "Epoch 1/1\n",
      "206/206 [==============================] - 0s 253us/step - loss: 0.1874 - acc: 0.0029 - val_loss: 0.2021 - val_acc: 0.0019\n",
      "Train on 174 samples, validate on 174 samples\n",
      "Epoch 1/1\n",
      "174/174 [==============================] - 0s 275us/step - loss: 0.2028 - acc: 0.0011 - val_loss: 0.1915 - val_acc: 0.0023\n",
      "Train on 278 samples, validate on 278 samples\n",
      "Epoch 1/1\n",
      "278/278 [==============================] - 0s 335us/step - loss: 0.2030 - acc: 0.0029 - val_loss: 0.1971 - val_acc: 0.0000e+00\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 302us/step - loss: 0.1959 - acc: 0.0077 - val_loss: 0.1954 - val_acc: 0.0013\n",
      "Train on 160 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 296us/step - loss: 0.2085 - acc: 0.0037 - val_loss: 0.2107 - val_acc: 0.0037\n",
      "Train on 282 samples, validate on 282 samples\n",
      "Epoch 1/1\n",
      "282/282 [==============================] - 0s 328us/step - loss: 0.2327 - acc: 0.0028 - val_loss: 0.2130 - val_acc: 0.0043\n",
      "Train on 138 samples, validate on 138 samples\n",
      "Epoch 1/1\n",
      "138/138 [==============================] - 0s 323us/step - loss: 0.2020 - acc: 0.0000e+00 - val_loss: 0.2020 - val_acc: 0.0014\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 417us/step - loss: 0.2015 - acc: 0.0000e+00 - val_loss: 0.1840 - val_acc: 0.0013\n",
      "Train on 154 samples, validate on 154 samples\n",
      "Epoch 1/1\n",
      "154/154 [==============================] - 0s 407us/step - loss: 0.1975 - acc: 0.0052 - val_loss: 0.1959 - val_acc: 0.0000e+00\n",
      "Train on 262 samples, validate on 262 samples\n",
      "Epoch 1/1\n",
      "262/262 [==============================] - 0s 553us/step - loss: 0.2332 - acc: 0.0015 - val_loss: 0.2190 - val_acc: 0.0031\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 470us/step - loss: 0.1899 - acc: 0.0064 - val_loss: 0.1812 - val_acc: 0.0038\n",
      "Train on 158 samples, validate on 158 samples\n",
      "Epoch 1/1\n",
      "158/158 [==============================] - 0s 450us/step - loss: 0.2221 - acc: 0.0063 - val_loss: 0.2288 - val_acc: 0.0025\n",
      "Train on 188 samples, validate on 188 samples\n",
      "Epoch 1/1\n",
      "188/188 [==============================] - 0s 373us/step - loss: 0.2002 - acc: 0.0043 - val_loss: 0.1976 - val_acc: 0.0128\n",
      "Train on 216 samples, validate on 216 samples\n",
      "Epoch 1/1\n",
      "216/216 [==============================] - 0s 401us/step - loss: 0.2041 - acc: 9.2593e-04 - val_loss: 0.1873 - val_acc: 0.0046\n",
      "Train on 176 samples, validate on 176 samples\n",
      "Epoch 1/1\n",
      "176/176 [==============================] - 0s 258us/step - loss: 0.1964 - acc: 0.0102 - val_loss: 0.2019 - val_acc: 0.0045\n",
      "Train on 212 samples, validate on 212 samples\n",
      "Epoch 1/1\n",
      "212/212 [==============================] - 0s 217us/step - loss: 0.1932 - acc: 0.0019 - val_loss: 0.1913 - val_acc: 0.0000e+00\n",
      "Train on 330 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "330/330 [==============================] - 0s 291us/step - loss: 0.2170 - acc: 0.0012 - val_loss: 0.2110 - val_acc: 0.0018\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 268us/step - loss: 0.2251 - acc: 0.0000e+00 - val_loss: 0.1815 - val_acc: 0.0022\n",
      "Train on 256 samples, validate on 256 samples\n",
      "Epoch 1/1\n",
      "256/256 [==============================] - 0s 210us/step - loss: 0.2091 - acc: 0.0023 - val_loss: 0.1835 - val_acc: 0.0047\n",
      "Train on 264 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "264/264 [==============================] - 0s 352us/step - loss: 0.2124 - acc: 0.0015 - val_loss: 0.1839 - val_acc: 7.5758e-04\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 270us/step - loss: 0.2076 - acc: 0.0030 - val_loss: 0.1753 - val_acc: 0.0020\n",
      "Train on 206 samples, validate on 206 samples\n",
      "Epoch 1/1\n",
      "206/206 [==============================] - 0s 250us/step - loss: 0.1898 - acc: 9.7087e-04 - val_loss: 0.1878 - val_acc: 0.0019\n",
      "Train on 288 samples, validate on 288 samples\n",
      "Epoch 1/1\n",
      "288/288 [==============================] - 0s 310us/step - loss: 0.2165 - acc: 0.0035 - val_loss: 0.1924 - val_acc: 0.0000e+00\n",
      "Train on 222 samples, validate on 222 samples\n",
      "Epoch 1/1\n",
      "222/222 [==============================] - 0s 224us/step - loss: 0.1993 - acc: 0.0000e+00 - val_loss: 0.1897 - val_acc: 0.0018\n",
      "Train on 288 samples, validate on 288 samples\n",
      "Epoch 1/1\n",
      "288/288 [==============================] - 0s 320us/step - loss: 0.1899 - acc: 6.9444e-04 - val_loss: 0.1919 - val_acc: 0.0000e+00\n",
      "Train on 78 samples, validate on 78 samples\n",
      "Epoch 1/1\n",
      "78/78 [==============================] - 0s 528us/step - loss: 0.2099 - acc: 0.0000e+00 - val_loss: 0.2007 - val_acc: 0.0000e+00\n",
      "Train on 164 samples, validate on 164 samples\n",
      "Epoch 1/1\n",
      "164/164 [==============================] - 0s 290us/step - loss: 0.2044 - acc: 0.0024 - val_loss: 0.2070 - val_acc: 0.0061\n",
      "Train on 204 samples, validate on 204 samples\n",
      "Epoch 1/1\n",
      "204/204 [==============================] - 0s 246us/step - loss: 0.1866 - acc: 0.0020 - val_loss: 0.1719 - val_acc: 0.0020\n",
      "Train on 220 samples, validate on 220 samples\n",
      "Epoch 1/1\n",
      "220/220 [==============================] - 0s 235us/step - loss: 0.1986 - acc: 0.0000e+00 - val_loss: 0.1999 - val_acc: 9.0909e-04\n",
      "Train on 194 samples, validate on 194 samples\n",
      "Epoch 1/1\n",
      "194/194 [==============================] - 0s 270us/step - loss: 0.2112 - acc: 0.0031 - val_loss: 0.1977 - val_acc: 0.0031\n",
      "Train on 196 samples, validate on 196 samples\n",
      "Epoch 1/1\n",
      "196/196 [==============================] - 0s 370us/step - loss: 0.1864 - acc: 0.0071 - val_loss: 0.1798 - val_acc: 0.0031\n",
      "Train on 114 samples, validate on 114 samples\n",
      "Epoch 1/1\n",
      "114/114 [==============================] - 0s 527us/step - loss: 0.2067 - acc: 0.0000e+00 - val_loss: 0.1946 - val_acc: 0.0035\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 324us/step - loss: 0.1861 - acc: 0.0030 - val_loss: 0.1804 - val_acc: 0.0050\n",
      "Train on 134 samples, validate on 134 samples\n",
      "Epoch 1/1\n",
      "134/134 [==============================] - 0s 514us/step - loss: 0.2119 - acc: 0.0030 - val_loss: 0.1967 - val_acc: 0.0030\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 445us/step - loss: 0.1831 - acc: 0.0014 - val_loss: 0.1682 - val_acc: 0.0041\n",
      "Train on 212 samples, validate on 212 samples\n",
      "Epoch 1/1\n",
      "212/212 [==============================] - 0s 351us/step - loss: 0.2091 - acc: 9.4340e-04 - val_loss: 0.2266 - val_acc: 0.0019\n",
      "Train on 210 samples, validate on 210 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 0s 338us/step - loss: 0.2205 - acc: 0.0029 - val_loss: 0.2121 - val_acc: 9.5238e-04\n",
      "Train on 158 samples, validate on 158 samples\n",
      "Epoch 1/1\n",
      "158/158 [==============================] - 0s 558us/step - loss: 0.2030 - acc: 0.0013 - val_loss: 0.2070 - val_acc: 0.0025\n",
      "Train on 198 samples, validate on 198 samples\n",
      "Epoch 1/1\n",
      "198/198 [==============================] - 0s 232us/step - loss: 0.2155 - acc: 0.0061 - val_loss: 0.2128 - val_acc: 0.0030\n",
      "Train on 206 samples, validate on 206 samples\n",
      "Epoch 1/1\n",
      "206/206 [==============================] - 0s 274us/step - loss: 0.2130 - acc: 0.0029 - val_loss: 0.1999 - val_acc: 0.0029\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 318us/step - loss: 0.2103 - acc: 0.0056 - val_loss: 0.2015 - val_acc: 0.0078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 214 samples, validate on 214 samples\n",
      "Epoch 1/1\n",
      "214/214 [==============================] - 0s 238us/step - loss: 0.2121 - acc: 0.0037 - val_loss: 0.2141 - val_acc: 0.0019\n",
      "Train on 186 samples, validate on 186 samples\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 267us/step - loss: 0.2148 - acc: 0.0000e+00 - val_loss: 0.1996 - val_acc: 0.0065\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 276us/step - loss: 0.2068 - acc: 0.0033 - val_loss: 0.2025 - val_acc: 0.0033\n",
      "Train on 222 samples, validate on 222 samples\n",
      "Epoch 1/1\n",
      "222/222 [==============================] - 0s 236us/step - loss: 0.2101 - acc: 0.0027 - val_loss: 0.2064 - val_acc: 0.0027\n",
      "Train on 296 samples, validate on 296 samples\n",
      "Epoch 1/1\n",
      "296/296 [==============================] - 0s 323us/step - loss: 0.2012 - acc: 6.7568e-04 - val_loss: 0.1949 - val_acc: 6.7568e-04\n",
      "Train on 230 samples, validate on 230 samples\n",
      "Epoch 1/1\n",
      "230/230 [==============================] - 0s 259us/step - loss: 0.1922 - acc: 0.0026 - val_loss: 0.1928 - val_acc: 0.0026\n",
      "Train on 190 samples, validate on 190 samples\n",
      "Epoch 1/1\n",
      "190/190 [==============================] - 0s 267us/step - loss: 0.2055 - acc: 0.0021 - val_loss: 0.2009 - val_acc: 0.0000e+00\n",
      "Train on 186 samples, validate on 186 samples\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 310us/step - loss: 0.2128 - acc: 0.0011 - val_loss: 0.2066 - val_acc: 0.0000e+00\n",
      "Train on 218 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "218/218 [==============================] - 0s 241us/step - loss: 0.2088 - acc: 0.0000e+00 - val_loss: 0.2001 - val_acc: 0.0000e+00\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 253us/step - loss: 0.1952 - acc: 0.0020 - val_loss: 0.2015 - val_acc: 9.9010e-04\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 313us/step - loss: 0.1989 - acc: 0.0027 - val_loss: 0.2006 - val_acc: 0.0000e+00\n",
      "Train on 252 samples, validate on 252 samples\n",
      "Epoch 1/1\n",
      "252/252 [==============================] - 0s 217us/step - loss: 0.2108 - acc: 0.0016 - val_loss: 0.1927 - val_acc: 0.0016\n",
      "Train on 152 samples, validate on 152 samples\n",
      "Epoch 1/1\n",
      "152/152 [==============================] - 0s 327us/step - loss: 0.2000 - acc: 0.0000e+00 - val_loss: 0.2112 - val_acc: 0.0013\n",
      "Train on 118 samples, validate on 118 samples\n",
      "Epoch 1/1\n",
      "118/118 [==============================] - 0s 362us/step - loss: 0.2049 - acc: 0.0017 - val_loss: 0.1996 - val_acc: 0.0051\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/1\n",
      "232/232 [==============================] - 0s 243us/step - loss: 0.2031 - acc: 0.0017 - val_loss: 0.2055 - val_acc: 8.6207e-04\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 391us/step - loss: 0.1809 - acc: 0.0000e+00 - val_loss: 0.1833 - val_acc: 0.0027\n",
      "Train on 144 samples, validate on 144 samples\n",
      "Epoch 1/1\n",
      "144/144 [==============================] - 0s 334us/step - loss: 0.2147 - acc: 0.0000e+00 - val_loss: 0.1963 - val_acc: 0.0000e+00\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 354us/step - loss: 0.1942 - acc: 0.0026 - val_loss: 0.1822 - val_acc: 0.0026\n",
      "Train on 212 samples, validate on 212 samples\n",
      "Epoch 1/1\n",
      "212/212 [==============================] - 0s 352us/step - loss: 0.2044 - acc: 0.0019 - val_loss: 0.1940 - val_acc: 0.0028\n",
      "Train on 194 samples, validate on 194 samples\n",
      "Epoch 1/1\n",
      "194/194 [==============================] - 0s 339us/step - loss: 0.1943 - acc: 0.0000e+00 - val_loss: 0.1920 - val_acc: 0.0021\n",
      "Train on 164 samples, validate on 164 samples\n",
      "Epoch 1/1\n",
      "164/164 [==============================] - 0s 479us/step - loss: 0.2157 - acc: 0.0000e+00 - val_loss: 0.1987 - val_acc: 0.0012\n",
      "Train on 346 samples, validate on 346 samples\n",
      "Epoch 1/1\n",
      "346/346 [==============================] - 0s 441us/step - loss: 0.2032 - acc: 0.0029 - val_loss: 0.1922 - val_acc: 5.7803e-04\n",
      "Train on 172 samples, validate on 172 samples\n",
      "Epoch 1/1\n",
      "172/172 [==============================] - 0s 457us/step - loss: 0.2024 - acc: 0.0023 - val_loss: 0.2075 - val_acc: 0.0035\n",
      "Train on 286 samples, validate on 286 samples\n",
      "Epoch 1/1\n",
      "286/286 [==============================] - 0s 463us/step - loss: 0.1780 - acc: 0.0021 - val_loss: 0.1834 - val_acc: 0.0035\n",
      "Train on 186 samples, validate on 186 samples\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 281us/step - loss: 0.2006 - acc: 0.0032 - val_loss: 0.1841 - val_acc: 0.0022\n",
      "Train on 216 samples, validate on 216 samples\n",
      "Epoch 1/1\n",
      "216/216 [==============================] - 0s 274us/step - loss: 0.1949 - acc: 0.0028 - val_loss: 0.1940 - val_acc: 0.0037\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 389us/step - loss: 0.1877 - acc: 0.0027 - val_loss: 0.1948 - val_acc: 0.0027\n",
      "Train on 132 samples, validate on 132 samples\n",
      "Epoch 1/1\n",
      "132/132 [==============================] - 0s 398us/step - loss: 0.1808 - acc: 0.0030 - val_loss: 0.1912 - val_acc: 0.0000e+00\n",
      "Train on 280 samples, validate on 280 samples\n",
      "Epoch 1/1\n",
      "280/280 [==============================] - 0s 389us/step - loss: 0.1764 - acc: 0.0014 - val_loss: 0.1864 - val_acc: 0.0029\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 352us/step - loss: 0.2194 - acc: 0.0014 - val_loss: 0.2003 - val_acc: 0.0000e+00\n",
      "Train on 190 samples, validate on 190 samples\n",
      "Epoch 1/1\n",
      "190/190 [==============================] - 0s 280us/step - loss: 0.2027 - acc: 0.0042 - val_loss: 0.1922 - val_acc: 0.0032\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 337us/step - loss: 0.1932 - acc: 0.0014 - val_loss: 0.1984 - val_acc: 0.0027\n",
      "Train on 140 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "140/140 [==============================] - 0s 316us/step - loss: 0.1978 - acc: 0.0014 - val_loss: 0.1924 - val_acc: 0.0029\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 309us/step - loss: 0.1946 - acc: 0.0044 - val_loss: 0.2022 - val_acc: 0.0011\n",
      "Train on 142 samples, validate on 142 samples\n",
      "Epoch 1/1\n",
      "142/142 [==============================] - 0s 305us/step - loss: 0.2144 - acc: 0.0042 - val_loss: 0.2149 - val_acc: 0.0000e+00\n",
      "Train on 108 samples, validate on 108 samples\n",
      "Epoch 1/1\n",
      "108/108 [==============================] - 0s 418us/step - loss: 0.1956 - acc: 0.0019 - val_loss: 0.1986 - val_acc: 0.0000e+00\n",
      "Train on 184 samples, validate on 184 samples\n",
      "Epoch 1/1\n",
      "184/184 [==============================] - 0s 329us/step - loss: 0.2029 - acc: 0.0000e+00 - val_loss: 0.2015 - val_acc: 0.0022\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 263us/step - loss: 0.2065 - acc: 0.0000e+00 - val_loss: 0.2116 - val_acc: 9.9010e-04\n",
      "Train on 152 samples, validate on 152 samples\n",
      "Epoch 1/1\n",
      "152/152 [==============================] - 0s 311us/step - loss: 0.2046 - acc: 0.0000e+00 - val_loss: 0.1958 - val_acc: 0.0000e+00\n",
      "Train on 174 samples, validate on 174 samples\n",
      "Epoch 1/1\n",
      "174/174 [==============================] - 0s 281us/step - loss: 0.1999 - acc: 0.0000e+00 - val_loss: 0.1984 - val_acc: 0.0011\n",
      "Train on 140 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "140/140 [==============================] - 0s 409us/step - loss: 0.2037 - acc: 0.0014 - val_loss: 0.1814 - val_acc: 0.0029\n",
      "Train on 228 samples, validate on 228 samples\n",
      "Epoch 1/1\n",
      "228/228 [==============================] - 0s 349us/step - loss: 0.2136 - acc: 0.0000e+00 - val_loss: 0.1990 - val_acc: 8.7719e-04\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 334us/step - loss: 0.1929 - acc: 9.9010e-04 - val_loss: 0.1919 - val_acc: 0.0000e+00\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 403us/step - loss: 0.2041 - acc: 0.0011 - val_loss: 0.1910 - val_acc: 0.0022\n",
      "Train on 242 samples, validate on 242 samples\n",
      "Epoch 1/1\n",
      "242/242 [==============================] - 0s 339us/step - loss: 0.2041 - acc: 0.0025 - val_loss: 0.2042 - val_acc: 0.0017\n",
      "Train on 144 samples, validate on 144 samples\n",
      "Epoch 1/1\n",
      "144/144 [==============================] - 0s 491us/step - loss: 0.1977 - acc: 0.0000e+00 - val_loss: 0.1948 - val_acc: 0.0028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 356 samples, validate on 356 samples\n",
      "Epoch 1/1\n",
      "356/356 [==============================] - 0s 425us/step - loss: 0.1989 - acc: 0.0000e+00 - val_loss: 0.1987 - val_acc: 0.0011\n",
      "Train on 234 samples, validate on 234 samples\n",
      "Epoch 1/1\n",
      "234/234 [==============================] - 0s 256us/step - loss: 0.1893 - acc: 8.5470e-04 - val_loss: 0.1764 - val_acc: 0.0000e+00\n",
      "Train on 184 samples, validate on 184 samples\n",
      "Epoch 1/1\n",
      "184/184 [==============================] - 0s 303us/step - loss: 0.1882 - acc: 0.0022 - val_loss: 0.1927 - val_acc: 0.0022\n",
      "Train on 238 samples, validate on 238 samples\n",
      "Epoch 1/1\n",
      "238/238 [==============================] - 0s 260us/step - loss: 0.2002 - acc: 0.0017 - val_loss: 0.1868 - val_acc: 8.4034e-04\n",
      "Train on 340 samples, validate on 340 samples\n",
      "Epoch 1/1\n",
      "340/340 [==============================] - 0s 324us/step - loss: 0.1954 - acc: 5.8824e-04 - val_loss: 0.1840 - val_acc: 5.8824e-04\n",
      "Train on 220 samples, validate on 220 samples\n",
      "Epoch 1/1\n",
      "220/220 [==============================] - 0s 301us/step - loss: 0.1917 - acc: 0.0000e+00 - val_loss: 0.1928 - val_acc: 0.0000e+00\n",
      "Train on 260 samples, validate on 260 samples\n",
      "Epoch 1/1\n",
      "260/260 [==============================] - 0s 425us/step - loss: 0.1981 - acc: 0.0015 - val_loss: 0.1820 - val_acc: 0.0000e+00\n",
      "Train on 216 samples, validate on 216 samples\n",
      "Epoch 1/1\n",
      "216/216 [==============================] - 0s 262us/step - loss: 0.1834 - acc: 9.2593e-04 - val_loss: 0.1783 - val_acc: 0.0000e+00\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 258us/step - loss: 0.2029 - acc: 0.0050 - val_loss: 0.1869 - val_acc: 0.0050\n",
      "Train on 124 samples, validate on 124 samples\n",
      "Epoch 1/1\n",
      "124/124 [==============================] - 0s 342us/step - loss: 0.2012 - acc: 0.0032 - val_loss: 0.2179 - val_acc: 0.0032\n",
      "Train on 192 samples, validate on 192 samples\n",
      "Epoch 1/1\n",
      "192/192 [==============================] - 0s 256us/step - loss: 0.2166 - acc: 0.0021 - val_loss: 0.2063 - val_acc: 0.0021\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 283us/step - loss: 0.2046 - acc: 0.0056 - val_loss: 0.2041 - val_acc: 0.0011\n",
      "Train on 282 samples, validate on 282 samples\n",
      "Epoch 1/1\n",
      "282/282 [==============================] - 0s 341us/step - loss: 0.2024 - acc: 0.0021 - val_loss: 0.1985 - val_acc: 7.0922e-04\n",
      "Train on 220 samples, validate on 220 samples\n",
      "Epoch 1/1\n",
      "220/220 [==============================] - 0s 264us/step - loss: 0.1936 - acc: 9.0909e-04 - val_loss: 0.1907 - val_acc: 0.0000e+00\n",
      "Train on 194 samples, validate on 194 samples\n",
      "Epoch 1/1\n",
      "194/194 [==============================] - 0s 345us/step - loss: 0.2062 - acc: 0.0021 - val_loss: 0.1958 - val_acc: 0.0000e+00\n",
      "Train on 302 samples, validate on 302 samples\n",
      "Epoch 1/1\n",
      "302/302 [==============================] - 0s 352us/step - loss: 0.2055 - acc: 0.0013 - val_loss: 0.2074 - val_acc: 0.0026\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 253us/step - loss: 0.1942 - acc: 0.0000e+00 - val_loss: 0.1853 - val_acc: 0.0020\n",
      "Train on 160 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 352us/step - loss: 0.2005 - acc: 0.0037 - val_loss: 0.1760 - val_acc: 0.0025\n",
      "Train on 158 samples, validate on 158 samples\n",
      "Epoch 1/1\n",
      "158/158 [==============================] - 0s 456us/step - loss: 0.2072 - acc: 0.0038 - val_loss: 0.1883 - val_acc: 0.0051\n",
      "Train on 154 samples, validate on 154 samples\n",
      "Epoch 1/1\n",
      "154/154 [==============================] - 0s 464us/step - loss: 0.2173 - acc: 0.0026 - val_loss: 0.2084 - val_acc: 0.0026\n",
      "Train on 166 samples, validate on 166 samples\n",
      "Epoch 1/1\n",
      "166/166 [==============================] - 0s 447us/step - loss: 0.2017 - acc: 0.0000e+00 - val_loss: 0.1914 - val_acc: 0.0024\n",
      "Train on 224 samples, validate on 224 samples\n",
      "Epoch 1/1\n",
      "224/224 [==============================] - 0s 342us/step - loss: 0.1881 - acc: 0.0063 - val_loss: 0.1775 - val_acc: 0.0000e+00\n",
      "Train on 294 samples, validate on 294 samples\n",
      "Epoch 1/1\n",
      "294/294 [==============================] - 0s 520us/step - loss: 0.1935 - acc: 0.0034 - val_loss: 0.1929 - val_acc: 6.8027e-04\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 596us/step - loss: 0.1841 - acc: 0.0080 - val_loss: 0.1805 - val_acc: 0.0067\n",
      "Train on 196 samples, validate on 196 samples\n",
      "Epoch 1/1\n",
      "196/196 [==============================] - 0s 234us/step - loss: 0.2032 - acc: 0.0010 - val_loss: 0.1908 - val_acc: 0.0041\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 305us/step - loss: 0.2082 - acc: 0.0014 - val_loss: 0.2009 - val_acc: 0.0027\n",
      "Train on 234 samples, validate on 234 samples\n",
      "Epoch 1/1\n",
      "234/234 [==============================] - 0s 225us/step - loss: 0.1964 - acc: 8.5470e-04 - val_loss: 0.1929 - val_acc: 0.0043\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 333us/step - loss: 0.2056 - acc: 0.0044 - val_loss: 0.1936 - val_acc: 0.0033\n",
      "Train on 130 samples, validate on 130 samples\n",
      "Epoch 1/1\n",
      "130/130 [==============================] - 0s 335us/step - loss: 0.1724 - acc: 0.0000e+00 - val_loss: 0.1902 - val_acc: 0.0031\n",
      "Train on 260 samples, validate on 260 samples\n",
      "Epoch 1/1\n",
      "260/260 [==============================] - 0s 395us/step - loss: 0.1968 - acc: 0.0000e+00 - val_loss: 0.1863 - val_acc: 0.0031\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 355us/step - loss: 0.1858 - acc: 0.0013 - val_loss: 0.1888 - val_acc: 0.0000e+00\n",
      "Train on 138 samples, validate on 138 samples\n",
      "Epoch 1/1\n",
      "138/138 [==============================] - 0s 305us/step - loss: 0.1848 - acc: 0.0000e+00 - val_loss: 0.1862 - val_acc: 0.0014\n",
      "Train on 184 samples, validate on 184 samples\n",
      "Epoch 1/1\n",
      "184/184 [==============================] - 0s 332us/step - loss: 0.1844 - acc: 0.0011 - val_loss: 0.1893 - val_acc: 0.0011\n",
      "Train on 206 samples, validate on 206 samples\n",
      "Epoch 1/1\n",
      "206/206 [==============================] - 0s 241us/step - loss: 0.1837 - acc: 0.0039 - val_loss: 0.2018 - val_acc: 0.0019\n",
      "Train on 174 samples, validate on 174 samples\n",
      "Epoch 1/1\n",
      "174/174 [==============================] - 0s 300us/step - loss: 0.2006 - acc: 0.0011 - val_loss: 0.1858 - val_acc: 0.0011\n",
      "Train on 278 samples, validate on 278 samples\n",
      "Epoch 1/1\n",
      "278/278 [==============================] - 0s 336us/step - loss: 0.2026 - acc: 0.0043 - val_loss: 0.1925 - val_acc: 0.0000e+00\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 309us/step - loss: 0.1899 - acc: 0.0064 - val_loss: 0.1854 - val_acc: 0.0013\n",
      "Train on 160 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 289us/step - loss: 0.2082 - acc: 0.0037 - val_loss: 0.2073 - val_acc: 0.0037\n",
      "Train on 282 samples, validate on 282 samples\n",
      "Epoch 1/1\n",
      "282/282 [==============================] - 0s 355us/step - loss: 0.2297 - acc: 0.0021 - val_loss: 0.2076 - val_acc: 0.0035\n",
      "Train on 138 samples, validate on 138 samples\n",
      "Epoch 1/1\n",
      "138/138 [==============================] - 0s 367us/step - loss: 0.1957 - acc: 0.0000e+00 - val_loss: 0.1938 - val_acc: 0.0014\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 320us/step - loss: 0.1956 - acc: 0.0000e+00 - val_loss: 0.1796 - val_acc: 0.0013\n",
      "Train on 154 samples, validate on 154 samples\n",
      "Epoch 1/1\n",
      "154/154 [==============================] - 0s 486us/step - loss: 0.1909 - acc: 0.0039 - val_loss: 0.1888 - val_acc: 0.0000e+00\n",
      "Train on 262 samples, validate on 262 samples\n",
      "Epoch 1/1\n",
      "262/262 [==============================] - 0s 543us/step - loss: 0.2345 - acc: 7.6336e-04 - val_loss: 0.2193 - val_acc: 0.0015\n",
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 366us/step - loss: 0.1860 - acc: 0.0013 - val_loss: 0.1797 - val_acc: 0.0026\n",
      "Train on 158 samples, validate on 158 samples\n",
      "Epoch 1/1\n",
      "158/158 [==============================] - 0s 491us/step - loss: 0.2273 - acc: 0.0038 - val_loss: 0.2330 - val_acc: 0.0000e+00\n",
      "Train on 188 samples, validate on 188 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 0s 414us/step - loss: 0.2017 - acc: 0.0021 - val_loss: 0.1984 - val_acc: 0.0074\n",
      "Train on 216 samples, validate on 216 samples\n",
      "Epoch 1/1\n",
      "216/216 [==============================] - 0s 540us/step - loss: 0.1998 - acc: 9.2593e-04 - val_loss: 0.1857 - val_acc: 0.0000e+00\n",
      "Train on 176 samples, validate on 176 samples\n",
      "Epoch 1/1\n",
      "176/176 [==============================] - 0s 299us/step - loss: 0.1924 - acc: 0.0011 - val_loss: 0.2006 - val_acc: 0.0011\n",
      "Train on 212 samples, validate on 212 samples\n",
      "Epoch 1/1\n",
      "212/212 [==============================] - 0s 240us/step - loss: 0.1900 - acc: 0.0028 - val_loss: 0.1886 - val_acc: 9.4340e-04\n",
      "Train on 330 samples, validate on 330 samples\n",
      "Epoch 1/1\n",
      "330/330 [==============================] - 0s 326us/step - loss: 0.2141 - acc: 0.0012 - val_loss: 0.2079 - val_acc: 0.0018\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 325us/step - loss: 0.2158 - acc: 0.0000e+00 - val_loss: 0.1753 - val_acc: 0.0011\n",
      "Train on 256 samples, validate on 256 samples\n",
      "Epoch 1/1\n",
      "256/256 [==============================] - 0s 242us/step - loss: 0.2008 - acc: 0.0023 - val_loss: 0.1781 - val_acc: 0.0031\n",
      "Train on 264 samples, validate on 264 samples\n",
      "Epoch 1/1\n",
      "264/264 [==============================] - 0s 359us/step - loss: 0.2098 - acc: 7.5758e-04 - val_loss: 0.1798 - val_acc: 7.5758e-04\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 247us/step - loss: 0.2004 - acc: 0.0020 - val_loss: 0.1709 - val_acc: 0.0010\n",
      "Train on 206 samples, validate on 206 samples\n",
      "Epoch 1/1\n",
      "206/206 [==============================] - 0s 248us/step - loss: 0.1835 - acc: 9.7087e-04 - val_loss: 0.1791 - val_acc: 9.7087e-04\n",
      "Train on 288 samples, validate on 288 samples\n",
      "Epoch 1/1\n",
      "288/288 [==============================] - 0s 335us/step - loss: 0.2155 - acc: 0.0014 - val_loss: 0.1906 - val_acc: 0.0000e+00\n",
      "Train on 222 samples, validate on 222 samples\n",
      "Epoch 1/1\n",
      "222/222 [==============================] - 0s 232us/step - loss: 0.1923 - acc: 0.0000e+00 - val_loss: 0.1799 - val_acc: 0.0027\n",
      "Train on 288 samples, validate on 288 samples\n",
      "Epoch 1/1\n",
      "288/288 [==============================] - 0s 383us/step - loss: 0.1867 - acc: 0.0014 - val_loss: 0.1849 - val_acc: 6.9444e-04\n",
      "Train on 78 samples, validate on 78 samples\n",
      "Epoch 1/1\n",
      "78/78 [==============================] - 0s 604us/step - loss: 0.2087 - acc: 0.0000e+00 - val_loss: 0.1950 - val_acc: 0.0000e+00\n",
      "Train on 164 samples, validate on 164 samples\n",
      "Epoch 1/1\n",
      "164/164 [==============================] - 0s 296us/step - loss: 0.1963 - acc: 0.0012 - val_loss: 0.1975 - val_acc: 0.0061\n",
      "Train on 204 samples, validate on 204 samples\n",
      "Epoch 1/1\n",
      "204/204 [==============================] - 0s 286us/step - loss: 0.1828 - acc: 0.0029 - val_loss: 0.1657 - val_acc: 9.8039e-04\n",
      "Train on 220 samples, validate on 220 samples\n",
      "Epoch 1/1\n",
      "220/220 [==============================] - 0s 237us/step - loss: 0.1875 - acc: 0.0018 - val_loss: 0.1902 - val_acc: 0.0018\n",
      "Train on 194 samples, validate on 194 samples\n",
      "Epoch 1/1\n",
      "194/194 [==============================] - 0s 260us/step - loss: 0.2013 - acc: 0.0000e+00 - val_loss: 0.1866 - val_acc: 0.0021\n",
      "Train on 196 samples, validate on 196 samples\n",
      "Epoch 1/1\n",
      "196/196 [==============================] - 0s 272us/step - loss: 0.1821 - acc: 0.0051 - val_loss: 0.1743 - val_acc: 0.0031\n",
      "Train on 114 samples, validate on 114 samples\n",
      "Epoch 1/1\n",
      "114/114 [==============================] - 0s 533us/step - loss: 0.1985 - acc: 0.0000e+00 - val_loss: 0.1865 - val_acc: 0.0000e+00\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 348us/step - loss: 0.1827 - acc: 0.0040 - val_loss: 0.1741 - val_acc: 0.0040\n",
      "Train on 134 samples, validate on 134 samples\n",
      "Epoch 1/1\n",
      "134/134 [==============================] - 0s 602us/step - loss: 0.2055 - acc: 0.0030 - val_loss: 0.1902 - val_acc: 0.0015\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 420us/step - loss: 0.1771 - acc: 0.0041 - val_loss: 0.1633 - val_acc: 0.0041\n",
      "Train on 212 samples, validate on 212 samples\n",
      "Epoch 1/1\n",
      "212/212 [==============================] - 0s 395us/step - loss: 0.2037 - acc: 0.0000e+00 - val_loss: 0.2195 - val_acc: 9.4340e-04\n",
      "Train on 210 samples, validate on 210 samples\n",
      "Epoch 1/1\n",
      "210/210 [==============================] - 0s 305us/step - loss: 0.2200 - acc: 0.0038 - val_loss: 0.2124 - val_acc: 0.0029\n",
      "Train on 158 samples, validate on 158 samples\n",
      "Epoch 1/1\n",
      "158/158 [==============================] - 0s 436us/step - loss: 0.2041 - acc: 0.0000e+00 - val_loss: 0.2085 - val_acc: 0.0025\n",
      "Train on 198 samples, validate on 198 samples\n",
      "Epoch 1/1\n",
      "198/198 [==============================] - 0s 235us/step - loss: 0.2111 - acc: 0.0020 - val_loss: 0.2088 - val_acc: 0.0030\n",
      "Train on 206 samples, validate on 206 samples\n",
      "Epoch 1/1\n",
      "206/206 [==============================] - 0s 234us/step - loss: 0.2142 - acc: 0.0019 - val_loss: 0.1979 - val_acc: 9.7087e-04\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 258us/step - loss: 0.2055 - acc: 0.0033 - val_loss: 0.1967 - val_acc: 0.0067\n",
      "Train on 214 samples, validate on 214 samples\n",
      "Epoch 1/1\n",
      "214/214 [==============================] - 0s 236us/step - loss: 0.2079 - acc: 0.0047 - val_loss: 0.2071 - val_acc: 0.0028\n",
      "Train on 186 samples, validate on 186 samples\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 282us/step - loss: 0.2095 - acc: 0.0000e+00 - val_loss: 0.1933 - val_acc: 0.0011\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 281us/step - loss: 0.2011 - acc: 0.0011 - val_loss: 0.1998 - val_acc: 0.0033\n",
      "Train on 222 samples, validate on 222 samples\n",
      "Epoch 1/1\n",
      "222/222 [==============================] - 0s 259us/step - loss: 0.2069 - acc: 0.0045 - val_loss: 0.2006 - val_acc: 0.0018\n",
      "Train on 296 samples, validate on 296 samples\n",
      "Epoch 1/1\n",
      "296/296 [==============================] - 0s 315us/step - loss: 0.1969 - acc: 0.0000e+00 - val_loss: 0.1882 - val_acc: 6.7568e-04\n",
      "Train on 230 samples, validate on 230 samples\n",
      "Epoch 1/1\n",
      "230/230 [==============================] - 0s 231us/step - loss: 0.1850 - acc: 0.0026 - val_loss: 0.1867 - val_acc: 0.0026\n",
      "Train on 190 samples, validate on 190 samples\n",
      "Epoch 1/1\n",
      "190/190 [==============================] - 0s 260us/step - loss: 0.1995 - acc: 0.0021 - val_loss: 0.1920 - val_acc: 0.0000e+00\n",
      "Train on 186 samples, validate on 186 samples\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 249us/step - loss: 0.2075 - acc: 0.0011 - val_loss: 0.1992 - val_acc: 0.0000e+00\n",
      "Train on 218 samples, validate on 218 samples\n",
      "Epoch 1/1\n",
      "218/218 [==============================] - 0s 228us/step - loss: 0.2067 - acc: 0.0000e+00 - val_loss: 0.1935 - val_acc: 0.0000e+00\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 246us/step - loss: 0.1904 - acc: 9.9010e-04 - val_loss: 0.1963 - val_acc: 0.0020\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 306us/step - loss: 0.1928 - acc: 0.0014 - val_loss: 0.1931 - val_acc: 0.0000e+00\n",
      "Train on 252 samples, validate on 252 samples\n",
      "Epoch 1/1\n",
      "252/252 [==============================] - 0s 206us/step - loss: 0.2052 - acc: 0.0016 - val_loss: 0.1850 - val_acc: 0.0016\n",
      "Train on 152 samples, validate on 152 samples\n",
      "Epoch 1/1\n",
      "152/152 [==============================] - 0s 311us/step - loss: 0.1958 - acc: 0.0000e+00 - val_loss: 0.2023 - val_acc: 0.0013\n",
      "Train on 118 samples, validate on 118 samples\n",
      "Epoch 1/1\n",
      "118/118 [==============================] - 0s 365us/step - loss: 0.2000 - acc: 0.0017 - val_loss: 0.1940 - val_acc: 0.0034\n",
      "Train on 232 samples, validate on 232 samples\n",
      "Epoch 1/1\n",
      "232/232 [==============================] - 0s 255us/step - loss: 0.2001 - acc: 0.0017 - val_loss: 0.1992 - val_acc: 8.6207e-04\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 337us/step - loss: 0.1781 - acc: 0.0013 - val_loss: 0.1782 - val_acc: 0.0027\n",
      "Train on 144 samples, validate on 144 samples\n",
      "Epoch 1/1\n",
      "144/144 [==============================] - 0s 309us/step - loss: 0.2102 - acc: 0.0000e+00 - val_loss: 0.1897 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 156 samples, validate on 156 samples\n",
      "Epoch 1/1\n",
      "156/156 [==============================] - 0s 343us/step - loss: 0.1923 - acc: 0.0026 - val_loss: 0.1776 - val_acc: 0.0051\n",
      "Train on 212 samples, validate on 212 samples\n",
      "Epoch 1/1\n",
      "212/212 [==============================] - 0s 350us/step - loss: 0.2004 - acc: 9.4340e-04 - val_loss: 0.1915 - val_acc: 0.0028\n",
      "Train on 194 samples, validate on 194 samples\n",
      "Epoch 1/1\n",
      "194/194 [==============================] - 0s 369us/step - loss: 0.1929 - acc: 0.0000e+00 - val_loss: 0.1862 - val_acc: 0.0021\n",
      "Train on 164 samples, validate on 164 samples\n",
      "Epoch 1/1\n",
      "164/164 [==============================] - 0s 424us/step - loss: 0.2115 - acc: 0.0000e+00 - val_loss: 0.1937 - val_acc: 0.0012\n",
      "Train on 346 samples, validate on 346 samples\n",
      "Epoch 1/1\n",
      "346/346 [==============================] - 0s 395us/step - loss: 0.1981 - acc: 0.0023 - val_loss: 0.1887 - val_acc: 0.0012\n",
      "Train on 172 samples, validate on 172 samples\n",
      "Epoch 1/1\n",
      "172/172 [==============================] - 0s 415us/step - loss: 0.1994 - acc: 0.0035 - val_loss: 0.2031 - val_acc: 0.0035\n",
      "Train on 286 samples, validate on 286 samples\n",
      "Epoch 1/1\n",
      "286/286 [==============================] - 0s 487us/step - loss: 0.1734 - acc: 0.0042 - val_loss: 0.1790 - val_acc: 0.0049\n",
      "Train on 186 samples, validate on 186 samples\n",
      "Epoch 1/1\n",
      "186/186 [==============================] - 0s 249us/step - loss: 0.1946 - acc: 0.0032 - val_loss: 0.1803 - val_acc: 0.0022\n",
      "Train on 216 samples, validate on 216 samples\n",
      "Epoch 1/1\n",
      "216/216 [==============================] - 0s 216us/step - loss: 0.1905 - acc: 0.0019 - val_loss: 0.1870 - val_acc: 0.0046\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 292us/step - loss: 0.1842 - acc: 0.0027 - val_loss: 0.1913 - val_acc: 0.0013\n",
      "Train on 132 samples, validate on 132 samples\n",
      "Epoch 1/1\n",
      "132/132 [==============================] - 0s 445us/step - loss: 0.1771 - acc: 0.0030 - val_loss: 0.1860 - val_acc: 0.0015\n",
      "Train on 280 samples, validate on 280 samples\n",
      "Epoch 1/1\n",
      "280/280 [==============================] - 0s 371us/step - loss: 0.1720 - acc: 0.0021 - val_loss: 0.1820 - val_acc: 0.0021\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 296us/step - loss: 0.2146 - acc: 0.0041 - val_loss: 0.1938 - val_acc: 0.0000e+00\n",
      "Train on 190 samples, validate on 190 samples\n",
      "Epoch 1/1\n",
      "190/190 [==============================] - 0s 257us/step - loss: 0.1986 - acc: 0.0053 - val_loss: 0.1873 - val_acc: 0.0042\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 358us/step - loss: 0.1882 - acc: 0.0014 - val_loss: 0.1932 - val_acc: 0.0027\n",
      "Train on 140 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "140/140 [==============================] - 0s 320us/step - loss: 0.1949 - acc: 0.0014 - val_loss: 0.1880 - val_acc: 0.0014\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 285us/step - loss: 0.1919 - acc: 0.0033 - val_loss: 0.1985 - val_acc: 0.0011\n",
      "Train on 142 samples, validate on 142 samples\n",
      "Epoch 1/1\n",
      "142/142 [==============================] - 0s 375us/step - loss: 0.2111 - acc: 0.0028 - val_loss: 0.2084 - val_acc: 0.0014\n",
      "Train on 108 samples, validate on 108 samples\n",
      "Epoch 1/1\n",
      "108/108 [==============================] - 0s 446us/step - loss: 0.1879 - acc: 0.0037 - val_loss: 0.1902 - val_acc: 0.0000e+00\n",
      "Train on 184 samples, validate on 184 samples\n",
      "Epoch 1/1\n",
      "184/184 [==============================] - 0s 268us/step - loss: 0.1963 - acc: 0.0000e+00 - val_loss: 0.1968 - val_acc: 0.0022\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 257us/step - loss: 0.2017 - acc: 0.0000e+00 - val_loss: 0.2065 - val_acc: 9.9010e-04\n",
      "Train on 152 samples, validate on 152 samples\n",
      "Epoch 1/1\n",
      "152/152 [==============================] - 0s 323us/step - loss: 0.1975 - acc: 0.0000e+00 - val_loss: 0.1908 - val_acc: 0.0000e+00\n",
      "Train on 174 samples, validate on 174 samples\n",
      "Epoch 1/1\n",
      "174/174 [==============================] - 0s 290us/step - loss: 0.1934 - acc: 0.0000e+00 - val_loss: 0.1936 - val_acc: 0.0000e+00\n",
      "Train on 140 samples, validate on 140 samples\n",
      "Epoch 1/1\n",
      "140/140 [==============================] - 0s 320us/step - loss: 0.1987 - acc: 0.0014 - val_loss: 0.1767 - val_acc: 0.0014\n",
      "Train on 228 samples, validate on 228 samples\n",
      "Epoch 1/1\n",
      "228/228 [==============================] - 0s 232us/step - loss: 0.2104 - acc: 8.7719e-04 - val_loss: 0.1943 - val_acc: 8.7719e-04\n",
      "Train on 202 samples, validate on 202 samples\n",
      "Epoch 1/1\n",
      "202/202 [==============================] - 0s 260us/step - loss: 0.1899 - acc: 9.9010e-04 - val_loss: 0.1866 - val_acc: 0.0000e+00\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 281us/step - loss: 0.1956 - acc: 0.0011 - val_loss: 0.1856 - val_acc: 0.0011\n",
      "Train on 242 samples, validate on 242 samples\n",
      "Epoch 1/1\n",
      "242/242 [==============================] - 0s 304us/step - loss: 0.2019 - acc: 0.0025 - val_loss: 0.2005 - val_acc: 0.0017\n",
      "Train on 144 samples, validate on 144 samples\n",
      "Epoch 1/1\n",
      "144/144 [==============================] - 0s 421us/step - loss: 0.1935 - acc: 0.0000e+00 - val_loss: 0.1908 - val_acc: 0.0014\n",
      "Train on 356 samples, validate on 356 samples\n",
      "Epoch 1/1\n",
      "356/356 [==============================] - 0s 407us/step - loss: 0.1944 - acc: 5.6180e-04 - val_loss: 0.1944 - val_acc: 5.6180e-04\n",
      "Train on 234 samples, validate on 234 samples\n",
      "Epoch 1/1\n",
      "234/234 [==============================] - 0s 317us/step - loss: 0.1855 - acc: 8.5470e-04 - val_loss: 0.1722 - val_acc: 0.0000e+00\n",
      "Train on 184 samples, validate on 184 samples\n",
      "Epoch 1/1\n",
      "184/184 [==============================] - 0s 432us/step - loss: 0.1841 - acc: 0.0011 - val_loss: 0.1899 - val_acc: 0.0011\n",
      "Train on 238 samples, validate on 238 samples\n",
      "Epoch 1/1\n",
      "238/238 [==============================] - 0s 396us/step - loss: 0.1977 - acc: 8.4034e-04 - val_loss: 0.1836 - val_acc: 0.0000e+00\n",
      "Train on 340 samples, validate on 340 samples\n",
      "Epoch 1/1\n",
      "340/340 [==============================] - 0s 279us/step - loss: 0.1901 - acc: 0.0012 - val_loss: 0.1800 - val_acc: 5.8824e-04\n",
      "Train on 220 samples, validate on 220 samples\n",
      "Epoch 1/1\n",
      "220/220 [==============================] - 0s 218us/step - loss: 0.1884 - acc: 0.0000e+00 - val_loss: 0.1893 - val_acc: 9.0909e-04\n",
      "Train on 260 samples, validate on 260 samples\n",
      "Epoch 1/1\n",
      "260/260 [==============================] - 0s 341us/step - loss: 0.1953 - acc: 7.6923e-04 - val_loss: 0.1784 - val_acc: 0.0000e+00\n",
      "Train on 216 samples, validate on 216 samples\n",
      "Epoch 1/1\n",
      "216/216 [==============================] - 0s 236us/step - loss: 0.1821 - acc: 0.0019 - val_loss: 0.1767 - val_acc: 0.0019\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "200/200 [==============================] - 0s 258us/step - loss: 0.1909 - acc: 0.0050 - val_loss: 0.1753 - val_acc: 0.0050\n",
      "Train on 124 samples, validate on 124 samples\n",
      "Epoch 1/1\n",
      "124/124 [==============================] - 0s 357us/step - loss: 0.1933 - acc: 0.0000e+00 - val_loss: 0.2069 - val_acc: 0.0032\n",
      "Train on 192 samples, validate on 192 samples\n",
      "Epoch 1/1\n",
      "192/192 [==============================] - 0s 272us/step - loss: 0.2084 - acc: 0.0010 - val_loss: 0.1964 - val_acc: 0.0010\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 268us/step - loss: 0.1981 - acc: 0.0044 - val_loss: 0.1922 - val_acc: 0.0011\n",
      "Train on 282 samples, validate on 282 samples\n",
      "Epoch 1/1\n",
      "282/282 [==============================] - 0s 337us/step - loss: 0.1924 - acc: 0.0043 - val_loss: 0.1905 - val_acc: 7.0922e-04\n",
      "Train on 220 samples, validate on 220 samples\n",
      "Epoch 1/1\n",
      "220/220 [==============================] - 0s 233us/step - loss: 0.1873 - acc: 9.0909e-04 - val_loss: 0.1864 - val_acc: 9.0909e-04\n",
      "Train on 194 samples, validate on 194 samples\n",
      "Epoch 1/1\n",
      "194/194 [==============================] - 0s 270us/step - loss: 0.2010 - acc: 0.0021 - val_loss: 0.1895 - val_acc: 0.0000e+00\n",
      "Train on 302 samples, validate on 302 samples\n",
      "Epoch 1/1\n",
      "302/302 [==============================] - 0s 316us/step - loss: 0.2034 - acc: 0.0013 - val_loss: 0.2035 - val_acc: 0.0020\n",
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 250us/step - loss: 0.1907 - acc: 0.0000e+00 - val_loss: 0.1809 - val_acc: 0.0020\n",
      "Train on 160 samples, validate on 160 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 317us/step - loss: 0.1979 - acc: 0.0050 - val_loss: 0.1719 - val_acc: 0.0025\n",
      "Train on 158 samples, validate on 158 samples\n",
      "Epoch 1/1\n",
      "158/158 [==============================] - 0s 297us/step - loss: 0.2060 - acc: 0.0038 - val_loss: 0.1848 - val_acc: 0.0038\n",
      "Train on 154 samples, validate on 154 samples\n",
      "Epoch 1/1\n",
      "154/154 [==============================] - 0s 336us/step - loss: 0.2140 - acc: 0.0026 - val_loss: 0.2036 - val_acc: 0.0026\n",
      "Train on 166 samples, validate on 166 samples\n",
      "Epoch 1/1\n",
      "166/166 [==============================] - 0s 391us/step - loss: 0.1906 - acc: 0.0012 - val_loss: 0.1841 - val_acc: 0.0024\n",
      "Train on 224 samples, validate on 224 samples\n",
      "Epoch 1/1\n",
      "224/224 [==============================] - 0s 341us/step - loss: 0.1830 - acc: 0.0054 - val_loss: 0.1721 - val_acc: 0.0000e+00\n",
      "Train on 294 samples, validate on 294 samples\n",
      "Epoch 1/1\n",
      "294/294 [==============================] - 0s 449us/step - loss: 0.1907 - acc: 0.0027 - val_loss: 0.1888 - val_acc: 6.8027e-04\n",
      "Train on 150 samples, validate on 150 samples\n",
      "Epoch 1/1\n",
      "150/150 [==============================] - 0s 469us/step - loss: 0.1804 - acc: 0.0093 - val_loss: 0.1766 - val_acc: 0.0067\n",
      "Train on 196 samples, validate on 196 samples\n",
      "Epoch 1/1\n",
      "196/196 [==============================] - 0s 340us/step - loss: 0.2014 - acc: 0.0020 - val_loss: 0.1869 - val_acc: 0.0041\n",
      "Train on 148 samples, validate on 148 samples\n",
      "Epoch 1/1\n",
      "148/148 [==============================] - 0s 413us/step - loss: 0.2025 - acc: 0.0027 - val_loss: 0.1977 - val_acc: 0.0027\n",
      "Train on 234 samples, validate on 234 samples\n",
      "Epoch 1/1\n",
      "234/234 [==============================] - 0s 331us/step - loss: 0.1959 - acc: 0.0034 - val_loss: 0.1911 - val_acc: 0.0043\n",
      "Train on 180 samples, validate on 180 samples\n",
      "Epoch 1/1\n",
      "180/180 [==============================] - 0s 271us/step - loss: 0.2018 - acc: 0.0022 - val_loss: 0.1907 - val_acc: 0.0033\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Bidirectional(GRU(Max_RNN, return_sequences=True), input_shape=(Max_RNN,513)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(GRU(Max_RNN, return_sequences=True)))\n",
    "# model.add(GRU(output_dim = 513, input_length = 5, input_dim = 513, return_sequences=True))\n",
    "\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(TimeDistributed(Dense(513, activation='sigmoid')))\n",
    "model.add(Dense(513, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "for e in range(5):\n",
    "    for (b_x,b_y), (v_x,v_y) in zip(next_batch(DATA_train_x, DATA_train_M), next_batch(DATA_val_x, DATA_val_M)):\n",
    "        model.fit(b_x, b_y, validation_data=(v_x,v_y), shuffle=True, batch_size=256)\n",
    "\n",
    "#     model.fit( , epochs=20, steps_per_epoch=700, validation_data=next_batch_mb(DATA_val_x, DATA_val_x,10), validation_steps=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.01%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = []\n",
    "for v_x,v_y in next_batch(DATA_val_x, DATA_val_x):\n",
    "    scores.append( model.evaluate(v_x, v_y,verbose=0)[1] )\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch_SXX_cmplxS_cmplx(S_,X_,X_cmplx_,S_cmplx_):\n",
    "    \n",
    "    batch_s = None\n",
    "    batch_x = None\n",
    "    batch_x_cmplx = None\n",
    "    batch_s_cmplx = None\n",
    "    \n",
    "    for e,(s,x,x_cmplx,s_cmplx) in enumerate( zip(S_,X_,X_cmplx_,S_cmplx_)): \n",
    "        batch_s = np.array(s.T) if batch_s is None else np.concatenate( (batch_s,s.T), axis=0)\n",
    "        batch_x = np.array(x.T) if batch_x is None else np.concatenate( (batch_x,x.T), axis=0)\n",
    "        batch_x_cmplx = np.array(x_cmplx.T) if batch_x_cmplx is None else np.concatenate( (batch_x_cmplx,x_cmplx.T), axis=0)\n",
    "        batch_s_cmplx = np.array(s_cmplx.T) if batch_s_cmplx is None else np.concatenate( (batch_s_cmplx,s_cmplx.T), axis=0)\n",
    "        \n",
    "        \n",
    "        if e>0 and (e+1)%10==0:\n",
    "            temp_s, batch_s = batch_s, None\n",
    "            temp_x, batch_x = batch_x, None\n",
    "            temp_x_cmplx, batch_x_cmplx = batch_x_cmplx, None\n",
    "            temp_s_cmplx, batch_s_cmplx = batch_s_cmplx, None\n",
    "            \n",
    "            temp_s = temp_s.reshape((-1,Max_RNN,513))\n",
    "            temp_x = temp_x.reshape((-1,Max_RNN,513))\n",
    "            temp_x_cmplx = temp_x_cmplx.reshape((-1,Max_RNN,513))\n",
    "\n",
    "            yield temp_s,temp_x,temp_x_cmplx,temp_s_cmplx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130, 5, 513)\n",
      "(130, 5, 513)\n",
      "(130, 5, 513)\n",
      "(650, 513)\n",
      "(513, 650)\n",
      "(513, 650)\n",
      "(260, 5, 513)\n",
      "(260, 5, 513)\n",
      "(260, 5, 513)\n",
      "(1300, 513)\n",
      "(513, 1300)\n",
      "(513, 1300)\n",
      "(150, 5, 513)\n",
      "(150, 5, 513)\n",
      "(150, 5, 513)\n",
      "(750, 513)\n",
      "(513, 750)\n",
      "(513, 750)\n",
      "(138, 5, 513)\n",
      "(138, 5, 513)\n",
      "(138, 5, 513)\n",
      "(690, 513)\n",
      "(513, 690)\n",
      "(513, 690)\n",
      "(184, 5, 513)\n",
      "(184, 5, 513)\n",
      "(184, 5, 513)\n",
      "(920, 513)\n",
      "(513, 920)\n",
      "(513, 920)\n",
      "(206, 5, 513)\n",
      "(206, 5, 513)\n",
      "(206, 5, 513)\n",
      "(1030, 513)\n",
      "(513, 1030)\n",
      "(513, 1030)\n",
      "(174, 5, 513)\n",
      "(174, 5, 513)\n",
      "(174, 5, 513)\n",
      "(870, 513)\n",
      "(513, 870)\n",
      "(513, 870)\n",
      "(278, 5, 513)\n",
      "(278, 5, 513)\n",
      "(278, 5, 513)\n",
      "(1390, 513)\n",
      "(513, 1390)\n",
      "(513, 1390)\n",
      "(156, 5, 513)\n",
      "(156, 5, 513)\n",
      "(156, 5, 513)\n",
      "(780, 513)\n",
      "(513, 780)\n",
      "(513, 780)\n",
      "(160, 5, 513)\n",
      "(160, 5, 513)\n",
      "(160, 5, 513)\n",
      "(800, 513)\n",
      "(513, 800)\n",
      "(513, 800)\n",
      "(282, 5, 513)\n",
      "(282, 5, 513)\n",
      "(282, 5, 513)\n",
      "(1410, 513)\n",
      "(513, 1410)\n",
      "(513, 1410)\n",
      "(138, 5, 513)\n",
      "(138, 5, 513)\n",
      "(138, 5, 513)\n",
      "(690, 513)\n",
      "(513, 690)\n",
      "(513, 690)\n",
      "(156, 5, 513)\n",
      "(156, 5, 513)\n",
      "(156, 5, 513)\n",
      "(780, 513)\n",
      "(513, 780)\n",
      "(513, 780)\n",
      "(154, 5, 513)\n",
      "(154, 5, 513)\n",
      "(154, 5, 513)\n",
      "(770, 513)\n",
      "(513, 770)\n",
      "(513, 770)\n",
      "(262, 5, 513)\n",
      "(262, 5, 513)\n",
      "(262, 5, 513)\n",
      "(1310, 513)\n",
      "(513, 1310)\n",
      "(513, 1310)\n",
      "(156, 5, 513)\n",
      "(156, 5, 513)\n",
      "(156, 5, 513)\n",
      "(780, 513)\n",
      "(513, 780)\n",
      "(513, 780)\n",
      "(158, 5, 513)\n",
      "(158, 5, 513)\n",
      "(158, 5, 513)\n",
      "(790, 513)\n",
      "(513, 790)\n",
      "(513, 790)\n",
      "(188, 5, 513)\n",
      "(188, 5, 513)\n",
      "(188, 5, 513)\n",
      "(940, 513)\n",
      "(513, 940)\n",
      "(513, 940)\n",
      "(216, 5, 513)\n",
      "(216, 5, 513)\n",
      "(216, 5, 513)\n",
      "(1080, 513)\n",
      "(513, 1080)\n",
      "(513, 1080)\n",
      "(176, 5, 513)\n",
      "(176, 5, 513)\n",
      "(176, 5, 513)\n",
      "(880, 513)\n",
      "(513, 880)\n",
      "(513, 880)\n",
      "(212, 5, 513)\n",
      "(212, 5, 513)\n",
      "(212, 5, 513)\n",
      "(1060, 513)\n",
      "(513, 1060)\n",
      "(513, 1060)\n",
      "(330, 5, 513)\n",
      "(330, 5, 513)\n",
      "(330, 5, 513)\n",
      "(1650, 513)\n",
      "(513, 1650)\n",
      "(513, 1650)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "(900, 513)\n",
      "(513, 900)\n",
      "(513, 900)\n",
      "(256, 5, 513)\n",
      "(256, 5, 513)\n",
      "(256, 5, 513)\n",
      "(1280, 513)\n",
      "(513, 1280)\n",
      "(513, 1280)\n",
      "(264, 5, 513)\n",
      "(264, 5, 513)\n",
      "(264, 5, 513)\n",
      "(1320, 513)\n",
      "(513, 1320)\n",
      "(513, 1320)\n",
      "(200, 5, 513)\n",
      "(200, 5, 513)\n",
      "(200, 5, 513)\n",
      "(1000, 513)\n",
      "(513, 1000)\n",
      "(513, 1000)\n",
      "(206, 5, 513)\n",
      "(206, 5, 513)\n",
      "(206, 5, 513)\n",
      "(1030, 513)\n",
      "(513, 1030)\n",
      "(513, 1030)\n",
      "(288, 5, 513)\n",
      "(288, 5, 513)\n",
      "(288, 5, 513)\n",
      "(1440, 513)\n",
      "(513, 1440)\n",
      "(513, 1440)\n",
      "(222, 5, 513)\n",
      "(222, 5, 513)\n",
      "(222, 5, 513)\n",
      "(1110, 513)\n",
      "(513, 1110)\n",
      "(513, 1110)\n",
      "(288, 5, 513)\n",
      "(288, 5, 513)\n",
      "(288, 5, 513)\n",
      "(1440, 513)\n",
      "(513, 1440)\n",
      "(513, 1440)\n",
      "(78, 5, 513)\n",
      "(78, 5, 513)\n",
      "(78, 5, 513)\n",
      "(390, 513)\n",
      "(513, 390)\n",
      "(513, 390)\n",
      "(164, 5, 513)\n",
      "(164, 5, 513)\n",
      "(164, 5, 513)\n",
      "(820, 513)\n",
      "(513, 820)\n",
      "(513, 820)\n",
      "(204, 5, 513)\n",
      "(204, 5, 513)\n",
      "(204, 5, 513)\n",
      "(1020, 513)\n",
      "(513, 1020)\n",
      "(513, 1020)\n",
      "(220, 5, 513)\n",
      "(220, 5, 513)\n",
      "(220, 5, 513)\n",
      "(1100, 513)\n",
      "(513, 1100)\n",
      "(513, 1100)\n",
      "(194, 5, 513)\n",
      "(194, 5, 513)\n",
      "(194, 5, 513)\n",
      "(970, 513)\n",
      "(513, 970)\n",
      "(513, 970)\n",
      "(196, 5, 513)\n",
      "(196, 5, 513)\n",
      "(196, 5, 513)\n",
      "(980, 513)\n",
      "(513, 980)\n",
      "(513, 980)\n",
      "(114, 5, 513)\n",
      "(114, 5, 513)\n",
      "(114, 5, 513)\n",
      "(570, 513)\n",
      "(513, 570)\n",
      "(513, 570)\n",
      "(202, 5, 513)\n",
      "(202, 5, 513)\n",
      "(202, 5, 513)\n",
      "(1010, 513)\n",
      "(513, 1010)\n",
      "(513, 1010)\n",
      "(134, 5, 513)\n",
      "(134, 5, 513)\n",
      "(134, 5, 513)\n",
      "(670, 513)\n",
      "(513, 670)\n",
      "(513, 670)\n",
      "(148, 5, 513)\n",
      "(148, 5, 513)\n",
      "(148, 5, 513)\n",
      "(740, 513)\n",
      "(513, 740)\n",
      "(513, 740)\n",
      "(212, 5, 513)\n",
      "(212, 5, 513)\n",
      "(212, 5, 513)\n",
      "(1060, 513)\n",
      "(513, 1060)\n",
      "(513, 1060)\n",
      "(210, 5, 513)\n",
      "(210, 5, 513)\n",
      "(210, 5, 513)\n",
      "(1050, 513)\n",
      "(513, 1050)\n",
      "(513, 1050)\n",
      "(158, 5, 513)\n",
      "(158, 5, 513)\n",
      "(158, 5, 513)\n",
      "(790, 513)\n",
      "(513, 790)\n",
      "(513, 790)\n",
      "(198, 5, 513)\n",
      "(198, 5, 513)\n",
      "(198, 5, 513)\n",
      "(990, 513)\n",
      "(513, 990)\n",
      "(513, 990)\n",
      "(206, 5, 513)\n",
      "(206, 5, 513)\n",
      "(206, 5, 513)\n",
      "(1030, 513)\n",
      "(513, 1030)\n",
      "(513, 1030)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "(900, 513)\n",
      "(513, 900)\n",
      "(513, 900)\n",
      "(214, 5, 513)\n",
      "(214, 5, 513)\n",
      "(214, 5, 513)\n",
      "(1070, 513)\n",
      "(513, 1070)\n",
      "(513, 1070)\n",
      "(186, 5, 513)\n",
      "(186, 5, 513)\n",
      "(186, 5, 513)\n",
      "(930, 513)\n",
      "(513, 930)\n",
      "(513, 930)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "(900, 513)\n",
      "(513, 900)\n",
      "(513, 900)\n",
      "(222, 5, 513)\n",
      "(222, 5, 513)\n",
      "(222, 5, 513)\n",
      "(1110, 513)\n",
      "(513, 1110)\n",
      "(513, 1110)\n",
      "(296, 5, 513)\n",
      "(296, 5, 513)\n",
      "(296, 5, 513)\n",
      "(1480, 513)\n",
      "(513, 1480)\n",
      "(513, 1480)\n",
      "(230, 5, 513)\n",
      "(230, 5, 513)\n",
      "(230, 5, 513)\n",
      "(1150, 513)\n",
      "(513, 1150)\n",
      "(513, 1150)\n",
      "(190, 5, 513)\n",
      "(190, 5, 513)\n",
      "(190, 5, 513)\n",
      "(950, 513)\n",
      "(513, 950)\n",
      "(513, 950)\n",
      "(186, 5, 513)\n",
      "(186, 5, 513)\n",
      "(186, 5, 513)\n",
      "(930, 513)\n",
      "(513, 930)\n",
      "(513, 930)\n",
      "(218, 5, 513)\n",
      "(218, 5, 513)\n",
      "(218, 5, 513)\n",
      "(1090, 513)\n",
      "(513, 1090)\n",
      "(513, 1090)\n",
      "(202, 5, 513)\n",
      "(202, 5, 513)\n",
      "(202, 5, 513)\n",
      "(1010, 513)\n",
      "(513, 1010)\n",
      "(513, 1010)\n",
      "(148, 5, 513)\n",
      "(148, 5, 513)\n",
      "(148, 5, 513)\n",
      "(740, 513)\n",
      "(513, 740)\n",
      "(513, 740)\n",
      "(252, 5, 513)\n",
      "(252, 5, 513)\n",
      "(252, 5, 513)\n",
      "(1260, 513)\n",
      "(513, 1260)\n",
      "(513, 1260)\n",
      "(152, 5, 513)\n",
      "(152, 5, 513)\n",
      "(152, 5, 513)\n",
      "(760, 513)\n",
      "(513, 760)\n",
      "(513, 760)\n",
      "(118, 5, 513)\n",
      "(118, 5, 513)\n",
      "(118, 5, 513)\n",
      "(590, 513)\n",
      "(513, 590)\n",
      "(513, 590)\n",
      "(232, 5, 513)\n",
      "(232, 5, 513)\n",
      "(232, 5, 513)\n",
      "(1160, 513)\n",
      "(513, 1160)\n",
      "(513, 1160)\n",
      "(150, 5, 513)\n",
      "(150, 5, 513)\n",
      "(150, 5, 513)\n",
      "(750, 513)\n",
      "(513, 750)\n",
      "(513, 750)\n",
      "(144, 5, 513)\n",
      "(144, 5, 513)\n",
      "(144, 5, 513)\n",
      "(720, 513)\n",
      "(513, 720)\n",
      "(513, 720)\n",
      "(156, 5, 513)\n",
      "(156, 5, 513)\n",
      "(156, 5, 513)\n",
      "(780, 513)\n",
      "(513, 780)\n",
      "(513, 780)\n",
      "(212, 5, 513)\n",
      "(212, 5, 513)\n",
      "(212, 5, 513)\n",
      "(1060, 513)\n",
      "(513, 1060)\n",
      "(513, 1060)\n",
      "(194, 5, 513)\n",
      "(194, 5, 513)\n",
      "(194, 5, 513)\n",
      "(970, 513)\n",
      "(513, 970)\n",
      "(513, 970)\n",
      "(164, 5, 513)\n",
      "(164, 5, 513)\n",
      "(164, 5, 513)\n",
      "(820, 513)\n",
      "(513, 820)\n",
      "(513, 820)\n",
      "(346, 5, 513)\n",
      "(346, 5, 513)\n",
      "(346, 5, 513)\n",
      "(1730, 513)\n",
      "(513, 1730)\n",
      "(513, 1730)\n",
      "(172, 5, 513)\n",
      "(172, 5, 513)\n",
      "(172, 5, 513)\n",
      "(860, 513)\n",
      "(513, 860)\n",
      "(513, 860)\n",
      "(286, 5, 513)\n",
      "(286, 5, 513)\n",
      "(286, 5, 513)\n",
      "(1430, 513)\n",
      "(513, 1430)\n",
      "(513, 1430)\n",
      "(186, 5, 513)\n",
      "(186, 5, 513)\n",
      "(186, 5, 513)\n",
      "(930, 513)\n",
      "(513, 930)\n",
      "(513, 930)\n",
      "(216, 5, 513)\n",
      "(216, 5, 513)\n",
      "(216, 5, 513)\n",
      "(1080, 513)\n",
      "(513, 1080)\n",
      "(513, 1080)\n",
      "(150, 5, 513)\n",
      "(150, 5, 513)\n",
      "(150, 5, 513)\n",
      "(750, 513)\n",
      "(513, 750)\n",
      "(513, 750)\n",
      "(132, 5, 513)\n",
      "(132, 5, 513)\n",
      "(132, 5, 513)\n",
      "(660, 513)\n",
      "(513, 660)\n",
      "(513, 660)\n",
      "(280, 5, 513)\n",
      "(280, 5, 513)\n",
      "(280, 5, 513)\n",
      "(1400, 513)\n",
      "(513, 1400)\n",
      "(513, 1400)\n",
      "(148, 5, 513)\n",
      "(148, 5, 513)\n",
      "(148, 5, 513)\n",
      "(740, 513)\n",
      "(513, 740)\n",
      "(513, 740)\n",
      "(190, 5, 513)\n",
      "(190, 5, 513)\n",
      "(190, 5, 513)\n",
      "(950, 513)\n",
      "(513, 950)\n",
      "(513, 950)\n",
      "(148, 5, 513)\n",
      "(148, 5, 513)\n",
      "(148, 5, 513)\n",
      "(740, 513)\n",
      "(513, 740)\n",
      "(513, 740)\n",
      "(140, 5, 513)\n",
      "(140, 5, 513)\n",
      "(140, 5, 513)\n",
      "(700, 513)\n",
      "(513, 700)\n",
      "(513, 700)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "(900, 513)\n",
      "(513, 900)\n",
      "(513, 900)\n",
      "(142, 5, 513)\n",
      "(142, 5, 513)\n",
      "(142, 5, 513)\n",
      "(710, 513)\n",
      "(513, 710)\n",
      "(513, 710)\n",
      "(108, 5, 513)\n",
      "(108, 5, 513)\n",
      "(108, 5, 513)\n",
      "(540, 513)\n",
      "(513, 540)\n",
      "(513, 540)\n",
      "(184, 5, 513)\n",
      "(184, 5, 513)\n",
      "(184, 5, 513)\n",
      "(920, 513)\n",
      "(513, 920)\n",
      "(513, 920)\n",
      "(202, 5, 513)\n",
      "(202, 5, 513)\n",
      "(202, 5, 513)\n",
      "(1010, 513)\n",
      "(513, 1010)\n",
      "(513, 1010)\n",
      "(152, 5, 513)\n",
      "(152, 5, 513)\n",
      "(152, 5, 513)\n",
      "(760, 513)\n",
      "(513, 760)\n",
      "(513, 760)\n",
      "(174, 5, 513)\n",
      "(174, 5, 513)\n",
      "(174, 5, 513)\n",
      "(870, 513)\n",
      "(513, 870)\n",
      "(513, 870)\n",
      "(140, 5, 513)\n",
      "(140, 5, 513)\n",
      "(140, 5, 513)\n",
      "(700, 513)\n",
      "(513, 700)\n",
      "(513, 700)\n",
      "(228, 5, 513)\n",
      "(228, 5, 513)\n",
      "(228, 5, 513)\n",
      "(1140, 513)\n",
      "(513, 1140)\n",
      "(513, 1140)\n",
      "(202, 5, 513)\n",
      "(202, 5, 513)\n",
      "(202, 5, 513)\n",
      "(1010, 513)\n",
      "(513, 1010)\n",
      "(513, 1010)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "(900, 513)\n",
      "(513, 900)\n",
      "(513, 900)\n",
      "(242, 5, 513)\n",
      "(242, 5, 513)\n",
      "(242, 5, 513)\n",
      "(1210, 513)\n",
      "(513, 1210)\n",
      "(513, 1210)\n",
      "(144, 5, 513)\n",
      "(144, 5, 513)\n",
      "(144, 5, 513)\n",
      "(720, 513)\n",
      "(513, 720)\n",
      "(513, 720)\n",
      "(356, 5, 513)\n",
      "(356, 5, 513)\n",
      "(356, 5, 513)\n",
      "(1780, 513)\n",
      "(513, 1780)\n",
      "(513, 1780)\n",
      "(234, 5, 513)\n",
      "(234, 5, 513)\n",
      "(234, 5, 513)\n",
      "(1170, 513)\n",
      "(513, 1170)\n",
      "(513, 1170)\n",
      "(184, 5, 513)\n",
      "(184, 5, 513)\n",
      "(184, 5, 513)\n",
      "(920, 513)\n",
      "(513, 920)\n",
      "(513, 920)\n",
      "(238, 5, 513)\n",
      "(238, 5, 513)\n",
      "(238, 5, 513)\n",
      "(1190, 513)\n",
      "(513, 1190)\n",
      "(513, 1190)\n",
      "(340, 5, 513)\n",
      "(340, 5, 513)\n",
      "(340, 5, 513)\n",
      "(1700, 513)\n",
      "(513, 1700)\n",
      "(513, 1700)\n",
      "(220, 5, 513)\n",
      "(220, 5, 513)\n",
      "(220, 5, 513)\n",
      "(1100, 513)\n",
      "(513, 1100)\n",
      "(513, 1100)\n",
      "(260, 5, 513)\n",
      "(260, 5, 513)\n",
      "(260, 5, 513)\n",
      "(1300, 513)\n",
      "(513, 1300)\n",
      "(513, 1300)\n",
      "(216, 5, 513)\n",
      "(216, 5, 513)\n",
      "(216, 5, 513)\n",
      "(1080, 513)\n",
      "(513, 1080)\n",
      "(513, 1080)\n",
      "(200, 5, 513)\n",
      "(200, 5, 513)\n",
      "(200, 5, 513)\n",
      "(1000, 513)\n",
      "(513, 1000)\n",
      "(513, 1000)\n",
      "(124, 5, 513)\n",
      "(124, 5, 513)\n",
      "(124, 5, 513)\n",
      "(620, 513)\n",
      "(513, 620)\n",
      "(513, 620)\n",
      "(192, 5, 513)\n",
      "(192, 5, 513)\n",
      "(192, 5, 513)\n",
      "(960, 513)\n",
      "(513, 960)\n",
      "(513, 960)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "(900, 513)\n",
      "(513, 900)\n",
      "(513, 900)\n",
      "(282, 5, 513)\n",
      "(282, 5, 513)\n",
      "(282, 5, 513)\n",
      "(1410, 513)\n",
      "(513, 1410)\n",
      "(513, 1410)\n",
      "(220, 5, 513)\n",
      "(220, 5, 513)\n",
      "(220, 5, 513)\n",
      "(1100, 513)\n",
      "(513, 1100)\n",
      "(513, 1100)\n",
      "(194, 5, 513)\n",
      "(194, 5, 513)\n",
      "(194, 5, 513)\n",
      "(970, 513)\n",
      "(513, 970)\n",
      "(513, 970)\n",
      "(302, 5, 513)\n",
      "(302, 5, 513)\n",
      "(302, 5, 513)\n",
      "(1510, 513)\n",
      "(513, 1510)\n",
      "(513, 1510)\n",
      "(200, 5, 513)\n",
      "(200, 5, 513)\n",
      "(200, 5, 513)\n",
      "(1000, 513)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513, 1000)\n",
      "(513, 1000)\n",
      "(160, 5, 513)\n",
      "(160, 5, 513)\n",
      "(160, 5, 513)\n",
      "(800, 513)\n",
      "(513, 800)\n",
      "(513, 800)\n",
      "(158, 5, 513)\n",
      "(158, 5, 513)\n",
      "(158, 5, 513)\n",
      "(790, 513)\n",
      "(513, 790)\n",
      "(513, 790)\n",
      "(154, 5, 513)\n",
      "(154, 5, 513)\n",
      "(154, 5, 513)\n",
      "(770, 513)\n",
      "(513, 770)\n",
      "(513, 770)\n",
      "(166, 5, 513)\n",
      "(166, 5, 513)\n",
      "(166, 5, 513)\n",
      "(830, 513)\n",
      "(513, 830)\n",
      "(513, 830)\n",
      "(224, 5, 513)\n",
      "(224, 5, 513)\n",
      "(224, 5, 513)\n",
      "(1120, 513)\n",
      "(513, 1120)\n",
      "(513, 1120)\n",
      "(294, 5, 513)\n",
      "(294, 5, 513)\n",
      "(294, 5, 513)\n",
      "(1470, 513)\n",
      "(513, 1470)\n",
      "(513, 1470)\n",
      "(150, 5, 513)\n",
      "(150, 5, 513)\n",
      "(150, 5, 513)\n",
      "(750, 513)\n",
      "(513, 750)\n",
      "(513, 750)\n",
      "(196, 5, 513)\n",
      "(196, 5, 513)\n",
      "(196, 5, 513)\n",
      "(980, 513)\n",
      "(513, 980)\n",
      "(513, 980)\n",
      "(148, 5, 513)\n",
      "(148, 5, 513)\n",
      "(148, 5, 513)\n",
      "(740, 513)\n",
      "(513, 740)\n",
      "(513, 740)\n",
      "(234, 5, 513)\n",
      "(234, 5, 513)\n",
      "(234, 5, 513)\n",
      "(1170, 513)\n",
      "(513, 1170)\n",
      "(513, 1170)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "(180, 5, 513)\n",
      "(900, 513)\n",
      "(513, 900)\n",
      "(513, 900)\n",
      "3.140206685271807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.96958233843578"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_s = 0.0\n",
    "sum_s_diff = 0.0\n",
    "\n",
    "\n",
    "for v_s,v_x,v_x_cmplx,v_s_cmplx in next_batch_SXX_cmplxS_cmplx(DATA_val_s,DATA_val_x,DATA_val_x_cmplx,DATA_val_s_cmplx):\n",
    "    \n",
    "#     print(v_s.shape)\n",
    "#     print(v_x.shape)\n",
    "#     print(v_x_cmplx.shape)\n",
    "#     print(v_s_cmplx.shape)\n",
    "    \n",
    "    mask = model.predict(v_x)\n",
    "    S_hat = (mask) * v_x_cmplx\n",
    "    S_hat = S_hat.reshape(-1,513).T\n",
    "    S = v_s_cmplx.T\n",
    "    \n",
    "#     print(S.shape)\n",
    "#     print(S_hat.shape)\n",
    "\n",
    "    S_org = librosa.istft(S, hop_length=512)\n",
    "    S_pred = librosa.istft(S_hat, hop_length=512)\n",
    "\n",
    "    sum_s += np.sum(S_org*S_org)\n",
    "    sum_s_diff += np.sum((S_org-S_pred)*(S_org-S_pred))\n",
    "    \n",
    "acc = sum_s/ sum_s_diff\n",
    "print(acc)\n",
    "\n",
    "10*np.log10(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_audio():\n",
    "    mags = None\n",
    "    \n",
    "    for e, file_x in enumerate(sorted(glob.glob(PATH_directory+PATH_test+MIX_format_val))):\n",
    "        print(e)\n",
    "        sn, sr = librosa.load(file_x, sr=None)\n",
    "        Sn = librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "        mag_Sn=np.abs(Sn)\n",
    "        \n",
    "        mags = np.array(mag_Sn.T) if mags is None else np.concatenate( (mags,mag_Sn.T), axis=0)\n",
    "        \n",
    "        if e>0 and (e+1)%10==0:\n",
    "            temp, mags = mags, None\n",
    "            \n",
    "            temp = temp.reshape((-1,Max_RNN,513))\n",
    "            mask = model.predict(temp)\n",
    "            \n",
    "            S_hat = (mask) * Sn\n",
    "            S_hat = S_hat.reshape(-1,513).T\n",
    "            \n",
    "            lenght_w = S_hat.shape[1]/10\n",
    "            for clip in range(10):\n",
    "                start_w = clip*lenght_w\n",
    "                end_w = (clip+1)*lenght_w\n",
    "                \n",
    "                wav = S_hat[:,start_w:end_w].T\n",
    "                S_time=librosa.istft(wav, hop_length=512)\n",
    "                fname = PATH_directory+PATH_denoise+ e + \"_redoise.wav\"\n",
    "                \n",
    "                print(audio_fname)\n",
    "                librosa.output.write_wav(fname, S_time, sr)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_audio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create IRM for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('DATA_train_s',DATA_train_s.shape)\n",
    "print('DATA_train_n',DATA_train_n.shape)\n",
    "print('DATA_train_x',DATA_train_x.shape)\n",
    "\n",
    "DATA_train_s = DATA_train_s.reshape( (513,5,-1))\n",
    "DATA_train_n = DATA_train_n.reshape( (513,5,-1))\n",
    "DATA_train_x = DATA_train_x.reshape( (513,5,-1))\n",
    "DATA_train_M = 1*(DATA_train_s>DATA_train_n)\n",
    "\n",
    "print('DATA_train_s',DATA_train_s.shape)\n",
    "print('DATA_train_n',DATA_train_n.shape)\n",
    "print('DATA_train_x',DATA_train_x.shape)\n",
    "print('DATA_train_M',DATA_train_M.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA_val_s = None\n",
    "DATA_val_n = None\n",
    "DATA_val_x = None\n",
    "\n",
    "for file_s, file_n, file_x in zip(sorted(glob.glob(PATH_directory+PATH_val+CLEAN_format_val)),sorted(glob.glob(PATH_directory+PATH_val+NOISE_format_val)),sorted(glob.glob(PATH_directory+PATH_val+MIX_format_val))):\n",
    "    \n",
    "    DATA_val_s = np.array(preprossed_data(file_s)) if DATA_val_s is None else np.concatenate( (DATA_val_s,preprossed_data(file_s)),axis=1)\n",
    "    DATA_val_n = np.array(preprossed_data(file_n)) if DATA_val_n is None else np.concatenate( (DATA_val_n,preprossed_data(file_n)),axis=1)\n",
    "    DATA_val_x = np.array(preprossed_data(file_x)) if DATA_val_x is None else np.concatenate( (DATA_val_x,preprossed_data(file_x)),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('DATA_val_s',DATA_val_s.shape)\n",
    "print('DATA_val_n',DATA_val_n.shape)\n",
    "print('DATA_val_x',DATA_val_x.shape)\n",
    "\n",
    "DATA_val_s = DATA_val_s.reshape( (513,5,-1))\n",
    "DATA_val_n = DATA_val_n.reshape( (513,5,-1))\n",
    "DATA_val_x = DATA_val_x.reshape( (513,5,-1))\n",
    "DATA_val_M = 1*(DATA_val_s>DATA_val_n)\n",
    "\n",
    "print('DATA_val_s',DATA_val_s.shape)\n",
    "print('DATA_val_n',DATA_val_n.shape)\n",
    "print('DATA_val_x',DATA_val_x.shape)\n",
    "print('DATA_val_M',DATA_val_M.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA_test_x = None\n",
    "\n",
    "for file_x in sorted(glob.glob(PATH_directory+PATH_test+MIX_format_test)):\n",
    "    DATA_test_x = np.array(preprossed_data(file_x)) if DATA_test_x is None else np.concatenate( (DATA_test_x,preprossed_data(file_x)),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DATA_test_x',DATA_test_x.shape)\n",
    "DATA_test_x = DATA_test_x.reshape( (513,5,-1))\n",
    "\n",
    "print('DATA_test_x',DATA_test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprossed_dataX(file_name):\n",
    "    \n",
    "    sn, sr=librosa.load(file_name, sr=None)\n",
    "    X=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Data Complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_val_sX = None\n",
    "DATA_val_xX = None\n",
    "\n",
    "for file_s, file_x in zip( sorted(glob.glob(PATH_directory+PATH_val+CLEAN_format_val)), sorted(glob.glob(PATH_directory+PATH_val+MIX_format_val))):\n",
    "    \n",
    "    DATA_val_sX = np.array(preprossed_dataX(file_s)) if DATA_val_sX is None else np.concatenate( (DATA_val_sX,preprossed_dataX(file_s)),axis=1)\n",
    "    DATA_val_xX = np.array(preprossed_dataX(file_x)) if DATA_val_xX is None else np.concatenate( (DATA_val_xX,preprossed_dataX(file_x)),axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_val_sX (513, 118550)\n",
      "DATA_val_xX (513, 118550)\n",
      "DATA_val_sX (513, 5, 23710)\n",
      "DATA_val_xX (513, 5, 23710)\n"
     ]
    }
   ],
   "source": [
    "print('DATA_val_sX',DATA_val_sX.shape)\n",
    "print('DATA_val_xX',DATA_val_xX.shape)\n",
    "\n",
    "DATA_val_sX = DATA_val_sX.reshape( (513,5,-1))\n",
    "DATA_val_xX = DATA_val_xX.reshape( (513,5,-1))\n",
    "\n",
    "print('DATA_val_sX',DATA_val_sX.shape)\n",
    "print('DATA_val_xX',DATA_val_xX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('DATA_val_sX.npy', DATA_val_sX)\n",
    "np.save('DATA_val_xX.npy', DATA_val_xX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_test_x (513, 44150)\n",
      "DATA_test_x (513, 5, 8830)\n"
     ]
    }
   ],
   "source": [
    "DATA_val_sX_f = np.load('DATA_val_sX.npy')\n",
    "DATA_val_xX_f = np.load('DATA_val_xX.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Dataset in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DATA_train_s' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-54af169ce951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DATA_train_s.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_train_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DATA_train_n.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_train_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DATA_train_x.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_train_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DATA_train_M.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA_train_M\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DATA_train_s' is not defined"
     ]
    }
   ],
   "source": [
    "np.save('DATA_train_s.npy', DATA_train_s)\n",
    "np.save('DATA_train_n.npy', DATA_train_n)\n",
    "np.save('DATA_train_x.npy', DATA_train_x)\n",
    "np.save('DATA_train_M.npy', DATA_train_M)\n",
    "\n",
    "\n",
    "np.save('DATA_val_s.npy', DATA_val_s)\n",
    "np.save('DATA_val_n.npy', DATA_val_n)\n",
    "np.save('DATA_val_x.npy', DATA_val_x)\n",
    "np.save('DATA_val_M.npy', DATA_val_M)\n",
    "\n",
    "np.save('DATA_test_x.npy', DATA_test_x)\n",
    "\n",
    "np.save('DATA_val_sX.npy', DATA_val_sX)\n",
    "np.save('DATA_val_xX.npy', DATA_val_xX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_train_s_f = np.load('DATA_train_s.npy')\n",
    "DATA_train_n_f = np.load('DATA_train_n.npy')\n",
    "DATA_train_x_f = np.load('DATA_train_x.npy')\n",
    "DATA_train_M_f = np.load('DATA_train_M.npy')\n",
    "\n",
    "DATA_val_s_f = np.load('DATA_val_s.npy')\n",
    "DATA_val_n_f = np.load('DATA_val_n.npy')\n",
    "DATA_val_x_f = np.load('DATA_val_x.npy')\n",
    "DATA_val_M_f = np.load('DATA_val_M.npy')\n",
    "\n",
    "DATA_test_x_f = np.load('DATA_test_x.npy')\n",
    "\n",
    "DATA_val_sX_f = np.load('DATA_val_sX.npy')\n",
    "DATA_val_xX_f = np.load('DATA_val_xX.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_train_s_f (513, 5, 23710)\n",
      "DATA_train_M_f (513, 5, 23710)\n",
      "DATA_val_x_f (513, 5, 23710)\n",
      "DATA_test_x_f (513, 5, 8830)\n",
      "DATA_val_xX_f (513, 5, 23710)\n"
     ]
    }
   ],
   "source": [
    "print('DATA_train_s_f', DATA_train_s_f.shape)\n",
    "print('DATA_train_M_f', DATA_train_M_f.shape)\n",
    "print('DATA_val_x_f', DATA_val_x_f.shape)\n",
    "print('DATA_test_x_f', DATA_test_x_f.shape)\n",
    "print('DATA_val_xX_f', DATA_val_xX_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_train_s_f (513, 118550)\n",
      "DATA_train_s_f (118550, 513)\n",
      "DATA_train_s_f (23710, 5, 513)\n"
     ]
    }
   ],
   "source": [
    "DATA_train_s_f = DATA_train_s_f.reshape(513,-1)\n",
    "print('DATA_train_s_f', DATA_train_s_f.shape)\n",
    "\n",
    "DATA_train_s_f = DATA_train_s_f.T\n",
    "print('DATA_train_s_f', DATA_train_s_f.shape)\n",
    "\n",
    "DATA_train_s_f = DATA_train_s_f.reshape(-1,5,513)\n",
    "print('DATA_train_s_f', DATA_train_s_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_train_n_f = (DATA_train_n_f.reshape(513,-1).T).reshape(-1,5,513)\n",
    "DATA_train_x_f = (DATA_train_x_f.reshape(513,-1).T).reshape(-1,5,513)\n",
    "DATA_train_M_f = (DATA_train_M_f.reshape(513,-1).T).reshape(-1,5,513)\n",
    "\n",
    "DATA_val_s_f = (DATA_val_s_f.reshape(513,-1).T).reshape(-1,5,513)\n",
    "DATA_val_n_f = (DATA_val_n_f.reshape(513,-1).T).reshape(-1,5,513)\n",
    "DATA_val_x_f = (DATA_val_x_f.reshape(513,-1).T).reshape(-1,5,513)\n",
    "DATA_val_M_f = (DATA_val_M_f.reshape(513,-1).T).reshape(-1,5,513)\n",
    "\n",
    "DATA_test_x_f = (DATA_test_x_f.reshape(513,-1).T).reshape(-1,5,513)\n",
    "\n",
    "DATA_val_sX_f = (DATA_val_sX_f.reshape(513,-1).T).reshape(-1,5,513)\n",
    "DATA_val_xX_f = (DATA_val_xX_f.reshape(513,-1).T).reshape(-1,5,513)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_train_s_f (23710, 5, 513)\n",
      "DATA_train_M_f (23710, 5, 513)\n",
      "DATA_val_x_f (23710, 5, 513)\n",
      "DATA_test_x_f (8830, 5, 513)\n",
      "DATA_val_xX_f (23710, 5, 513)\n"
     ]
    }
   ],
   "source": [
    "print('DATA_train_s_f', DATA_train_s_f.shape)\n",
    "print('DATA_train_M_f', DATA_train_M_f.shape)\n",
    "print('DATA_val_x_f', DATA_val_x_f.shape)\n",
    "print('DATA_test_x_f', DATA_test_x_f.shape)\n",
    "print('DATA_val_xX_f', DATA_val_xX_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23710, 5, 513, 2)\n",
      "(23710, 5, 513, 2)\n"
     ]
    }
   ],
   "source": [
    "tr_categorical_labels = to_categorical(DATA_train_M_f, num_classes=2)\n",
    "print(tr_categorical_labels.shape)\n",
    "\n",
    "v_categorical_labels = to_categorical(DATA_val_M_f, num_classes=2)\n",
    "print(v_categorical_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_RNN = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 5, 10)             15570     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 10)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5, 513)            5643      \n",
      "=================================================================\n",
      "Total params: 21,213\n",
      "Trainable params: 21,213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/Anaconda3-5.0.1/envs/e533/lib/python3.6/site-packages/keras/models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23710 samples, validate on 23710 samples\n",
      "Epoch 1/10\n",
      "23710/23710 [==============================] - 55s 2ms/step - loss: 0.5898 - acc: 0.6839 - val_loss: 0.5429 - val_acc: 0.7291\n",
      "Epoch 2/10\n",
      "23710/23710 [==============================] - 52s 2ms/step - loss: 0.5468 - acc: 0.7229 - val_loss: 0.5234 - val_acc: 0.7439\n",
      "Epoch 3/10\n",
      "23710/23710 [==============================] - 52s 2ms/step - loss: 0.5346 - acc: 0.7325 - val_loss: 0.5102 - val_acc: 0.7529\n",
      "Epoch 4/10\n",
      "23710/23710 [==============================] - 53s 2ms/step - loss: 0.5267 - acc: 0.7385 - val_loss: 0.5061 - val_acc: 0.7552\n",
      "Epoch 5/10\n",
      "23710/23710 [==============================] - 53s 2ms/step - loss: 0.5212 - acc: 0.7429 - val_loss: 0.5035 - val_acc: 0.7576\n",
      "Epoch 6/10\n",
      "23710/23710 [==============================] - 52s 2ms/step - loss: 0.5175 - acc: 0.7459 - val_loss: 0.4956 - val_acc: 0.7625\n",
      "Epoch 7/10\n",
      "23710/23710 [==============================] - 52s 2ms/step - loss: 0.5145 - acc: 0.7477 - val_loss: 0.4921 - val_acc: 0.7653\n",
      "Epoch 8/10\n",
      "23710/23710 [==============================] - 52s 2ms/step - loss: 0.5125 - acc: 0.7493 - val_loss: 0.4954 - val_acc: 0.7636\n",
      "Epoch 9/10\n",
      "23710/23710 [==============================] - 52s 2ms/step - loss: 0.5103 - acc: 0.7507 - val_loss: 0.4912 - val_acc: 0.7656\n",
      "Epoch 10/10\n",
      "23710/23710 [==============================] - 89s 4ms/step - loss: 0.5079 - acc: 0.7526 - val_loss: 0.4871 - val_acc: 0.7687\n",
      "Accuracy: 76.87%\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add( Bidirectional(GRU(Max_RNN, return_sequences=True), input_shape=(Max_RNN,513) ))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(513, activation='sigmoid'))\n",
    "          \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# model.fit(DATA_train_x_f.T, tr_categorical_labels, validation_data=(DATA_val_x_f.T,v_categorical_labels), shuffle=True, nb_epoch=100, batch_size=10)\n",
    "# # Final evaluation of the model\n",
    "# scores = model.evaluate(DATA_val_x_f.T, v_categorical_labels, verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "model.fit(DATA_train_x_f, DATA_train_M_f, validation_data=(DATA_val_x_f,DATA_val_M_f), shuffle=True, nb_epoch=10, batch_size=10)\n",
    "\n",
    "# model.fit(DATA_train_x_f, tr_categorical_labels, validation_data=(DATA_val_x_f,v_categorical_labels), shuffle=True, nb_epoch=10, batch_size=10)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(DATA_val_x_f, DATA_val_M_f, verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/Anaconda3-5.0.1/envs/e533/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `GRU` call to the Keras 2 API: `GRU(input_shape=(5, 513), return_sequences=True, units=513)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_1 (GRU)                  (None, 5, 513)            1580553   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5, 513)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 5, 513)            263682    \n",
      "=================================================================\n",
      "Total params: 1,844,235\n",
      "Trainable params: 1,844,235\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 23710 samples, validate on 23710 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(GRU(output_dim = 513, input_shape=(Max_RNN,513), return_sequences=True))\n",
    "# model.add(GRU(output_dim = 513, input_length = 5, input_dim = 513, return_sequences=True))\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "model.add(TimeDistributed(Dense(513, activation='sigmoid')))\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(DATA_train_x_f, DATA_train_M_f, validation_data=(DATA_val_x_f,DATA_val_M_f), shuffle=True, epochs=10, batch_size=10)\n",
    "\n",
    "# model.fit(DATA_train_x_f, tr_categorical_labels, validation_data=(DATA_val_x_f,v_categorical_labels), shuffle=True, nb_epoch=10, batch_size=10)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(DATA_val_x_f, DATA_val_M_f, verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprossed_dataX(file_name):\n",
    "    \n",
    "    sn, sr=librosa.load(file_name, sr=None)\n",
    "    X=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_val_sX = None\n",
    "DATA_val_xX = None\n",
    "\n",
    "for file_s, file_x in zip( sorted(glob.glob(PATH_directory+PATH_val+CLEAN_format_val)), sorted(glob.glob(PATH_directory+PATH_val+MIX_format_val))):\n",
    "    \n",
    "    DATA_val_sX = np.array(preprossed_dataX(file_s)) if DATA_val_sX is None else np.concatenate( (DATA_val_sX,preprossed_dataX(file_s)),axis=1)\n",
    "    DATA_val_xX = np.array(preprossed_dataX(file_x)) if DATA_val_xX is None else np.concatenate( (DATA_val_xX,preprossed_dataX(file_x)),axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_val_xX (513, 118550)\n",
      "DATA_val_xX (513, 5, 23710)\n"
     ]
    }
   ],
   "source": [
    "print('DATA_val_sX',DATA_val_sX.shape)\n",
    "print('DATA_val_xX',DATA_val_xX.shape)\n",
    "\n",
    "DATA_val_sX = DATA_val_sX.reshape( (513,5,-1))\n",
    "DATA_val_xX = DATA_val_xX.reshape( (513,5,-1))\n",
    "\n",
    "print('DATA_val_sX',DATA_val_sX.shape)\n",
    "print('DATA_val_xX',DATA_val_xX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('DATA_val_sX.npy', DATA_val_sX)\n",
    "np.save('DATA_val_xX.npy', DATA_val_xX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_val_sX_f = np.load('DATA_val_sX.npy')\n",
    "DATA_val_xX_f = np.load('DATA_val_xX.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model : the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 400 arrays: [array([[0.31088, 0.00439, 0.02531, ..., 0.03555, 0.05921, 0.02048],\n       [0.20403, 0.21771, 0.24856, ..., 0.07964, 0.19597, 0.11246],\n       [0.00054, 0.27443, 0.5295 , ..., 0.14783, 0.20548, 0.119...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-255f5ea9f2e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_test_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/Anaconda3-5.0.1/envs/e533/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[0;32m-> 1025\u001b[0;31m                                   steps=steps)\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/Anaconda3-5.0.1/envs/e533/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1822\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1823\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1824\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1825\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/Anaconda3-5.0.1/envs/e533/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model : the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 400 arrays: [array([[0.31088, 0.00439, 0.02531, ..., 0.03555, 0.05921, 0.02048],\n       [0.20403, 0.21771, 0.24856, ..., 0.07964, 0.19597, 0.11246],\n       [0.00054, 0.27443, 0.5295 , ..., 0.14783, 0.20548, 0.119..."
     ]
    }
   ],
   "source": [
    "labels = model.predict(DATA_test_x)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_hat = (labels) * DATA_val_xX_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513, 118550)\n",
      "(513, 118550)\n"
     ]
    }
   ],
   "source": [
    "# S_org = (DATA_val_s_f).flatten()\n",
    "# S_pred = S_hat.flatten()\n",
    "st = DATA_val_sX_f.reshape(513,-1)\n",
    "st_hat = S_hat.reshape(513,-1)\n",
    "\n",
    "print(st.shape)\n",
    "print(st_hat.shape)\n",
    "\n",
    "S_org = librosa.istft(st, hop_length=512)\n",
    "S_pred = librosa.istft(st_hat, hop_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60697088,) (60697088,)\n"
     ]
    }
   ],
   "source": [
    "print(S_pred.shape, S_org.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1112037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.139689683914185"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = np.sum(S_org*S_org)/ np.sum((S_org-S_pred)*(S_org-S_pred))\n",
    "print(acc)\n",
    "\n",
    "10*np.log10(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-09b5a3680f5d>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-09b5a3680f5d>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    sn, sr=librosa.load('train_dirty_male.wav', sr=None)\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Noisy -> Input Data\n",
    "sn, sr=librosa.load('train_dirty_male.wav', sr=None)\n",
    "X=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "X_mag = np.abs(X)\n",
    "\n",
    "# Clean -> Label\n",
    "s, sr=librosa.load('train_clean_male.wav', sr=None)\n",
    "S=librosa.stft(s, n_fft=1024, hop_length=512)\n",
    "S_mag = np.abs(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for generating next batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(X,Y, batch_size):\n",
    "    num_samples, _ = X.shape\n",
    "    \n",
    "    selected_indics = np.random.randint(num_samples-batch_size)\n",
    "#     print(selected_indics)\n",
    "    return X[selected_indics:selected_indics+batch_size], Y[selected_indics:selected_indics+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITERATION = 1000\n",
    "BATCH_SIZE = X_mag.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier Initialization of Weights\n",
    "These are the weight initialization function used in defining model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable (shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = np.sqrt(2.0/sum(shape)) )\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable (shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = np.sqrt(1.0/sum(shape)) )\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the fully connected model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 513]) \n",
    "\n",
    "W_1 = weight_variable([513, 1024])\n",
    "b_1 = bias_variable([1024])\n",
    "\n",
    "W_2 = weight_variable([1024, 1024])\n",
    "b_2 = bias_variable([1024])\n",
    "\n",
    "W_3 = weight_variable([1024, 1024])\n",
    "b_3 = bias_variable([1024])\n",
    "\n",
    "W_4 = weight_variable([1024, 1024])\n",
    "b_4 = bias_variable([1024])\n",
    "\n",
    "W_5 = weight_variable([1024, 513])\n",
    "b_5 = bias_variable([513])\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, [None, 513]) # original\n",
    "\n",
    "\n",
    "# Layer connections and Activation functions\n",
    "y_1 = tf.nn.relu(tf.matmul(x, W_1) + b_1)\n",
    "y_2 = tf.nn.relu(tf.matmul(y_1, W_2) + b_2)\n",
    "y_3 = tf.nn.relu(tf.matmul(y_2, W_3) + b_3)\n",
    "y_4 = tf.nn.relu(tf.matmul(y_3, W_4) + b_4)\n",
    "y =  tf.nn.relu(tf.matmul(y_2, W_5) + b_5) # predicted\n",
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "mse = tf.reduce_sum( tf.losses.mean_squared_error(labels=y_, predictions=y) )\n",
    "train_step = tf.train.AdamOptimizer().minimize(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model\n",
    "We try different batch sizes, but for whole input dimension batch size gives better result and it removes the need of batch normalization. Since the input dimension is not very large, we can do this. For larger input dimensions, we have to adopt mini batch techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration to control GPU use\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.33\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "\n",
    "# Train Model\n",
    "for _ in range(NUM_ITERATION):\n",
    "    for _ in range(((X_mag.T).shape[0]//BATCH_SIZE)):\n",
    "        batch_xs, batch_ys = next_batch(X_mag.T,S_mag.T, BATCH_SIZE)\n",
    "        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513, 142)\n"
     ]
    }
   ],
   "source": [
    "# Load Test data-1\n",
    "sn, sr=librosa.load('test_x_01.wav', sr=None)\n",
    "X_test_01=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "X_mag_test_01 = np.abs(X_test_01)\n",
    "\n",
    "# Load Test data-2\n",
    "sn, sr=librosa.load('test_x_02.wav', sr=None)\n",
    "X_test_02=librosa.stft(sn, n_fft=1024, hop_length=512)\n",
    "X_mag_test_02 = np.abs(X_test_02)\n",
    "\n",
    "print(X_mag_test_01.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Test Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model-1\n",
    "S_hat_mag_test_01=sess.run(y, feed_dict={x: X_mag_test_01.T})\n",
    "S_hat_test_01=(X_test_01/X_mag_test_01)*S_hat_mag_test_01.T\n",
    "S_hat_01=librosa.istft(S_hat_test_01, hop_length=512)\n",
    "librosa.output.write_wav('test_s_01_recons.wav', S_hat_01, sr)\n",
    "\n",
    "# Test model-2\n",
    "S_hat_mag_test_02=sess.run(y, feed_dict={x: X_mag_test_02.T})\n",
    "S_hat_test_02=(X_test_02/X_mag_test_02)*S_hat_mag_test_02.T\n",
    "S_hat_02=librosa.istft(S_hat_test_02, hop_length=512)\n",
    "librosa.output.write_wav('test_s_02_recons.wav', S_hat_02, sr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
