{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected cross entropy loss if the model:\n",
      "- learns neither dependency: 0.661563238158\n",
      "- learns first dependency:   0.519166699707\n",
      "- learns both dependencies:  0.454454367449\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Expected cross entropy loss if the model:\")\n",
    "print(\"- learns neither dependency:\", -(0.625 * np.log(0.625) +\n",
    "                                      0.375 * np.log(0.375)))\n",
    "# Learns first dependency only ==> 0.51916669970720941\n",
    "print(\"- learns first dependency:  \",\n",
    "      -0.5 * (0.875 * np.log(0.875) + 0.125 * np.log(0.125))\n",
    "      -0.5 * (0.625 * np.log(0.625) + 0.375 * np.log(0.375)))\n",
    "print(\"- learns both dependencies: \", -0.50 * (0.75 * np.log(0.75) + 0.25 * np.log(0.25))\n",
    "      - 0.25 * (2 * 0.50 * np.log (0.50)) - 0.25 * (0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global config variables\n",
    "num_steps = 20 # number of truncated backprop steps ('n' in the discussion above)\n",
    "batch_size = 200\n",
    "num_classes = 2\n",
    "state_size = 1024\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(size=1000000):\n",
    "    X = np.array(np.random.choice(2, size=(size,)))\n",
    "    Y = []\n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if X[i-3] == 1:\n",
    "            threshold += 0.5\n",
    "        if X[i-8] == 1:\n",
    "            threshold -= 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X, np.array(Y)\n",
    "\n",
    "# adapted from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py\n",
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    raw_x, raw_y = raw_data\n",
    "    data_length = len(raw_x)\n",
    "\n",
    "    # partition raw data into batches and stack them vertically in a data matrix\n",
    "    batch_partition_length = data_length // batch_size\n",
    "    data_x = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = raw_x[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "        data_y[i] = raw_y[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "    # further divide batch partitions into num_steps for truncated backprop\n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, i * num_steps:(i + 1) * num_steps]\n",
    "        y = data_y[:, i * num_steps:(i + 1) * num_steps]\n",
    "        yield (x, y)\n",
    "\n",
    "def gen_epochs(n, num_steps):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\"\"\"\n",
    "Placeholders\n",
    "\"\"\"\n",
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "\"\"\"\n",
    "RNN Inputs\n",
    "\"\"\"\n",
    "\n",
    "# Turn our x placeholder into a list of one-hot tensors:\n",
    "# rnn_inputs is a list of num_steps tensors with shape [batch_size, num_classes]\n",
    "x_one_hot = tf.one_hot(x, num_classes)\n",
    "rnn_inputs = tf.unstack(x_one_hot, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Definition of rnn_cell\n",
    "\n",
    "This is very similar to the __call__ method on Tensorflow's BasicRNNCell.\n",
    "\"\"\"\n",
    "with tf.variable_scope('rnn_cell'):\n",
    "    W_h = tf.get_variable('W_h', [num_classes + state_size, state_size]) # Concatenating W_hh and W_xh for efficiency\n",
    "    b_h = tf.get_variable('b_h', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "\n",
    "def rnn_cell(rnn_input, state):\n",
    "    with tf.variable_scope('rnn_cell', reuse=True):\n",
    "        W_h = tf.get_variable('W_h', [num_classes + state_size, state_size])\n",
    "        b_h = tf.get_variable('b_h', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "    return tf.tanh(tf.matmul(tf.concat([rnn_input, state], axis=1), W_h) + b_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adding rnn_cells to graph\n",
    "\n",
    "This is a simplified version of the \"static_rnn\" function from Tensorflow's api. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn.py#L41\n",
    "Note: In practice, using \"dynamic_rnn\" is a better choice that the \"static_rnn\":\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L390\n",
    "\"\"\"\n",
    "state = init_state\n",
    "rnn_outputs = []\n",
    "for rnn_input in rnn_inputs:\n",
    "    state = rnn_cell(rnn_input, state)\n",
    "    rnn_outputs.append(state)\n",
    "final_state = rnn_outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predictions, loss, training step\n",
    "\n",
    "Losses is similar to the \"sequence_loss\"\n",
    "function from Tensorflow's API, except that here we are using a list of 2D tensors, instead of a 3D tensor. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/loss.py#L30\n",
    "\"\"\"\n",
    "\n",
    "#logits and predictions\n",
    "with tf.variable_scope('softmax'):\n",
    "    W_y = tf.get_variable('W_y', [state_size, num_classes])\n",
    "    b_y = tf.get_variable('b_y', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "logits = [tf.matmul(rnn_output, W_y) + b_y for rnn_output in rnn_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "# Turn our y placeholder into a list of labels\n",
    "y_as_list = tf.unstack(y, num=num_steps, axis=1)\n",
    "\n",
    "#losses and train_step\n",
    "losses = []\n",
    "for logit, label in zip(logits, y_as_list):\n",
    "    losses.append(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logit))\n",
    "\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train the network\n",
    "\"\"\"\n",
    "\n",
    "def train_network(num_epochs, num_steps, state_size=4):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        epoch_training_losses = np.zeros(num_epochs)\n",
    "        \n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):\n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            training_losses = []\n",
    "            \n",
    "            print(\"\\nEPOCH\", idx)\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                tr_losses, training_loss_, training_state, _ = \\\n",
    "                    sess.run([losses,\n",
    "                              total_loss,\n",
    "                              final_state,\n",
    "                              optimizer],\n",
    "                                  feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                training_loss += training_loss_\n",
    "                \n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    print(\"Average loss at step\", step,\n",
    "                              \":\", training_loss/100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                    training_loss = 0\n",
    "                    \n",
    "            \n",
    "            epoch_training_losses[idx] = np.mean(np.array(training_losses))\n",
    "            print(\"Epoch training loss :\", epoch_training_losses[idx])\n",
    "\n",
    "    return epoch_training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 0\n",
      "Average loss at step 100 : 5.23334505439\n",
      "Average loss at step 200 : 1.84934262514\n",
      "Average loss at step 300 : 1.33609352469\n",
      "Average loss at step 400 : 1.1687880075\n",
      "Epoch training loss : 2.39689230293\n",
      "\n",
      "EPOCH 1\n",
      "Average loss at step 100 : 0.900888188481\n",
      "Average loss at step 200 : 0.823753510118\n",
      "Average loss at step 300 : 0.79296028316\n",
      "Average loss at step 400 : 0.775264136195\n",
      "Epoch training loss : 0.823216529489\n",
      "\n",
      "EPOCH 2\n",
      "Average loss at step 100 : 0.718144524097\n",
      "Average loss at step 200 : 0.633815484047\n",
      "Average loss at step 300 : 0.577784819007\n",
      "Average loss at step 400 : 0.570665014386\n",
      "Epoch training loss : 0.625102460384\n",
      "\n",
      "EPOCH 3\n",
      "Average loss at step 100 : 0.556012340486\n",
      "Average loss at step 200 : 0.542415933907\n",
      "Average loss at step 300 : 0.5315213871\n",
      "Average loss at step 400 : 0.516890492439\n",
      "Epoch training loss : 0.536710038483\n",
      "\n",
      "EPOCH 4\n",
      "Average loss at step 100 : 0.503349284828\n",
      "Average loss at step 200 : 0.487604597211\n",
      "Average loss at step 300 : 0.482148230076\n",
      "Average loss at step 400 : 0.481402595341\n",
      "Epoch training loss : 0.488626176864\n",
      "\n",
      "EPOCH 5\n",
      "Average loss at step 100 : 0.476238286793\n",
      "Average loss at step 200 : 0.465720279813\n",
      "Average loss at step 300 : 0.463579128087\n",
      "Average loss at step 400 : 0.464377244413\n",
      "Epoch training loss : 0.467478734776\n",
      "\n",
      "EPOCH 6\n",
      "Average loss at step 100 : 0.466421151459\n",
      "Average loss at step 200 : 0.460134645104\n",
      "Average loss at step 300 : 0.461508817673\n",
      "Average loss at step 400 : 0.459557688832\n",
      "Epoch training loss : 0.461905575767\n",
      "\n",
      "EPOCH 7\n",
      "Average loss at step 100 : 0.465845044553\n",
      "Average loss at step 200 : 0.457312882543\n",
      "Average loss at step 300 : 0.458031934798\n",
      "Average loss at step 400 : 0.459322108924\n",
      "Epoch training loss : 0.460127992705\n",
      "\n",
      "EPOCH 8\n",
      "Average loss at step 100 : 0.462765629888\n",
      "Average loss at step 200 : 0.456665854156\n",
      "Average loss at step 300 : 0.459637043178\n",
      "Average loss at step 400 : 0.457694111168\n",
      "Epoch training loss : 0.459190659598\n",
      "\n",
      "EPOCH 9\n",
      "Average loss at step 100 : 0.464266802371\n",
      "Average loss at step 200 : 0.457052252591\n",
      "Average loss at step 300 : 0.458636054099\n",
      "Average loss at step 400 : 0.456194244623\n",
      "Epoch training loss : 0.459037338421\n",
      "\n",
      "EPOCH 10\n",
      "Average loss at step 100 : 0.463686208129\n",
      "Average loss at step 200 : 0.457094491124\n",
      "Average loss at step 300 : 0.456255460083\n",
      "Average loss at step 400 : 0.458052567244\n",
      "Epoch training loss : 0.458772181645\n",
      "\n",
      "EPOCH 11\n",
      "Average loss at step 100 : 0.462175582051\n",
      "Average loss at step 200 : 0.456961068511\n",
      "Average loss at step 300 : 0.457316469252\n",
      "Average loss at step 400 : 0.456861619651\n",
      "Epoch training loss : 0.458328684866\n",
      "\n",
      "EPOCH 12\n",
      "Average loss at step 100 : 0.464043084383\n",
      "Average loss at step 200 : 0.45510997355\n",
      "Average loss at step 300 : 0.454173409343\n",
      "Average loss at step 400 : 0.456625042558\n",
      "Epoch training loss : 0.457487877458\n",
      "\n",
      "EPOCH 13\n",
      "Average loss at step 100 : 0.462288098335\n",
      "Average loss at step 200 : 0.455600773394\n",
      "Average loss at step 300 : 0.454737587869\n",
      "Average loss at step 400 : 0.456221978962\n",
      "Epoch training loss : 0.45721210964\n",
      "\n",
      "EPOCH 14\n",
      "Average loss at step 100 : 0.461691367328\n",
      "Average loss at step 200 : 0.455782767832\n",
      "Average loss at step 300 : 0.456488939524\n",
      "Average loss at step 400 : 0.456585336328\n",
      "Epoch training loss : 0.457637102753\n",
      "\n",
      "EPOCH 15\n",
      "Average loss at step 100 : 0.460685555041\n",
      "Average loss at step 200 : 0.454689454734\n",
      "Average loss at step 300 : 0.456232987046\n",
      "Average loss at step 400 : 0.456805781424\n",
      "Epoch training loss : 0.457103444561\n",
      "\n",
      "EPOCH 16\n",
      "Average loss at step 100 : 0.46222558558\n",
      "Average loss at step 200 : 0.456746922135\n",
      "Average loss at step 300 : 0.454166937768\n",
      "Average loss at step 400 : 0.456392115355\n",
      "Epoch training loss : 0.45738289021\n",
      "\n",
      "EPOCH 17\n",
      "Average loss at step 100 : 0.461376978457\n",
      "Average loss at step 200 : 0.456552252173\n",
      "Average loss at step 300 : 0.457014449835\n",
      "Average loss at step 400 : 0.455724356472\n",
      "Epoch training loss : 0.457667009234\n",
      "\n",
      "EPOCH 18\n",
      "Average loss at step 100 : 0.461399672329\n",
      "Average loss at step 200 : 0.455158245265\n",
      "Average loss at step 300 : 0.457052716911\n",
      "Average loss at step 400 : 0.455829880536\n",
      "Epoch training loss : 0.45736012876\n",
      "\n",
      "EPOCH 19\n",
      "Average loss at step 100 : 0.462444660962\n",
      "Average loss at step 200 : 0.456427136362\n",
      "Average loss at step 300 : 0.455589141548\n",
      "Average loss at step 400 : 0.454673214555\n",
      "Epoch training loss : 0.457283538356\n",
      "\n",
      "EPOCH 20\n",
      "Average loss at step 100 : 0.461567353606\n",
      "Average loss at step 200 : 0.457536925673\n",
      "Average loss at step 300 : 0.456456206143\n",
      "Average loss at step 400 : 0.453522852361\n",
      "Epoch training loss : 0.457270834446\n",
      "\n",
      "EPOCH 21\n",
      "Average loss at step 100 : 0.460658386648\n",
      "Average loss at step 200 : 0.457760053575\n",
      "Average loss at step 300 : 0.455629811585\n",
      "Average loss at step 400 : 0.457087242007\n",
      "Epoch training loss : 0.457783873454\n",
      "\n",
      "EPOCH 22\n",
      "Average loss at step 100 : 0.463505396247\n",
      "Average loss at step 200 : 0.456842557788\n",
      "Average loss at step 300 : 0.45503924638\n",
      "Average loss at step 400 : 0.455641211569\n",
      "Epoch training loss : 0.457757102996\n",
      "\n",
      "EPOCH 23\n",
      "Average loss at step 100 : 0.461289710701\n",
      "Average loss at step 200 : 0.455266956389\n",
      "Average loss at step 300 : 0.455061666667\n",
      "Average loss at step 400 : 0.455361138582\n",
      "Epoch training loss : 0.456744868085\n",
      "\n",
      "EPOCH 24\n",
      "Average loss at step 100 : 0.460663078725\n",
      "Average loss at step 200 : 0.455678480566\n",
      "Average loss at step 300 : 0.456665535867\n",
      "Average loss at step 400 : 0.45492652148\n",
      "Epoch training loss : 0.45698340416\n",
      "\n",
      "EPOCH 25\n",
      "Average loss at step 100 : 0.461568645537\n",
      "Average loss at step 200 : 0.455587823093\n",
      "Average loss at step 300 : 0.455380829573\n",
      "Average loss at step 400 : 0.455910974741\n",
      "Epoch training loss : 0.457112068236\n",
      "\n",
      "EPOCH 26\n",
      "Average loss at step 100 : 0.460403424203\n",
      "Average loss at step 200 : 0.45574526459\n",
      "Average loss at step 300 : 0.455142435133\n",
      "Average loss at step 400 : 0.455719642341\n",
      "Epoch training loss : 0.456752691567\n",
      "\n",
      "EPOCH 27\n",
      "Average loss at step 100 : 0.46069367379\n",
      "Average loss at step 200 : 0.454797692895\n",
      "Average loss at step 300 : 0.455927658379\n",
      "Average loss at step 400 : 0.455562370718\n",
      "Epoch training loss : 0.456745348945\n",
      "\n",
      "EPOCH 28\n",
      "Average loss at step 100 : 0.46158079803\n",
      "Average loss at step 200 : 0.455303885043\n",
      "Average loss at step 300 : 0.453913326561\n",
      "Average loss at step 400 : 0.456294881999\n",
      "Epoch training loss : 0.456773222908\n",
      "\n",
      "EPOCH 29\n",
      "Average loss at step 100 : 0.46074519366\n",
      "Average loss at step 200 : 0.455347081423\n",
      "Average loss at step 300 : 0.45509629339\n",
      "Average loss at step 400 : 0.456850064695\n",
      "Epoch training loss : 0.457009658292\n",
      "\n",
      "EPOCH 30\n",
      "Average loss at step 100 : 0.461509671211\n",
      "Average loss at step 200 : 0.455710033476\n",
      "Average loss at step 300 : 0.454723774493\n",
      "Average loss at step 400 : 0.454291827977\n",
      "Epoch training loss : 0.456558826789\n",
      "\n",
      "EPOCH 31\n",
      "Average loss at step 100 : 0.462337569892\n",
      "Average loss at step 200 : 0.454841834009\n",
      "Average loss at step 300 : 0.455267782211\n",
      "Average loss at step 400 : 0.456072785854\n",
      "Epoch training loss : 0.457129992992\n",
      "\n",
      "EPOCH 32\n",
      "Average loss at step 100 : 0.461475946605\n",
      "Average loss at step 200 : 0.456591269672\n",
      "Average loss at step 300 : 0.455311101079\n",
      "Average loss at step 400 : 0.45478085041\n",
      "Epoch training loss : 0.457039791942\n",
      "\n",
      "EPOCH 33\n",
      "Average loss at step 100 : 0.461144828498\n",
      "Average loss at step 200 : 0.455477569401\n",
      "Average loss at step 300 : 0.454720174074\n",
      "Average loss at step 400 : 0.457462835908\n",
      "Epoch training loss : 0.45720135197\n",
      "\n",
      "EPOCH 34\n",
      "Average loss at step 100 : 0.460625702441\n",
      "Average loss at step 200 : 0.456640614867\n",
      "Average loss at step 300 : 0.455429161191\n",
      "Average loss at step 400 : 0.456279866695\n",
      "Epoch training loss : 0.457243836299\n",
      "\n",
      "EPOCH 35\n",
      "Average loss at step 100 : 0.461812931001\n",
      "Average loss at step 200 : 0.454302124977\n",
      "Average loss at step 300 : 0.455172853768\n",
      "Average loss at step 400 : 0.456181590855\n",
      "Epoch training loss : 0.45686737515\n",
      "\n",
      "EPOCH 36\n",
      "Average loss at step 100 : 0.461401243806\n",
      "Average loss at step 200 : 0.458022769094\n",
      "Average loss at step 300 : 0.455084266663\n",
      "Average loss at step 400 : 0.454505808651\n",
      "Epoch training loss : 0.457253522053\n",
      "\n",
      "EPOCH 37\n",
      "Average loss at step 100 : 0.460048207939\n",
      "Average loss at step 200 : 0.456102640927\n",
      "Average loss at step 300 : 0.456058193445\n",
      "Average loss at step 400 : 0.455394867361\n",
      "Epoch training loss : 0.456900977418\n",
      "\n",
      "EPOCH 38\n",
      "Average loss at step 100 : 0.462445116639\n",
      "Average loss at step 200 : 0.45735637337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 300 : 0.455855282545\n",
      "Average loss at step 400 : 0.45498953402\n",
      "Epoch training loss : 0.457661576644\n",
      "\n",
      "EPOCH 39\n",
      "Average loss at step 100 : 0.46190253824\n",
      "Average loss at step 200 : 0.455339223742\n",
      "Average loss at step 300 : 0.455311035216\n",
      "Average loss at step 400 : 0.455494381785\n",
      "Epoch training loss : 0.457011794746\n",
      "\n",
      "EPOCH 40\n",
      "Average loss at step 100 : 0.460901653469\n",
      "Average loss at step 200 : 0.453977181911\n",
      "Average loss at step 300 : 0.454856483936\n",
      "Average loss at step 400 : 0.455642648041\n",
      "Epoch training loss : 0.456344491839\n",
      "\n",
      "EPOCH 41\n",
      "Average loss at step 100 : 0.461754957736\n",
      "Average loss at step 200 : 0.455122143328\n",
      "Average loss at step 300 : 0.454703035355\n",
      "Average loss at step 400 : 0.455918884277\n",
      "Epoch training loss : 0.456874755174\n",
      "\n",
      "EPOCH 42\n",
      "Average loss at step 100 : 0.460518634021\n",
      "Average loss at step 200 : 0.455539596677\n",
      "Average loss at step 300 : 0.456188410521\n",
      "Average loss at step 400 : 0.455708051622\n",
      "Epoch training loss : 0.45698867321\n",
      "\n",
      "EPOCH 43\n",
      "Average loss at step 100 : 0.460603468418\n",
      "Average loss at step 200 : 0.454631170928\n",
      "Average loss at step 300 : 0.454665663242\n",
      "Average loss at step 400 : 0.45710973084\n",
      "Epoch training loss : 0.456752508357\n",
      "\n",
      "EPOCH 44\n",
      "Average loss at step 100 : 0.460744516551\n",
      "Average loss at step 200 : 0.454170205295\n",
      "Average loss at step 300 : 0.455933345556\n",
      "Average loss at step 400 : 0.455480385721\n",
      "Epoch training loss : 0.456582113281\n",
      "\n",
      "EPOCH 45\n",
      "Average loss at step 100 : 0.461199503839\n",
      "Average loss at step 200 : 0.457016104162\n",
      "Average loss at step 300 : 0.456313795745\n",
      "Average loss at step 400 : 0.455114405155\n",
      "Epoch training loss : 0.457410952225\n",
      "\n",
      "EPOCH 46\n",
      "Average loss at step 100 : 0.461096032858\n",
      "Average loss at step 200 : 0.45402125299\n",
      "Average loss at step 300 : 0.456061958075\n",
      "Average loss at step 400 : 0.454293328524\n",
      "Epoch training loss : 0.456368143111\n",
      "\n",
      "EPOCH 47\n",
      "Average loss at step 100 : 0.460956311524\n",
      "Average loss at step 200 : 0.45534591049\n",
      "Average loss at step 300 : 0.455708820522\n",
      "Average loss at step 400 : 0.455091025829\n",
      "Epoch training loss : 0.456775517091\n",
      "\n",
      "EPOCH 48\n",
      "Average loss at step 100 : 0.461438774467\n",
      "Average loss at step 200 : 0.455758868754\n",
      "Average loss at step 300 : 0.45637178123\n",
      "Average loss at step 400 : 0.454971005917\n",
      "Epoch training loss : 0.457135107592\n",
      "\n",
      "EPOCH 49\n",
      "Average loss at step 100 : 0.461430251896\n",
      "Average loss at step 200 : 0.455640369654\n",
      "Average loss at step 300 : 0.453878775835\n",
      "Average loss at step 400 : 0.455683935881\n",
      "Epoch training loss : 0.456658333316\n",
      "\n",
      "EPOCH 50\n",
      "Average loss at step 100 : 0.462077661157\n",
      "Average loss at step 200 : 0.455949152708\n",
      "Average loss at step 300 : 0.455916480124\n",
      "Average loss at step 400 : 0.455689198971\n",
      "Epoch training loss : 0.45740812324\n",
      "\n",
      "EPOCH 51\n",
      "Average loss at step 100 : 0.46203350246\n",
      "Average loss at step 200 : 0.455193292499\n",
      "Average loss at step 300 : 0.454169408679\n",
      "Average loss at step 400 : 0.454877730608\n",
      "Epoch training loss : 0.456568483561\n",
      "\n",
      "EPOCH 52\n",
      "Average loss at step 100 : 0.460038763583\n",
      "Average loss at step 200 : 0.454777151942\n",
      "Average loss at step 300 : 0.454627857208\n",
      "Average loss at step 400 : 0.455333034396\n",
      "Epoch training loss : 0.456194201782\n",
      "\n",
      "EPOCH 53\n",
      "Average loss at step 100 : 0.460637081265\n",
      "Average loss at step 200 : 0.455325821042\n",
      "Average loss at step 300 : 0.456757636368\n",
      "Average loss at step 400 : 0.454353966117\n",
      "Epoch training loss : 0.456768626198\n",
      "\n",
      "EPOCH 54\n",
      "Average loss at step 100 : 0.460770674944\n",
      "Average loss at step 200 : 0.455545423925\n",
      "Average loss at step 300 : 0.455017640293\n",
      "Average loss at step 400 : 0.45476221472\n",
      "Epoch training loss : 0.45652398847\n",
      "\n",
      "EPOCH 55\n",
      "Average loss at step 100 : 0.462022344172\n",
      "Average loss at step 200 : 0.454997921288\n",
      "Average loss at step 300 : 0.456796917915\n",
      "Average loss at step 400 : 0.454847676456\n",
      "Epoch training loss : 0.457166214958\n",
      "\n",
      "EPOCH 56\n",
      "Average loss at step 100 : 0.461470715702\n",
      "Average loss at step 200 : 0.455935389102\n",
      "Average loss at step 300 : 0.454840988815\n",
      "Average loss at step 400 : 0.456681753695\n",
      "Epoch training loss : 0.457232211828\n",
      "\n",
      "EPOCH 57\n",
      "Average loss at step 100 : 0.460587592423\n",
      "Average loss at step 200 : 0.454801788628\n",
      "Average loss at step 300 : 0.456239592731\n",
      "Average loss at step 400 : 0.454559242725\n",
      "Epoch training loss : 0.456547054127\n",
      "\n",
      "EPOCH 58\n",
      "Average loss at step 100 : 0.46117413044\n",
      "Average loss at step 200 : 0.455555945039\n",
      "Average loss at step 300 : 0.455786639452\n",
      "Average loss at step 400 : 0.456026064456\n",
      "Epoch training loss : 0.457135694847\n",
      "\n",
      "EPOCH 59\n",
      "Average loss at step 100 : 0.461690075994\n",
      "Average loss at step 200 : 0.455810423493\n",
      "Average loss at step 300 : 0.456315610409\n",
      "Average loss at step 400 : 0.456025332212\n",
      "Epoch training loss : 0.457460360527\n",
      "\n",
      "EPOCH 60\n",
      "Average loss at step 100 : 0.459996358752\n",
      "Average loss at step 200 : 0.45397472769\n",
      "Average loss at step 300 : 0.455900343657\n",
      "Average loss at step 400 : 0.454697272182\n",
      "Epoch training loss : 0.45614217557\n",
      "\n",
      "EPOCH 61\n",
      "Average loss at step 100 : 0.459699106812\n",
      "Average loss at step 200 : 0.453936518133\n",
      "Average loss at step 300 : 0.45456555903\n",
      "Average loss at step 400 : 0.454034151137\n",
      "Epoch training loss : 0.455558833778\n",
      "\n",
      "EPOCH 62\n",
      "Average loss at step 100 : 0.461991846561\n",
      "Average loss at step 200 : 0.453393294215\n",
      "Average loss at step 300 : 0.453587168753\n",
      "Average loss at step 400 : 0.455750795305\n",
      "Epoch training loss : 0.456180776209\n",
      "\n",
      "EPOCH 63\n",
      "Average loss at step 100 : 0.462569927275\n",
      "Average loss at step 200 : 0.454610234499\n",
      "Average loss at step 300 : 0.456211017072\n",
      "Average loss at step 400 : 0.45505023241\n",
      "Epoch training loss : 0.457110352814\n",
      "\n",
      "EPOCH 64\n",
      "Average loss at step 100 : 0.459425648749\n",
      "Average loss at step 200 : 0.455230348706\n",
      "Average loss at step 300 : 0.454628632069\n",
      "Average loss at step 400 : 0.456119193137\n",
      "Epoch training loss : 0.456350955665\n",
      "\n",
      "EPOCH 65\n",
      "Average loss at step 100 : 0.461280959845\n",
      "Average loss at step 200 : 0.45451885581\n",
      "Average loss at step 300 : 0.456400893331\n",
      "Average loss at step 400 : 0.45532029599\n",
      "Epoch training loss : 0.456880251244\n",
      "\n",
      "EPOCH 66\n",
      "Average loss at step 100 : 0.460836603343\n",
      "Average loss at step 200 : 0.456226527393\n",
      "Average loss at step 300 : 0.454049341977\n",
      "Average loss at step 400 : 0.455607224703\n",
      "Epoch training loss : 0.456679924354\n",
      "\n",
      "EPOCH 67\n",
      "Average loss at step 100 : 0.461615806818\n",
      "Average loss at step 200 : 0.455257757902\n",
      "Average loss at step 300 : 0.455499704182\n",
      "Average loss at step 400 : 0.455033969879\n",
      "Epoch training loss : 0.456851809695\n",
      "\n",
      "EPOCH 68\n",
      "Average loss at step 100 : 0.460734393299\n",
      "Average loss at step 200 : 0.45502361685\n",
      "Average loss at step 300 : 0.45540489465\n",
      "Average loss at step 400 : 0.456773921847\n",
      "Epoch training loss : 0.456984206662\n",
      "\n",
      "EPOCH 69\n",
      "Average loss at step 100 : 0.45957506299\n",
      "Average loss at step 200 : 0.456479792297\n",
      "Average loss at step 300 : 0.45537275672\n",
      "Average loss at step 400 : 0.454382689893\n",
      "Epoch training loss : 0.456452575475\n",
      "\n",
      "EPOCH 70\n",
      "Average loss at step 100 : 0.460871563852\n",
      "Average loss at step 200 : 0.456886771321\n",
      "Average loss at step 300 : 0.456235036552\n",
      "Average loss at step 400 : 0.456250659525\n",
      "Epoch training loss : 0.457561007813\n",
      "\n",
      "EPOCH 71\n",
      "Average loss at step 100 : 0.461657191217\n",
      "Average loss at step 200 : 0.455074113309\n",
      "Average loss at step 300 : 0.455566011667\n",
      "Average loss at step 400 : 0.455414667726\n",
      "Epoch training loss : 0.45692799598\n",
      "\n",
      "EPOCH 72\n",
      "Average loss at step 100 : 0.461981981993\n",
      "Average loss at step 200 : 0.455815679133\n",
      "Average loss at step 300 : 0.456111868918\n",
      "Average loss at step 400 : 0.454840589464\n",
      "Epoch training loss : 0.457187529877\n",
      "\n",
      "EPOCH 73\n",
      "Average loss at step 100 : 0.460935728252\n",
      "Average loss at step 200 : 0.455664910972\n",
      "Average loss at step 300 : 0.454442795813\n",
      "Average loss at step 400 : 0.455049832463\n",
      "Epoch training loss : 0.456523316875\n",
      "\n",
      "EPOCH 74\n",
      "Average loss at step 100 : 0.459941305518\n",
      "Average loss at step 200 : 0.455390610695\n",
      "Average loss at step 300 : 0.454619136453\n",
      "Average loss at step 400 : 0.45673317492\n",
      "Epoch training loss : 0.456671056896\n",
      "\n",
      "EPOCH 75\n",
      "Average loss at step 100 : 0.458867274821\n",
      "Average loss at step 200 : 0.45567537576\n",
      "Average loss at step 300 : 0.454298760593\n",
      "Average loss at step 400 : 0.455653946102\n",
      "Epoch training loss : 0.456123839319\n",
      "\n",
      "EPOCH 76\n",
      "Average loss at step 100 : 0.462167986929\n",
      "Average loss at step 200 : 0.453507395983\n",
      "Average loss at step 300 : 0.454970017076\n",
      "Average loss at step 400 : 0.454644821584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss : 0.456322555393\n",
      "\n",
      "EPOCH 77\n",
      "Average loss at step 100 : 0.46117611438\n",
      "Average loss at step 200 : 0.454446790516\n",
      "Average loss at step 300 : 0.454379842281\n",
      "Average loss at step 400 : 0.456502313018\n",
      "Epoch training loss : 0.456626265049\n",
      "\n",
      "EPOCH 78\n",
      "Average loss at step 100 : 0.462541861534\n",
      "Average loss at step 200 : 0.455571972132\n",
      "Average loss at step 300 : 0.453335564435\n",
      "Average loss at step 400 : 0.455470516384\n",
      "Epoch training loss : 0.456729978621\n",
      "\n",
      "EPOCH 79\n",
      "Average loss at step 100 : 0.460056195855\n",
      "Average loss at step 200 : 0.455051579475\n",
      "Average loss at step 300 : 0.454628020227\n",
      "Average loss at step 400 : 0.456688261628\n",
      "Epoch training loss : 0.456606014296\n",
      "\n",
      "EPOCH 80\n",
      "Average loss at step 100 : 0.460287311077\n",
      "Average loss at step 200 : 0.455304474831\n",
      "Average loss at step 300 : 0.454682426155\n",
      "Average loss at step 400 : 0.456355243027\n",
      "Epoch training loss : 0.456657363772\n",
      "\n",
      "EPOCH 81\n",
      "Average loss at step 100 : 0.460859349966\n",
      "Average loss at step 200 : 0.454837644696\n",
      "Average loss at step 300 : 0.454810636938\n",
      "Average loss at step 400 : 0.454674227536\n",
      "Epoch training loss : 0.456295464784\n",
      "\n",
      "EPOCH 82\n",
      "Average loss at step 100 : 0.459530143738\n",
      "Average loss at step 200 : 0.455072716475\n",
      "Average loss at step 300 : 0.455380467176\n",
      "Average loss at step 400 : 0.454743139744\n",
      "Epoch training loss : 0.456181616783\n",
      "\n",
      "EPOCH 83\n",
      "Average loss at step 100 : 0.461387350559\n",
      "Average loss at step 200 : 0.455415241718\n",
      "Average loss at step 300 : 0.455229353607\n",
      "Average loss at step 400 : 0.455920801759\n",
      "Epoch training loss : 0.456988186911\n",
      "\n",
      "EPOCH 84\n",
      "Average loss at step 100 : 0.460588218868\n",
      "Average loss at step 200 : 0.455814217031\n",
      "Average loss at step 300 : 0.455503412783\n",
      "Average loss at step 400 : 0.456082991958\n",
      "Epoch training loss : 0.45699721016\n",
      "\n",
      "EPOCH 85\n",
      "Average loss at step 100 : 0.461864286065\n",
      "Average loss at step 200 : 0.456280673444\n",
      "Average loss at step 300 : 0.455356628001\n",
      "Average loss at step 400 : 0.456161629856\n",
      "Epoch training loss : 0.457415804341\n",
      "\n",
      "EPOCH 86\n",
      "Average loss at step 100 : 0.461156057715\n",
      "Average loss at step 200 : 0.456161328852\n",
      "Average loss at step 300 : 0.454660101235\n",
      "Average loss at step 400 : 0.455292336643\n",
      "Epoch training loss : 0.456817456111\n",
      "\n",
      "EPOCH 87\n",
      "Average loss at step 100 : 0.460104227066\n",
      "Average loss at step 200 : 0.457048089206\n",
      "Average loss at step 300 : 0.455471217334\n",
      "Average loss at step 400 : 0.453079864085\n",
      "Epoch training loss : 0.456425849423\n",
      "\n",
      "EPOCH 88\n",
      "Average loss at step 100 : 0.461768343151\n",
      "Average loss at step 200 : 0.456664963067\n",
      "Average loss at step 300 : 0.454657519758\n",
      "Average loss at step 400 : 0.454198440313\n",
      "Epoch training loss : 0.456822316572\n",
      "\n",
      "EPOCH 89\n",
      "Average loss at step 100 : 0.461298427582\n",
      "Average loss at step 200 : 0.455780992806\n",
      "Average loss at step 300 : 0.454805171788\n",
      "Average loss at step 400 : 0.455717047453\n",
      "Epoch training loss : 0.456900409907\n",
      "\n",
      "EPOCH 90\n",
      "Average loss at step 100 : 0.460620406866\n",
      "Average loss at step 200 : 0.455936265588\n",
      "Average loss at step 300 : 0.45595813334\n",
      "Average loss at step 400 : 0.455601841211\n",
      "Epoch training loss : 0.457029161751\n",
      "\n",
      "EPOCH 91\n",
      "Average loss at step 100 : 0.460429545641\n",
      "Average loss at step 200 : 0.454969049096\n",
      "Average loss at step 300 : 0.452670317888\n",
      "Average loss at step 400 : 0.454823977053\n",
      "Epoch training loss : 0.45572322242\n",
      "\n",
      "EPOCH 92\n",
      "Average loss at step 100 : 0.461190699935\n",
      "Average loss at step 200 : 0.454630039036\n",
      "Average loss at step 300 : 0.45638520509\n",
      "Average loss at step 400 : 0.456642515063\n",
      "Epoch training loss : 0.457212114781\n",
      "\n",
      "EPOCH 93\n",
      "Average loss at step 100 : 0.461643916368\n",
      "Average loss at step 200 : 0.455847793818\n",
      "Average loss at step 300 : 0.455591614544\n",
      "Average loss at step 400 : 0.455381085873\n",
      "Epoch training loss : 0.457116102651\n",
      "\n",
      "EPOCH 94\n",
      "Average loss at step 100 : 0.459849950671\n",
      "Average loss at step 200 : 0.454812945127\n",
      "Average loss at step 300 : 0.455507762432\n",
      "Average loss at step 400 : 0.455244240761\n",
      "Epoch training loss : 0.456353724748\n",
      "\n",
      "EPOCH 95\n",
      "Average loss at step 100 : 0.461633310914\n",
      "Average loss at step 200 : 0.452881788313\n",
      "Average loss at step 300 : 0.455168717802\n",
      "Average loss at step 400 : 0.454456005394\n",
      "Epoch training loss : 0.456034955606\n",
      "\n",
      "EPOCH 96\n",
      "Average loss at step 100 : 0.459447480738\n",
      "Average loss at step 200 : 0.457071853876\n",
      "Average loss at step 300 : 0.45611910969\n",
      "Average loss at step 400 : 0.455980243683\n",
      "Epoch training loss : 0.457154671997\n",
      "\n",
      "EPOCH 97\n",
      "Average loss at step 100 : 0.46110742569\n",
      "Average loss at step 200 : 0.454833329022\n",
      "Average loss at step 300 : 0.455732800364\n",
      "Average loss at step 400 : 0.454511359036\n",
      "Epoch training loss : 0.456546228528\n",
      "\n",
      "EPOCH 98\n",
      "Average loss at step 100 : 0.461958554387\n",
      "Average loss at step 200 : 0.455182898045\n",
      "Average loss at step 300 : 0.45435269177\n",
      "Average loss at step 400 : 0.452493023872\n",
      "Epoch training loss : 0.455996792018\n",
      "\n",
      "EPOCH 99\n",
      "Average loss at step 100 : 0.460794308782\n",
      "Average loss at step 200 : 0.454929459691\n",
      "Average loss at step 300 : 0.457308549881\n",
      "Average loss at step 400 : 0.454780862629\n",
      "Epoch training loss : 0.456953295246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbcc06e7400>]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nv5nZiHHwm5mNGAe/mdmI+f+ldYh6pw/30QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbcc80fc0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_losses = train_network(100,num_steps, state_size)\n",
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
