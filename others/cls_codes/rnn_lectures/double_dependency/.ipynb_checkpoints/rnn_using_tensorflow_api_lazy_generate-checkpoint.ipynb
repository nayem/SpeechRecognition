{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from :  https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected cross entropy loss if the model:\n",
      "- learns neither dependency: 0.661563238158\n",
      "- learns first dependency:   0.519166699707\n",
      "- learns both dependencies:  0.454454367449\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Expected cross entropy loss if the model:\")\n",
    "print(\"- learns neither dependency:\", -(0.625 * np.log(0.625) +\n",
    "                                      0.375 * np.log(0.375)))\n",
    "# Learns first dependency only ==> 0.51916669970720941\n",
    "print(\"- learns first dependency:  \",\n",
    "      -0.5 * (0.875 * np.log(0.875) + 0.125 * np.log(0.125))\n",
    "      -0.5 * (0.625 * np.log(0.625) + 0.375 * np.log(0.375)))\n",
    "print(\"- learns both dependencies: \", -0.50 * (0.75 * np.log(0.75) + 0.25 * np.log(0.25))\n",
    "      - 0.25 * (2 * 0.50 * np.log (0.50)) - 0.25 * (0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global config variables\n",
    "num_steps = 10 # number of truncated backprop steps ('n' in the discussion above)\n",
    "num_seqs =  10000\n",
    "batch_size = 200\n",
    "num_classes = 2\n",
    "state_size = 1024\n",
    "learning_rate = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(size=1000000):\n",
    "    X = np.array(np.random.choice(2, size=(size,)))\n",
    "    Y = []\n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if X[i-3] == 1:\n",
    "            threshold += 0.5\n",
    "        if X[i-8] == 1:\n",
    "            threshold -= 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X, np.array(Y)\n",
    "\n",
    "# adapted from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py\n",
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    raw_x, raw_y = raw_data\n",
    "    data_length = len(raw_x)\n",
    "\n",
    "    # partition raw data into batches and stack them vertically in a data matrix\n",
    "    batch_partition_length = data_length // batch_size\n",
    "    data_x = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = raw_x[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "        data_y[i] = raw_y[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "    # further divide batch partitions into num_steps for truncated backprop\n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, i * num_steps:(i + 1) * num_steps]\n",
    "        y = data_y[:, i * num_steps:(i + 1) * num_steps]\n",
    "        yield (x, y)\n",
    "\n",
    "def gen_epochs(n, num_steps):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 10, 1024)\n",
      "(200, 1024)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\"\"\"\n",
    "Placeholders\n",
    "\"\"\"\n",
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "\"\"\"\n",
    "Inputs\n",
    "\"\"\"\n",
    "\n",
    "rnn_inputs = tf.one_hot(x, num_classes)\n",
    "\n",
    "\"\"\"\n",
    "RNN\n",
    "\"\"\"\n",
    "\n",
    "cell = tf.contrib.rnn.BasicRNNCell(state_size)\n",
    "rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "print(rnn_outputs.shape) # <n, t, h>\n",
    "print(final_state.shape) # <n, h>\n",
    "\"\"\"\n",
    "Predictions, loss, training step\n",
    "\"\"\"\n",
    "\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "logits = tf.reshape(\n",
    "            tf.matmul(tf.reshape(rnn_outputs, [-1, state_size]), W) + b,\n",
    "            [batch_size, num_steps, num_classes]) # <n, t, o>\n",
    "\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train the network\n",
    "\"\"\"\n",
    "\n",
    "def train_network(num_epochs, num_steps, state_size=4):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        epoch_training_losses = np.zeros(num_epochs)\n",
    "        \n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):\n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            training_losses = []\n",
    "            \n",
    "            print(\"\\nEPOCH\", idx)\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                tr_losses, training_loss_, training_state, _ = \\\n",
    "                    sess.run([losses,\n",
    "                              total_loss,\n",
    "                              final_state,\n",
    "                              optimizer],\n",
    "                                  feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                training_loss += training_loss_\n",
    "                \n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    print(\"Average loss at step\", step,\n",
    "                              \":\", training_loss/100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                    training_loss = 0\n",
    "                    \n",
    "            \n",
    "            epoch_training_losses[idx] = np.mean(np.array(training_losses))\n",
    "            print(\"Epoch training loss :\", epoch_training_losses[idx])\n",
    "\n",
    "    return epoch_training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 0\n",
      "Average loss at step 100 : 22.3305703998\n",
      "Average loss at step 200 : 9.82172878265\n",
      "Average loss at step 300 : 6.13336995363\n",
      "Average loss at step 400 : 2.76864891529\n",
      "Epoch training loss : 10.2635795128\n",
      "\n",
      "EPOCH 1\n",
      "Average loss at step 100 : 6.04686322927\n",
      "Average loss at step 200 : 5.11781975985\n",
      "Average loss at step 300 : 4.01131737471\n",
      "Average loss at step 400 : 1.91892935276\n",
      "Epoch training loss : 4.27373242915\n",
      "\n",
      "EPOCH 2\n",
      "Average loss at step 100 : 4.33756043196\n",
      "Average loss at step 200 : 3.85316433668\n",
      "Average loss at step 300 : 2.81386181712\n",
      "Average loss at step 400 : 1.34786674857\n",
      "Epoch training loss : 3.08811333358\n",
      "\n",
      "EPOCH 3\n",
      "Average loss at step 100 : 3.44485418797\n",
      "Average loss at step 200 : 3.10069979668\n",
      "Average loss at step 300 : 1.96051684856\n",
      "Average loss at step 400 : 2.16096857548\n",
      "Epoch training loss : 2.66675985217\n",
      "\n",
      "EPOCH 4\n",
      "Average loss at step 100 : 2.83920959473\n",
      "Average loss at step 200 : 1.84951525688\n",
      "Average loss at step 300 : 2.07423359513\n",
      "Average loss at step 400 : 2.76794553757\n",
      "Epoch training loss : 2.38272599608\n",
      "\n",
      "EPOCH 5\n",
      "Average loss at step 100 : 1.30633124411\n",
      "Average loss at step 200 : 2.66078771353\n",
      "Average loss at step 300 : 2.42150784254\n",
      "Average loss at step 400 : 1.99445730448\n",
      "Epoch training loss : 2.09577102616\n",
      "\n",
      "EPOCH 6\n",
      "Average loss at step 100 : 2.37656385183\n",
      "Average loss at step 200 : 2.08662888765\n",
      "Average loss at step 300 : 1.4051595211\n",
      "Average loss at step 400 : 2.25334269166\n",
      "Epoch training loss : 2.03042373806\n",
      "\n",
      "EPOCH 7\n",
      "Average loss at step 100 : 1.49723657012\n",
      "Average loss at step 200 : 2.10502549529\n",
      "Average loss at step 300 : 1.66285729885\n",
      "Average loss at step 400 : 1.54471102238\n",
      "Epoch training loss : 1.70245759666\n",
      "\n",
      "EPOCH 8\n",
      "Average loss at step 100 : 1.84627572417\n",
      "Average loss at step 200 : 1.61050277591\n",
      "Average loss at step 300 : 1.52311596453\n",
      "Average loss at step 400 : 1.85225816727\n",
      "Epoch training loss : 1.70803815797\n",
      "\n",
      "EPOCH 9\n",
      "Average loss at step 100 : 1.59457033157\n",
      "Average loss at step 200 : 1.8069973433\n",
      "Average loss at step 300 : 1.59586149693\n",
      "Average loss at step 400 : 1.55485291958\n",
      "Epoch training loss : 1.63807052284\n",
      "\n",
      "EPOCH 10\n",
      "Average loss at step 100 : 1.7207496047\n",
      "Average loss at step 200 : 1.63045002222\n",
      "Average loss at step 300 : 1.39331294954\n",
      "Average loss at step 400 : 1.58123590708\n",
      "Epoch training loss : 1.58143712088\n",
      "\n",
      "EPOCH 11\n",
      "Average loss at step 100 : 1.5084868443\n",
      "Average loss at step 200 : 1.46131222963\n",
      "Average loss at step 300 : 1.44236811042\n",
      "Average loss at step 400 : 1.42147653222\n",
      "Epoch training loss : 1.45841092914\n",
      "\n",
      "EPOCH 12\n",
      "Average loss at step 100 : 1.48342905879\n",
      "Average loss at step 200 : 1.43549110174\n",
      "Average loss at step 300 : 1.35794697165\n",
      "Average loss at step 400 : 1.41647987247\n",
      "Epoch training loss : 1.42333675116\n",
      "\n",
      "EPOCH 13\n",
      "Average loss at step 100 : 1.38555089116\n",
      "Average loss at step 200 : 1.3718467617\n",
      "Average loss at step 300 : 1.37406823874\n",
      "Average loss at step 400 : 1.36937651992\n",
      "Epoch training loss : 1.37521060288\n",
      "\n",
      "EPOCH 14\n",
      "Average loss at step 100 : 1.33142377913\n",
      "Average loss at step 200 : 1.3473737669\n",
      "Average loss at step 300 : 1.3383600831\n",
      "Average loss at step 400 : 1.32819073081\n",
      "Epoch training loss : 1.33633708999\n",
      "\n",
      "EPOCH 15\n",
      "Average loss at step 100 : 1.31639060259\n",
      "Average loss at step 200 : 1.3102663362\n",
      "Average loss at step 300 : 1.2893881464\n",
      "Average loss at step 400 : 1.27981242061\n",
      "Epoch training loss : 1.29896437645\n",
      "\n",
      "EPOCH 16\n",
      "Average loss at step 100 : 1.28870547652\n",
      "Average loss at step 200 : 1.27123202801\n",
      "Average loss at step 300 : 1.29079519868\n",
      "Average loss at step 400 : 1.26072233319\n",
      "Epoch training loss : 1.2778637591\n",
      "\n",
      "EPOCH 17\n",
      "Average loss at step 100 : 1.24105748892\n",
      "Average loss at step 200 : 1.22289598227\n",
      "Average loss at step 300 : 1.22869236231\n",
      "Average loss at step 400 : 1.24948092222\n",
      "Epoch training loss : 1.23553168893\n",
      "\n",
      "EPOCH 18\n",
      "Average loss at step 100 : 1.2289494437\n",
      "Average loss at step 200 : 1.22530058622\n",
      "Average loss at step 300 : 1.21346003294\n",
      "Average loss at step 400 : 1.19362973094\n",
      "Epoch training loss : 1.21533494845\n",
      "\n",
      "EPOCH 19\n",
      "Average loss at step 100 : 1.20394843996\n",
      "Average loss at step 200 : 1.18438124895\n",
      "Average loss at step 300 : 1.16868557334\n",
      "Average loss at step 400 : 1.18250584364\n",
      "Epoch training loss : 1.18488027647\n",
      "\n",
      "EPOCH 20\n",
      "Average loss at step 100 : 1.18104167819\n",
      "Average loss at step 200 : 1.17626993418\n",
      "Average loss at step 300 : 1.18275867105\n",
      "Average loss at step 400 : 1.14866940498\n",
      "Epoch training loss : 1.1721849221\n",
      "\n",
      "EPOCH 21\n",
      "Average loss at step 100 : 1.16005755305\n",
      "Average loss at step 200 : 1.15944187403\n",
      "Average loss at step 300 : 1.14530442715\n",
      "Average loss at step 400 : 1.14913021684\n",
      "Epoch training loss : 1.15348351777\n",
      "\n",
      "EPOCH 22\n",
      "Average loss at step 100 : 1.12004498065\n",
      "Average loss at step 200 : 1.12912219405\n",
      "Average loss at step 300 : 1.12136949182\n",
      "Average loss at step 400 : 1.10716610968\n",
      "Epoch training loss : 1.11942569405\n",
      "\n",
      "EPOCH 23\n",
      "Average loss at step 100 : 1.1094145453\n",
      "Average loss at step 200 : 1.11080117881\n",
      "Average loss at step 300 : 1.08280522823\n",
      "Average loss at step 400 : 1.09471864045\n",
      "Epoch training loss : 1.0994348982\n",
      "\n",
      "EPOCH 24\n",
      "Average loss at step 100 : 1.07982955694\n",
      "Average loss at step 200 : 1.0859950918\n",
      "Average loss at step 300 : 1.08705584824\n",
      "Average loss at step 400 : 1.0748028636\n",
      "Epoch training loss : 1.08192084014\n",
      "\n",
      "EPOCH 25\n",
      "Average loss at step 100 : 1.0664802283\n",
      "Average loss at step 200 : 1.07149971843\n",
      "Average loss at step 300 : 1.06189199448\n",
      "Average loss at step 400 : 1.06033655524\n",
      "Epoch training loss : 1.06505212411\n",
      "\n",
      "EPOCH 26\n",
      "Average loss at step 100 : 1.04693241775\n",
      "Average loss at step 200 : 1.07199223578\n",
      "Average loss at step 300 : 1.04811905682\n",
      "Average loss at step 400 : 1.03445185959\n",
      "Epoch training loss : 1.05037389249\n",
      "\n",
      "EPOCH 27\n",
      "Average loss at step 100 : 1.04065651655\n",
      "Average loss at step 200 : 1.03986642361\n",
      "Average loss at step 300 : 1.03152996659\n",
      "Average loss at step 400 : 1.02012413144\n",
      "Epoch training loss : 1.03304425955\n",
      "\n",
      "EPOCH 28\n",
      "Average loss at step 100 : 1.03836188138\n",
      "Average loss at step 200 : 1.02237163424\n",
      "Average loss at step 300 : 0.997115365267\n",
      "Average loss at step 400 : 1.02573432744\n",
      "Epoch training loss : 1.02089580208\n",
      "\n",
      "EPOCH 29\n",
      "Average loss at step 100 : 1.01513687134\n",
      "Average loss at step 200 : 0.986923508048\n",
      "Average loss at step 300 : 1.00421213686\n",
      "Average loss at step 400 : 1.00112897635\n",
      "Epoch training loss : 1.00185037315\n",
      "\n",
      "EPOCH 30\n",
      "Average loss at step 100 : 1.00790575802\n",
      "Average loss at step 200 : 0.992969785333\n",
      "Average loss at step 300 : 0.988822096586\n",
      "Average loss at step 400 : 0.987344928384\n",
      "Epoch training loss : 0.994260642081\n",
      "\n",
      "EPOCH 31\n",
      "Average loss at step 100 : 0.986007537842\n",
      "Average loss at step 200 : 0.966672339439\n",
      "Average loss at step 300 : 0.975852352381\n",
      "Average loss at step 400 : 0.968959326744\n",
      "Epoch training loss : 0.974372889102\n",
      "\n",
      "EPOCH 32\n",
      "Average loss at step 100 : 0.963474295735\n",
      "Average loss at step 200 : 0.953008019328\n",
      "Average loss at step 300 : 0.954331970811\n",
      "Average loss at step 400 : 0.959221681356\n",
      "Epoch training loss : 0.957508991808\n",
      "\n",
      "EPOCH 33\n",
      "Average loss at step 100 : 0.970122749805\n",
      "Average loss at step 200 : 0.946270233989\n",
      "Average loss at step 300 : 0.949784154296\n",
      "Average loss at step 400 : 0.959044539332\n",
      "Epoch training loss : 0.956305419356\n",
      "\n",
      "EPOCH 34\n",
      "Average loss at step 100 : 0.941186376214\n",
      "Average loss at step 200 : 0.9292549932\n",
      "Average loss at step 300 : 0.922432519197\n",
      "Average loss at step 400 : 0.941848863363\n",
      "Epoch training loss : 0.933680687994\n",
      "\n",
      "EPOCH 35\n",
      "Average loss at step 100 : 0.93944771111\n",
      "Average loss at step 200 : 0.942521268725\n",
      "Average loss at step 300 : 0.927002111673\n",
      "Average loss at step 400 : 0.921376792789\n",
      "Epoch training loss : 0.932586971074\n",
      "\n",
      "EPOCH 36\n",
      "Average loss at step 100 : 0.917984597087\n",
      "Average loss at step 200 : 0.915199401379\n",
      "Average loss at step 300 : 0.918863918185\n",
      "Average loss at step 400 : 0.92068684876\n",
      "Epoch training loss : 0.918183691353\n",
      "\n",
      "EPOCH 37\n",
      "Average loss at step 100 : 0.912490496635\n",
      "Average loss at step 200 : 0.899985797405\n",
      "Average loss at step 300 : 0.893993681073\n",
      "Average loss at step 400 : 0.911451004148\n",
      "Epoch training loss : 0.904480244815\n",
      "\n",
      "EPOCH 38\n",
      "Average loss at step 100 : 0.90318321228\n",
      "Average loss at step 200 : 0.893665734529\n",
      "Average loss at step 300 : 0.903013268709\n",
      "Average loss at step 400 : 0.880701192021\n",
      "Epoch training loss : 0.895140851885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 39\n",
      "Average loss at step 100 : 0.889384043813\n",
      "Average loss at step 200 : 0.886698371172\n",
      "Average loss at step 300 : 0.898734672666\n",
      "Average loss at step 400 : 0.877508904338\n",
      "Epoch training loss : 0.888081497997\n",
      "\n",
      "EPOCH 40\n",
      "Average loss at step 100 : 0.889790956378\n",
      "Average loss at step 200 : 0.866548358202\n",
      "Average loss at step 300 : 0.871712152362\n",
      "Average loss at step 400 : 0.879104768038\n",
      "Epoch training loss : 0.876789058745\n",
      "\n",
      "EPOCH 41\n",
      "Average loss at step 100 : 0.875146500468\n",
      "Average loss at step 200 : 0.878174923658\n",
      "Average loss at step 300 : 0.854384824038\n",
      "Average loss at step 400 : 0.862648019791\n",
      "Epoch training loss : 0.867588566989\n",
      "\n",
      "EPOCH 42\n",
      "Average loss at step 100 : 0.857760199904\n",
      "Average loss at step 200 : 0.868977528811\n",
      "Average loss at step 300 : 0.86684692502\n",
      "Average loss at step 400 : 0.85405307889\n",
      "Epoch training loss : 0.861909433156\n",
      "\n",
      "EPOCH 43\n",
      "Average loss at step 100 : 0.851809332967\n",
      "Average loss at step 200 : 0.852956593633\n",
      "Average loss at step 300 : 0.86336466372\n",
      "Average loss at step 400 : 0.85316637218\n",
      "Epoch training loss : 0.855324240625\n",
      "\n",
      "EPOCH 44\n",
      "Average loss at step 100 : 0.841594206691\n",
      "Average loss at step 200 : 0.845645985603\n",
      "Average loss at step 300 : 0.859968107939\n",
      "Average loss at step 400 : 0.835772498846\n",
      "Epoch training loss : 0.84574519977\n",
      "\n",
      "EPOCH 45\n",
      "Average loss at step 100 : 0.849026138783\n",
      "Average loss at step 200 : 0.831831527352\n",
      "Average loss at step 300 : 0.845600739717\n",
      "Average loss at step 400 : 0.836632850766\n",
      "Epoch training loss : 0.840772814155\n",
      "\n",
      "EPOCH 46\n",
      "Average loss at step 100 : 0.840388587713\n",
      "Average loss at step 200 : 0.835334083438\n",
      "Average loss at step 300 : 0.826341417432\n",
      "Average loss at step 400 : 0.830811431408\n",
      "Epoch training loss : 0.833218879998\n",
      "\n",
      "EPOCH 47\n",
      "Average loss at step 100 : 0.831559870839\n",
      "Average loss at step 200 : 0.813466206789\n",
      "Average loss at step 300 : 0.818744516969\n",
      "Average loss at step 400 : 0.820016709566\n",
      "Epoch training loss : 0.820946826041\n",
      "\n",
      "EPOCH 48\n",
      "Average loss at step 100 : 0.814328249097\n",
      "Average loss at step 200 : 0.816782500148\n",
      "Average loss at step 300 : 0.809031428099\n",
      "Average loss at step 400 : 0.823996554613\n",
      "Epoch training loss : 0.816034682989\n",
      "\n",
      "EPOCH 49\n",
      "Average loss at step 100 : 0.814964138269\n",
      "Average loss at step 200 : 0.813538856506\n",
      "Average loss at step 300 : 0.840839167833\n",
      "Average loss at step 400 : 0.803312543035\n",
      "Epoch training loss : 0.818163676411\n",
      "\n",
      "EPOCH 50\n",
      "Average loss at step 100 : 0.816131535769\n",
      "Average loss at step 200 : 0.798878033161\n",
      "Average loss at step 300 : 0.809893259406\n",
      "Average loss at step 400 : 0.815314478874\n",
      "Epoch training loss : 0.810054326802\n",
      "\n",
      "EPOCH 51\n",
      "Average loss at step 100 : 0.807749513984\n",
      "Average loss at step 200 : 0.79927461803\n",
      "Average loss at step 300 : 0.797156025171\n",
      "Average loss at step 400 : 0.793149799109\n",
      "Epoch training loss : 0.799332489073\n",
      "\n",
      "EPOCH 52\n",
      "Average loss at step 100 : 0.796640692949\n",
      "Average loss at step 200 : 0.803467176557\n",
      "Average loss at step 300 : 0.784590697289\n",
      "Average loss at step 400 : 0.78186289072\n",
      "Epoch training loss : 0.791640364379\n",
      "\n",
      "EPOCH 53\n",
      "Average loss at step 100 : 0.796687449217\n",
      "Average loss at step 200 : 0.790778067708\n",
      "Average loss at step 300 : 0.780557720661\n",
      "Average loss at step 400 : 0.78976061821\n",
      "Epoch training loss : 0.789445963949\n",
      "\n",
      "EPOCH 54\n",
      "Average loss at step 100 : 0.796273922324\n",
      "Average loss at step 200 : 0.791449574232\n",
      "Average loss at step 300 : 0.796098225713\n",
      "Average loss at step 400 : 0.791726112366\n",
      "Epoch training loss : 0.793886958659\n",
      "\n",
      "EPOCH 55\n",
      "Average loss at step 100 : 0.779738380909\n",
      "Average loss at step 200 : 0.768954769969\n",
      "Average loss at step 300 : 0.778803966045\n",
      "Average loss at step 400 : 0.776069775224\n",
      "Epoch training loss : 0.775891723037\n",
      "\n",
      "EPOCH 56\n",
      "Average loss at step 100 : 0.790563923717\n",
      "Average loss at step 200 : 0.758722275496\n",
      "Average loss at step 300 : 0.788856817484\n",
      "Average loss at step 400 : 0.774144968987\n",
      "Epoch training loss : 0.778071996421\n",
      "\n",
      "EPOCH 57\n",
      "Average loss at step 100 : 0.777904703617\n",
      "Average loss at step 200 : 0.767152141333\n",
      "Average loss at step 300 : 0.778228535056\n",
      "Average loss at step 400 : 0.755068855286\n",
      "Epoch training loss : 0.769588558823\n",
      "\n",
      "EPOCH 58\n",
      "Average loss at step 100 : 0.757373197079\n",
      "Average loss at step 200 : 0.765559583306\n",
      "Average loss at step 300 : 0.761956545115\n",
      "Average loss at step 400 : 0.756374348998\n",
      "Epoch training loss : 0.760315918624\n",
      "\n",
      "EPOCH 59\n",
      "Average loss at step 100 : 0.78501278758\n",
      "Average loss at step 200 : 0.761891465187\n",
      "Average loss at step 300 : 0.755092533827\n",
      "Average loss at step 400 : 0.749960713983\n",
      "Epoch training loss : 0.762989375144\n",
      "\n",
      "EPOCH 60\n",
      "Average loss at step 100 : 0.771177005172\n",
      "Average loss at step 200 : 0.739457695484\n",
      "Average loss at step 300 : 0.770317416191\n",
      "Average loss at step 400 : 0.754508550167\n",
      "Epoch training loss : 0.758865166754\n",
      "\n",
      "EPOCH 61\n",
      "Average loss at step 100 : 0.746296802759\n",
      "Average loss at step 200 : 0.757358856797\n",
      "Average loss at step 300 : 0.74641677022\n",
      "Average loss at step 400 : 0.7417570436\n",
      "Epoch training loss : 0.747957368344\n",
      "\n",
      "EPOCH 62\n",
      "Average loss at step 100 : 0.761594261527\n",
      "Average loss at step 200 : 0.742926738262\n",
      "Average loss at step 300 : 0.74932641685\n",
      "Average loss at step 400 : 0.739929529428\n",
      "Epoch training loss : 0.748444236517\n",
      "\n",
      "EPOCH 63\n",
      "Average loss at step 100 : 0.743362666368\n",
      "Average loss at step 200 : 0.741688138843\n",
      "Average loss at step 300 : 0.743299209476\n",
      "Average loss at step 400 : 0.759545680881\n",
      "Epoch training loss : 0.746973923892\n",
      "\n",
      "EPOCH 64\n",
      "Average loss at step 100 : 0.76028285563\n",
      "Average loss at step 200 : 0.756634062529\n",
      "Average loss at step 300 : 0.743921799064\n",
      "Average loss at step 400 : 0.767187893987\n",
      "Epoch training loss : 0.757006652802\n",
      "\n",
      "EPOCH 65\n",
      "Average loss at step 100 : 0.737357097268\n",
      "Average loss at step 200 : 0.749397693872\n",
      "Average loss at step 300 : 0.744890795946\n",
      "Average loss at step 400 : 0.716867169738\n",
      "Epoch training loss : 0.737128189206\n",
      "\n",
      "EPOCH 66\n",
      "Average loss at step 100 : 0.757767751813\n",
      "Average loss at step 200 : 0.729178059101\n",
      "Average loss at step 300 : 0.711893385649\n",
      "Average loss at step 400 : 0.719656175375\n",
      "Epoch training loss : 0.729623842984\n",
      "\n",
      "EPOCH 67\n",
      "Average loss at step 100 : 0.758462682366\n",
      "Average loss at step 200 : 0.747339857221\n",
      "Average loss at step 300 : 0.712914717793\n",
      "Average loss at step 400 : 0.737114992738\n",
      "Epoch training loss : 0.73895806253\n",
      "\n",
      "EPOCH 68\n",
      "Average loss at step 100 : 0.74761079967\n",
      "Average loss at step 200 : 0.732763462067\n",
      "Average loss at step 300 : 0.731047516465\n",
      "Average loss at step 400 : 0.7355265522\n",
      "Epoch training loss : 0.736737082601\n",
      "\n",
      "EPOCH 69\n",
      "Average loss at step 100 : 0.728848528862\n",
      "Average loss at step 200 : 0.733227574229\n",
      "Average loss at step 300 : 0.71881875515\n",
      "Average loss at step 400 : 0.733976588249\n",
      "Epoch training loss : 0.728717861623\n",
      "\n",
      "EPOCH 70\n",
      "Average loss at step 100 : 0.730526589751\n",
      "Average loss at step 200 : 0.727388988733\n",
      "Average loss at step 300 : 0.716082411408\n",
      "Average loss at step 400 : 0.698125206232\n",
      "Epoch training loss : 0.718030799031\n",
      "\n",
      "EPOCH 71\n",
      "Average loss at step 100 : 0.737446135283\n",
      "Average loss at step 200 : 0.705145730972\n",
      "Average loss at step 300 : 0.725980776548\n",
      "Average loss at step 400 : 0.726049650908\n",
      "Epoch training loss : 0.723655573428\n",
      "\n",
      "EPOCH 72\n",
      "Average loss at step 100 : 0.7199982059\n",
      "Average loss at step 200 : 0.72704952538\n",
      "Average loss at step 300 : 0.713917548656\n",
      "Average loss at step 400 : 0.703289939761\n",
      "Epoch training loss : 0.716063804924\n",
      "\n",
      "EPOCH 73\n",
      "Average loss at step 100 : 0.718597351909\n",
      "Average loss at step 200 : 0.719958130717\n",
      "Average loss at step 300 : 0.723280332088\n",
      "Average loss at step 400 : 0.714358827472\n",
      "Epoch training loss : 0.719048660547\n",
      "\n",
      "EPOCH 74\n",
      "Average loss at step 100 : 0.728647273779\n",
      "Average loss at step 200 : 0.725433496833\n",
      "Average loss at step 300 : 0.718836778402\n",
      "Average loss at step 400 : 0.712947577238\n",
      "Epoch training loss : 0.721466281563\n",
      "\n",
      "EPOCH 75\n",
      "Average loss at step 100 : 0.74198669374\n",
      "Average loss at step 200 : 0.720166572332\n",
      "Average loss at step 300 : 0.715094068646\n",
      "Average loss at step 400 : 0.705750333071\n",
      "Epoch training loss : 0.720749416947\n",
      "\n",
      "EPOCH 76\n",
      "Average loss at step 100 : 0.727822605371\n",
      "Average loss at step 200 : 0.714696299434\n",
      "Average loss at step 300 : 0.726812381744\n",
      "Average loss at step 400 : 0.717063102722\n",
      "Epoch training loss : 0.721598597318\n",
      "\n",
      "EPOCH 77\n",
      "Average loss at step 100 : 0.692355675697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 200 : 0.694354376197\n",
      "Average loss at step 300 : 0.719778177738\n",
      "Average loss at step 400 : 0.71781046927\n",
      "Epoch training loss : 0.706074674726\n",
      "\n",
      "EPOCH 78\n",
      "Average loss at step 100 : 0.709820660949\n",
      "Average loss at step 200 : 0.710062428713\n",
      "Average loss at step 300 : 0.687449287176\n",
      "Average loss at step 400 : 0.716001694798\n",
      "Epoch training loss : 0.705833517909\n",
      "\n",
      "EPOCH 79\n",
      "Average loss at step 100 : 0.700529034138\n",
      "Average loss at step 200 : 0.699285548925\n",
      "Average loss at step 300 : 0.714241468906\n",
      "Average loss at step 400 : 0.707386670113\n",
      "Epoch training loss : 0.705360680521\n",
      "\n",
      "EPOCH 80\n",
      "Average loss at step 100 : 0.700776774883\n",
      "Average loss at step 200 : 0.679536912441\n",
      "Average loss at step 300 : 0.719317144156\n",
      "Average loss at step 400 : 0.682032563686\n",
      "Epoch training loss : 0.695415848792\n",
      "\n",
      "EPOCH 81\n",
      "Average loss at step 100 : 0.690099233985\n",
      "Average loss at step 200 : 0.725041383505\n",
      "Average loss at step 300 : 0.706257320046\n",
      "Average loss at step 400 : 0.697268704772\n",
      "Epoch training loss : 0.704666660577\n",
      "\n",
      "EPOCH 82\n",
      "Average loss at step 100 : 0.701783311963\n",
      "Average loss at step 200 : 0.698314985037\n",
      "Average loss at step 300 : 0.713665702939\n",
      "Average loss at step 400 : 0.7170198524\n",
      "Epoch training loss : 0.707695963085\n",
      "\n",
      "EPOCH 83\n",
      "Average loss at step 100 : 0.68810769558\n",
      "Average loss at step 200 : 0.697013215423\n",
      "Average loss at step 300 : 0.714032231569\n",
      "Average loss at step 400 : 0.721759779453\n",
      "Epoch training loss : 0.705228230506\n",
      "\n",
      "EPOCH 84\n",
      "Average loss at step 100 : 0.686161875725\n",
      "Average loss at step 200 : 0.678610668778\n",
      "Average loss at step 300 : 0.704999026656\n",
      "Average loss at step 400 : 0.701457979679\n",
      "Epoch training loss : 0.69280738771\n",
      "\n",
      "EPOCH 85\n",
      "Average loss at step 100 : 0.698896810412\n",
      "Average loss at step 200 : 0.680005435944\n",
      "Average loss at step 300 : 0.665598968267\n",
      "Average loss at step 400 : 0.681515336037\n",
      "Epoch training loss : 0.681504137665\n",
      "\n",
      "EPOCH 86\n",
      "Average loss at step 100 : 0.693054873347\n",
      "Average loss at step 200 : 0.69330997467\n",
      "Average loss at step 300 : 0.705836038589\n",
      "Average loss at step 400 : 0.690912403464\n",
      "Epoch training loss : 0.695778322518\n",
      "\n",
      "EPOCH 87\n",
      "Average loss at step 100 : 0.725252965093\n",
      "Average loss at step 200 : 0.68876652658\n",
      "Average loss at step 300 : 0.692059698701\n",
      "Average loss at step 400 : 0.694432919621\n",
      "Epoch training loss : 0.700128027499\n",
      "\n",
      "EPOCH 88\n",
      "Average loss at step 100 : 0.698217617869\n",
      "Average loss at step 200 : 0.701212013364\n",
      "Average loss at step 300 : 0.693366544843\n",
      "Average loss at step 400 : 0.696646299958\n",
      "Epoch training loss : 0.697360619009\n",
      "\n",
      "EPOCH 89\n",
      "Average loss at step 100 : 0.700784977078\n",
      "Average loss at step 200 : 0.703842530251\n",
      "Average loss at step 300 : 0.700222351551\n",
      "Average loss at step 400 : 0.693744177818\n",
      "Epoch training loss : 0.699648509175\n",
      "\n",
      "EPOCH 90\n",
      "Average loss at step 100 : 0.723673632145\n",
      "Average loss at step 200 : 0.676061928868\n",
      "Average loss at step 300 : 0.686926624775\n",
      "Average loss at step 400 : 0.693415091634\n",
      "Epoch training loss : 0.695019319355\n",
      "\n",
      "EPOCH 91\n",
      "Average loss at step 100 : 0.696387659311\n",
      "Average loss at step 200 : 0.679050374627\n",
      "Average loss at step 300 : 0.6966226089\n",
      "Average loss at step 400 : 0.699662593007\n",
      "Epoch training loss : 0.692930808961\n",
      "\n",
      "EPOCH 92\n",
      "Average loss at step 100 : 0.706223480701\n",
      "Average loss at step 200 : 0.683295874\n",
      "Average loss at step 300 : 0.698465826511\n",
      "Average loss at step 400 : 0.68437844336\n",
      "Epoch training loss : 0.693090906143\n",
      "\n",
      "EPOCH 93\n",
      "Average loss at step 100 : 0.696560006142\n",
      "Average loss at step 200 : 0.678212566376\n",
      "Average loss at step 300 : 0.695715110302\n",
      "Average loss at step 400 : 0.692169685364\n",
      "Epoch training loss : 0.690664342046\n",
      "\n",
      "EPOCH 94\n",
      "Average loss at step 100 : 0.67730275929\n",
      "Average loss at step 200 : 0.676915997863\n",
      "Average loss at step 300 : 0.673157776594\n",
      "Average loss at step 400 : 0.676955914497\n",
      "Epoch training loss : 0.676083112061\n",
      "\n",
      "EPOCH 95\n",
      "Average loss at step 100 : 0.699883652925\n",
      "Average loss at step 200 : 0.708858197927\n",
      "Average loss at step 300 : 0.696577749848\n",
      "Average loss at step 400 : 0.694656671286\n",
      "Epoch training loss : 0.699994067997\n",
      "\n",
      "EPOCH 96\n",
      "Average loss at step 100 : 0.706180513501\n",
      "Average loss at step 200 : 0.686636992693\n",
      "Average loss at step 300 : 0.683755971789\n",
      "Average loss at step 400 : 0.697293720245\n",
      "Epoch training loss : 0.693466799557\n",
      "\n",
      "EPOCH 97\n",
      "Average loss at step 100 : 0.700760737062\n",
      "Average loss at step 200 : 0.70595741868\n",
      "Average loss at step 300 : 0.675246121287\n",
      "Average loss at step 400 : 0.673464416265\n",
      "Epoch training loss : 0.688857173324\n",
      "\n",
      "EPOCH 98\n",
      "Average loss at step 100 : 0.694097749591\n",
      "Average loss at step 200 : 0.689406378865\n",
      "Average loss at step 300 : 0.696691421866\n",
      "Average loss at step 400 : 0.68681918323\n",
      "Epoch training loss : 0.691753683388\n",
      "\n",
      "EPOCH 99\n",
      "Average loss at step 100 : 0.68645870924\n",
      "Average loss at step 200 : 0.692897741795\n",
      "Average loss at step 300 : 0.699808364511\n",
      "Average loss at step 400 : 0.672242027521\n",
      "Epoch training loss : 0.687851710767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f96545a3da0>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f96b40a5470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_losses = train_network(100,num_steps, state_size)\n",
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
