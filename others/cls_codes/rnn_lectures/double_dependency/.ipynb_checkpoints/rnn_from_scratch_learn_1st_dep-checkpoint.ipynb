{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected cross entropy loss if the model:\n",
      "- learns neither dependency: 0.661563238158\n",
      "- learns first dependency:   0.519166699707\n",
      "- learns both dependencies:  0.454454367449\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Expected cross entropy loss if the model:\")\n",
    "print(\"- learns neither dependency:\", -(0.625 * np.log(0.625) +\n",
    "                                      0.375 * np.log(0.375)))\n",
    "# Learns first dependency only ==> 0.51916669970720941\n",
    "print(\"- learns first dependency:  \",\n",
    "      -0.5 * (0.875 * np.log(0.875) + 0.125 * np.log(0.125))\n",
    "      -0.5 * (0.625 * np.log(0.625) + 0.375 * np.log(0.375)))\n",
    "print(\"- learns both dependencies: \", -0.50 * (0.75 * np.log(0.75) + 0.25 * np.log(0.25))\n",
    "      - 0.25 * (2 * 0.50 * np.log (0.50)) - 0.25 * (0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global config variables\n",
    "num_steps = 5 # number of truncated backprop steps ('n' in the discussion above)\n",
    "batch_size = 200\n",
    "num_classes = 2\n",
    "state_size = 4\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(size=1000000):\n",
    "    X = np.array(np.random.choice(2, size=(size,)))\n",
    "    Y = []\n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if X[i-3] == 1:\n",
    "            threshold += 0.5\n",
    "        if X[i-8] == 1:\n",
    "            threshold -= 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X, np.array(Y)\n",
    "\n",
    "# adapted from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py\n",
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    raw_x, raw_y = raw_data\n",
    "    data_length = len(raw_x)\n",
    "\n",
    "    # partition raw data into batches and stack them vertically in a data matrix\n",
    "    batch_partition_length = data_length // batch_size\n",
    "    data_x = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = raw_x[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "        data_y[i] = raw_y[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "    # further divide batch partitions into num_steps for truncated backprop\n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, i * num_steps:(i + 1) * num_steps]\n",
    "        y = data_y[:, i * num_steps:(i + 1) * num_steps]\n",
    "        yield (x, y)\n",
    "\n",
    "def gen_epochs(n, num_steps):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\"\"\"\n",
    "Placeholders\n",
    "\"\"\"\n",
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "\"\"\"\n",
    "RNN Inputs\n",
    "\"\"\"\n",
    "\n",
    "# Turn our x placeholder into a list of one-hot tensors:\n",
    "# rnn_inputs is a list of num_steps tensors with shape [batch_size, num_classes]\n",
    "x_one_hot = tf.one_hot(x, num_classes)\n",
    "rnn_inputs = tf.unstack(x_one_hot, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Definition of rnn_cell\n",
    "\n",
    "This is very similar to the __call__ method on Tensorflow's BasicRNNCell.\n",
    "\"\"\"\n",
    "with tf.variable_scope('rnn_cell'):\n",
    "    W_h = tf.get_variable('W_h', [num_classes + state_size, state_size]) # Concatenating W_hh and W_xh for efficiency\n",
    "    b_h = tf.get_variable('b_h', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "\n",
    "def rnn_cell(rnn_input, state):\n",
    "    with tf.variable_scope('rnn_cell', reuse=True):\n",
    "        W_h = tf.get_variable('W_h', [num_classes + state_size, state_size])\n",
    "        b_h = tf.get_variable('b_h', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "    return tf.tanh(tf.matmul(tf.concat([rnn_input, state], axis=1), W_h) + b_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adding rnn_cells to graph\n",
    "\n",
    "This is a simplified version of the \"static_rnn\" function from Tensorflow's api. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn.py#L41\n",
    "Note: In practice, using \"dynamic_rnn\" is a better choice that the \"static_rnn\":\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py#L390\n",
    "\"\"\"\n",
    "state = init_state\n",
    "rnn_outputs = []\n",
    "for rnn_input in rnn_inputs:\n",
    "    state = rnn_cell(rnn_input, state)\n",
    "    rnn_outputs.append(state)\n",
    "final_state = rnn_outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predictions, loss, training step\n",
    "\n",
    "Losses is similar to the \"sequence_loss\"\n",
    "function from Tensorflow's API, except that here we are using a list of 2D tensors, instead of a 3D tensor. See:\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/loss.py#L30\n",
    "\"\"\"\n",
    "\n",
    "#logits and predictions\n",
    "with tf.variable_scope('softmax'):\n",
    "    W_y = tf.get_variable('W_y', [state_size, num_classes])\n",
    "    b_y = tf.get_variable('b_y', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    \n",
    "logits = [tf.matmul(rnn_output, W_y) + b_y for rnn_output in rnn_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "# Turn our y placeholder into a list of labels\n",
    "y_as_list = tf.unstack(y, num=num_steps, axis=1)\n",
    "\n",
    "#losses and train_step\n",
    "losses = []\n",
    "for logit, label in zip(logits, y_as_list):\n",
    "    losses.append(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logit))\n",
    "\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train the network\n",
    "\"\"\"\n",
    "\n",
    "def train_network(num_epochs, num_steps, state_size=4):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        epoch_training_losses = np.zeros(num_epochs)\n",
    "        \n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):\n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            training_losses = []\n",
    "            \n",
    "            print(\"\\nEPOCH\", idx)\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                tr_losses, training_loss_, training_state, _ = \\\n",
    "                    sess.run([losses,\n",
    "                              total_loss,\n",
    "                              final_state,\n",
    "                              optimizer],\n",
    "                                  feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                training_loss += training_loss_\n",
    "                \n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    print(\"Average loss at step\", step,\n",
    "                              \":\", training_loss/100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                    training_loss = 0\n",
    "                    \n",
    "            \n",
    "            epoch_training_losses[idx] = np.mean(np.array(training_losses))\n",
    "            print(\"Epoch training loss :\", epoch_training_losses[idx])\n",
    "\n",
    "    return epoch_training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 0\n",
      "Average loss at step 100 : 0.564522241354\n",
      "Average loss at step 200 : 0.522128885388\n",
      "Average loss at step 300 : 0.521052979827\n",
      "Average loss at step 400 : 0.519054949284\n",
      "Average loss at step 500 : 0.524261210263\n",
      "Average loss at step 600 : 0.51894816488\n",
      "Average loss at step 700 : 0.518813562989\n",
      "Average loss at step 800 : 0.519155274332\n",
      "Average loss at step 900 : 0.519530082047\n",
      "Epoch training loss : 0.52527415004\n",
      "\n",
      "EPOCH 1\n",
      "Average loss at step 100 : 0.525667688847\n",
      "Average loss at step 200 : 0.520542412698\n",
      "Average loss at step 300 : 0.517638013363\n",
      "Average loss at step 400 : 0.518034579158\n",
      "Average loss at step 500 : 0.5166153422\n",
      "Average loss at step 600 : 0.521813015342\n",
      "Average loss at step 700 : 0.519313836694\n",
      "Average loss at step 800 : 0.518611475527\n",
      "Average loss at step 900 : 0.519285955131\n",
      "Epoch training loss : 0.519724702107\n",
      "\n",
      "EPOCH 2\n",
      "Average loss at step 100 : 0.527044809163\n",
      "Average loss at step 200 : 0.519960045516\n",
      "Average loss at step 300 : 0.518204390407\n",
      "Average loss at step 400 : 0.521639157534\n",
      "Average loss at step 500 : 0.521827076674\n",
      "Average loss at step 600 : 0.519188524783\n",
      "Average loss at step 700 : 0.517171972692\n",
      "Average loss at step 800 : 0.52054574579\n",
      "Average loss at step 900 : 0.51833817035\n",
      "Epoch training loss : 0.520435543656\n",
      "\n",
      "EPOCH 3\n",
      "Average loss at step 100 : 0.525478279889\n",
      "Average loss at step 200 : 0.520260674357\n",
      "Average loss at step 300 : 0.520671088099\n",
      "Average loss at step 400 : 0.520285678506\n",
      "Average loss at step 500 : 0.521668798923\n",
      "Average loss at step 600 : 0.520861317217\n",
      "Average loss at step 700 : 0.51862808466\n",
      "Average loss at step 800 : 0.520761320889\n",
      "Average loss at step 900 : 0.516937306225\n",
      "Epoch training loss : 0.520616949863\n",
      "\n",
      "EPOCH 4\n",
      "Average loss at step 100 : 0.527942728996\n",
      "Average loss at step 200 : 0.519314409196\n",
      "Average loss at step 300 : 0.519132711291\n",
      "Average loss at step 400 : 0.517251646519\n",
      "Average loss at step 500 : 0.517552438378\n",
      "Average loss at step 600 : 0.519958198965\n",
      "Average loss at step 700 : 0.520098231733\n",
      "Average loss at step 800 : 0.519897997677\n",
      "Average loss at step 900 : 0.519784513414\n",
      "Epoch training loss : 0.520103652908\n",
      "\n",
      "EPOCH 5\n",
      "Average loss at step 100 : 0.524503306746\n",
      "Average loss at step 200 : 0.515983577669\n",
      "Average loss at step 300 : 0.517245913744\n",
      "Average loss at step 400 : 0.516805140376\n",
      "Average loss at step 500 : 0.516991792917\n",
      "Average loss at step 600 : 0.518913524151\n",
      "Average loss at step 700 : 0.515298800766\n",
      "Average loss at step 800 : 0.518993686438\n",
      "Average loss at step 900 : 0.517555378675\n",
      "Epoch training loss : 0.518032346831\n",
      "\n",
      "EPOCH 6\n",
      "Average loss at step 100 : 0.523102561235\n",
      "Average loss at step 200 : 0.51645873636\n",
      "Average loss at step 300 : 0.519063258171\n",
      "Average loss at step 400 : 0.519014376402\n",
      "Average loss at step 500 : 0.516513575315\n",
      "Average loss at step 600 : 0.5142616117\n",
      "Average loss at step 700 : 0.516056173444\n",
      "Average loss at step 800 : 0.513838858008\n",
      "Average loss at step 900 : 0.513867350817\n",
      "Epoch training loss : 0.516908500161\n",
      "\n",
      "EPOCH 7\n",
      "Average loss at step 100 : 0.520509572327\n",
      "Average loss at step 200 : 0.518052588999\n",
      "Average loss at step 300 : 0.514292203784\n",
      "Average loss at step 400 : 0.516189002693\n",
      "Average loss at step 500 : 0.515003504157\n",
      "Average loss at step 600 : 0.5153563869\n",
      "Average loss at step 700 : 0.512401627302\n",
      "Average loss at step 800 : 0.514513521492\n",
      "Average loss at step 900 : 0.51183495909\n",
      "Epoch training loss : 0.515350374083\n",
      "\n",
      "EPOCH 8\n",
      "Average loss at step 100 : 0.520232147276\n",
      "Average loss at step 200 : 0.511524404585\n",
      "Average loss at step 300 : 0.513259546757\n",
      "Average loss at step 400 : 0.513055780828\n",
      "Average loss at step 500 : 0.51255358845\n",
      "Average loss at step 600 : 0.5118456918\n",
      "Average loss at step 700 : 0.513431688249\n",
      "Average loss at step 800 : 0.51036547035\n",
      "Average loss at step 900 : 0.513288131356\n",
      "Epoch training loss : 0.513284049961\n",
      "\n",
      "EPOCH 9\n",
      "Average loss at step 100 : 0.517694872022\n",
      "Average loss at step 200 : 0.512728512585\n",
      "Average loss at step 300 : 0.508926230669\n",
      "Average loss at step 400 : 0.511777985692\n",
      "Average loss at step 500 : 0.510686650276\n",
      "Average loss at step 600 : 0.512489845455\n",
      "Average loss at step 700 : 0.510303486586\n",
      "Average loss at step 800 : 0.512326058149\n",
      "Average loss at step 900 : 0.511004029512\n",
      "Epoch training loss : 0.51199307455\n",
      "\n",
      "EPOCH 10\n",
      "Average loss at step 100 : 0.51922285825\n",
      "Average loss at step 200 : 0.511871069968\n",
      "Average loss at step 300 : 0.511450285614\n",
      "Average loss at step 400 : 0.512044116557\n",
      "Average loss at step 500 : 0.515204639435\n",
      "Average loss at step 600 : 0.512458387315\n",
      "Average loss at step 700 : 0.507160916924\n",
      "Average loss at step 800 : 0.511738478839\n",
      "Average loss at step 900 : 0.509724035561\n",
      "Epoch training loss : 0.51231942094\n",
      "\n",
      "EPOCH 11\n",
      "Average loss at step 100 : 0.516473922133\n",
      "Average loss at step 200 : 0.513739849627\n",
      "Average loss at step 300 : 0.51025660485\n",
      "Average loss at step 400 : 0.511806611121\n",
      "Average loss at step 500 : 0.513314899206\n",
      "Average loss at step 600 : 0.509595628977\n",
      "Average loss at step 700 : 0.512060209811\n",
      "Average loss at step 800 : 0.512409508526\n",
      "Average loss at step 900 : 0.509982468784\n",
      "Epoch training loss : 0.512182189226\n",
      "\n",
      "EPOCH 12\n",
      "Average loss at step 100 : 0.517426001728\n",
      "Average loss at step 200 : 0.511283483207\n",
      "Average loss at step 300 : 0.510050480366\n",
      "Average loss at step 400 : 0.511758827865\n",
      "Average loss at step 500 : 0.508813533187\n",
      "Average loss at step 600 : 0.508298692703\n",
      "Average loss at step 700 : 0.511343507767\n",
      "Average loss at step 800 : 0.51246945858\n",
      "Average loss at step 900 : 0.513156092167\n",
      "Epoch training loss : 0.511622230841\n",
      "\n",
      "EPOCH 13\n",
      "Average loss at step 100 : 0.517667787373\n",
      "Average loss at step 200 : 0.513250178397\n",
      "Average loss at step 300 : 0.511284368634\n",
      "Average loss at step 400 : 0.512252691388\n",
      "Average loss at step 500 : 0.51060323298\n",
      "Average loss at step 600 : 0.510268907547\n",
      "Average loss at step 700 : 0.512388600707\n",
      "Average loss at step 800 : 0.510272049308\n",
      "Average loss at step 900 : 0.510842660964\n",
      "Epoch training loss : 0.512092275255\n",
      "\n",
      "EPOCH 14\n",
      "Average loss at step 100 : 0.51811974436\n",
      "Average loss at step 200 : 0.510071222782\n",
      "Average loss at step 300 : 0.508639541566\n",
      "Average loss at step 400 : 0.509719623625\n",
      "Average loss at step 500 : 0.514224373698\n",
      "Average loss at step 600 : 0.510897071064\n",
      "Average loss at step 700 : 0.514004278183\n",
      "Average loss at step 800 : 0.510829442143\n",
      "Average loss at step 900 : 0.515701851845\n",
      "Epoch training loss : 0.51246746103\n",
      "\n",
      "EPOCH 15\n",
      "Average loss at step 100 : 0.516180214286\n",
      "Average loss at step 200 : 0.510665135384\n",
      "Average loss at step 300 : 0.509085010886\n",
      "Average loss at step 400 : 0.512423706353\n",
      "Average loss at step 500 : 0.512359808981\n",
      "Average loss at step 600 : 0.512787927389\n",
      "Average loss at step 700 : 0.512085521519\n",
      "Average loss at step 800 : 0.512512174845\n",
      "Average loss at step 900 : 0.509873661399\n",
      "Epoch training loss : 0.511997017894\n",
      "\n",
      "EPOCH 16\n",
      "Average loss at step 100 : 0.519987951219\n",
      "Average loss at step 200 : 0.513375096619\n",
      "Average loss at step 300 : 0.510638177097\n",
      "Average loss at step 400 : 0.509911300242\n",
      "Average loss at step 500 : 0.510080385208\n",
      "Average loss at step 600 : 0.509930076599\n",
      "Average loss at step 700 : 0.513516095281\n",
      "Average loss at step 800 : 0.510412051976\n",
      "Average loss at step 900 : 0.512549197674\n",
      "Epoch training loss : 0.512266703546\n",
      "\n",
      "EPOCH 17\n",
      "Average loss at step 100 : 0.517993040085\n",
      "Average loss at step 200 : 0.509441534281\n",
      "Average loss at step 300 : 0.509864393175\n",
      "Average loss at step 400 : 0.510228178203\n",
      "Average loss at step 500 : 0.511279542148\n",
      "Average loss at step 600 : 0.510688329637\n",
      "Average loss at step 700 : 0.511566413939\n",
      "Average loss at step 800 : 0.510756671131\n",
      "Average loss at step 900 : 0.511855677366\n",
      "Epoch training loss : 0.511519308885\n",
      "\n",
      "EPOCH 18\n",
      "Average loss at step 100 : 0.515778265595\n",
      "Average loss at step 200 : 0.511424528658\n",
      "Average loss at step 300 : 0.510945354402\n",
      "Average loss at step 400 : 0.509794804156\n",
      "Average loss at step 500 : 0.510355147421\n",
      "Average loss at step 600 : 0.513888936639\n",
      "Average loss at step 700 : 0.507590413392\n",
      "Average loss at step 800 : 0.510053316951\n",
      "Average loss at step 900 : 0.509984558821\n",
      "Epoch training loss : 0.511090591782\n",
      "\n",
      "EPOCH 19\n",
      "Average loss at step 100 : 0.514676312208\n",
      "Average loss at step 200 : 0.511362247467\n",
      "Average loss at step 300 : 0.508529134989\n",
      "Average loss at step 400 : 0.51060566783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 500 : 0.510507645905\n",
      "Average loss at step 600 : 0.510154648423\n",
      "Average loss at step 700 : 0.508282347023\n",
      "Average loss at step 800 : 0.51194645524\n",
      "Average loss at step 900 : 0.51036239624\n",
      "Epoch training loss : 0.510714095036\n",
      "\n",
      "EPOCH 20\n",
      "Average loss at step 100 : 0.517221550941\n",
      "Average loss at step 200 : 0.512515731156\n",
      "Average loss at step 300 : 0.513666501939\n",
      "Average loss at step 400 : 0.511531083584\n",
      "Average loss at step 500 : 0.510558236837\n",
      "Average loss at step 600 : 0.51179567039\n",
      "Average loss at step 700 : 0.510507135689\n",
      "Average loss at step 800 : 0.508725916445\n",
      "Average loss at step 900 : 0.511704852879\n",
      "Epoch training loss : 0.512025186651\n",
      "\n",
      "EPOCH 21\n",
      "Average loss at step 100 : 0.516947077513\n",
      "Average loss at step 200 : 0.511546642482\n",
      "Average loss at step 300 : 0.510426007807\n",
      "Average loss at step 400 : 0.50824038744\n",
      "Average loss at step 500 : 0.511229123473\n",
      "Average loss at step 600 : 0.513281036317\n",
      "Average loss at step 700 : 0.510568721294\n",
      "Average loss at step 800 : 0.50848410517\n",
      "Average loss at step 900 : 0.509392044842\n",
      "Epoch training loss : 0.511123905149\n",
      "\n",
      "EPOCH 22\n",
      "Average loss at step 100 : 0.51812639147\n",
      "Average loss at step 200 : 0.512336309254\n",
      "Average loss at step 300 : 0.511122897267\n",
      "Average loss at step 400 : 0.512839327753\n",
      "Average loss at step 500 : 0.508454532623\n",
      "Average loss at step 600 : 0.509206396043\n",
      "Average loss at step 700 : 0.510499384105\n",
      "Average loss at step 800 : 0.512340075374\n",
      "Average loss at step 900 : 0.512115714252\n",
      "Epoch training loss : 0.511893447571\n",
      "\n",
      "EPOCH 23\n",
      "Average loss at step 100 : 0.516034261584\n",
      "Average loss at step 200 : 0.509910154939\n",
      "Average loss at step 300 : 0.512422433197\n",
      "Average loss at step 400 : 0.511546136737\n",
      "Average loss at step 500 : 0.511851964295\n",
      "Average loss at step 600 : 0.510467458367\n",
      "Average loss at step 700 : 0.512147581577\n",
      "Average loss at step 800 : 0.512699413598\n",
      "Average loss at step 900 : 0.510690483451\n",
      "Epoch training loss : 0.511974431972\n",
      "\n",
      "EPOCH 24\n",
      "Average loss at step 100 : 0.520911875963\n",
      "Average loss at step 200 : 0.512059339881\n",
      "Average loss at step 300 : 0.509518149793\n",
      "Average loss at step 400 : 0.509338794351\n",
      "Average loss at step 500 : 0.509860852063\n",
      "Average loss at step 600 : 0.508246727586\n",
      "Average loss at step 700 : 0.513335392475\n",
      "Average loss at step 800 : 0.510963578224\n",
      "Average loss at step 900 : 0.515104790926\n",
      "Epoch training loss : 0.512148833474\n",
      "\n",
      "EPOCH 25\n",
      "Average loss at step 100 : 0.516680056453\n",
      "Average loss at step 200 : 0.513201241791\n",
      "Average loss at step 300 : 0.51295001477\n",
      "Average loss at step 400 : 0.512995505035\n",
      "Average loss at step 500 : 0.511461672187\n",
      "Average loss at step 600 : 0.508500680327\n",
      "Average loss at step 700 : 0.510780283213\n",
      "Average loss at step 800 : 0.508215547502\n",
      "Average loss at step 900 : 0.512822491825\n",
      "Epoch training loss : 0.511956388123\n",
      "\n",
      "EPOCH 26\n",
      "Average loss at step 100 : 0.517747949958\n",
      "Average loss at step 200 : 0.509656751454\n",
      "Average loss at step 300 : 0.509550316334\n",
      "Average loss at step 400 : 0.512259792387\n",
      "Average loss at step 500 : 0.513210260868\n",
      "Average loss at step 600 : 0.510136941075\n",
      "Average loss at step 700 : 0.508745102584\n",
      "Average loss at step 800 : 0.513468509316\n",
      "Average loss at step 900 : 0.512090751529\n",
      "Epoch training loss : 0.511874041723\n",
      "\n",
      "EPOCH 27\n",
      "Average loss at step 100 : 0.517112695277\n",
      "Average loss at step 200 : 0.51087949276\n",
      "Average loss at step 300 : 0.511619485021\n",
      "Average loss at step 400 : 0.512701537609\n",
      "Average loss at step 500 : 0.512201060057\n",
      "Average loss at step 600 : 0.512614997923\n",
      "Average loss at step 700 : 0.510932222009\n",
      "Average loss at step 800 : 0.511365656257\n",
      "Average loss at step 900 : 0.513559873998\n",
      "Epoch training loss : 0.512554113434\n",
      "\n",
      "EPOCH 28\n",
      "Average loss at step 100 : 0.519566187263\n",
      "Average loss at step 200 : 0.509894428253\n",
      "Average loss at step 300 : 0.509949019551\n",
      "Average loss at step 400 : 0.514246818125\n",
      "Average loss at step 500 : 0.513376024961\n",
      "Average loss at step 600 : 0.51230674535\n",
      "Average loss at step 700 : 0.51264426142\n",
      "Average loss at step 800 : 0.509797007143\n",
      "Average loss at step 900 : 0.513170021474\n",
      "Epoch training loss : 0.512772279282\n",
      "\n",
      "EPOCH 29\n",
      "Average loss at step 100 : 0.516637709141\n",
      "Average loss at step 200 : 0.512759239972\n",
      "Average loss at step 300 : 0.511092510223\n",
      "Average loss at step 400 : 0.512984226942\n",
      "Average loss at step 500 : 0.510304685533\n",
      "Average loss at step 600 : 0.511240186989\n",
      "Average loss at step 700 : 0.508307150602\n",
      "Average loss at step 800 : 0.509416590631\n",
      "Average loss at step 900 : 0.509565955698\n",
      "Epoch training loss : 0.51136758397\n",
      "\n",
      "EPOCH 30\n",
      "Average loss at step 100 : 0.518871464431\n",
      "Average loss at step 200 : 0.511300440431\n",
      "Average loss at step 300 : 0.511339703202\n",
      "Average loss at step 400 : 0.511150230467\n",
      "Average loss at step 500 : 0.510371946692\n",
      "Average loss at step 600 : 0.513933800161\n",
      "Average loss at step 700 : 0.509719300866\n",
      "Average loss at step 800 : 0.509832804501\n",
      "Average loss at step 900 : 0.512181174457\n",
      "Epoch training loss : 0.512077873912\n",
      "\n",
      "EPOCH 31\n",
      "Average loss at step 100 : 0.517032176256\n",
      "Average loss at step 200 : 0.512861661911\n",
      "Average loss at step 300 : 0.512508157194\n",
      "Average loss at step 400 : 0.510537979007\n",
      "Average loss at step 500 : 0.513003044128\n",
      "Average loss at step 600 : 0.515412203074\n",
      "Average loss at step 700 : 0.514070982039\n",
      "Average loss at step 800 : 0.512487294376\n",
      "Average loss at step 900 : 0.513126969934\n",
      "Epoch training loss : 0.51344894088\n",
      "\n",
      "EPOCH 32\n",
      "Average loss at step 100 : 0.516943940818\n",
      "Average loss at step 200 : 0.51275472492\n",
      "Average loss at step 300 : 0.511885651946\n",
      "Average loss at step 400 : 0.512147719264\n",
      "Average loss at step 500 : 0.510927631855\n",
      "Average loss at step 600 : 0.51328588903\n",
      "Average loss at step 700 : 0.511436548233\n",
      "Average loss at step 800 : 0.511488485634\n",
      "Average loss at step 900 : 0.512106380463\n",
      "Epoch training loss : 0.512552996907\n",
      "\n",
      "EPOCH 33\n",
      "Average loss at step 100 : 0.51562787652\n",
      "Average loss at step 200 : 0.514965194464\n",
      "Average loss at step 300 : 0.512786916792\n",
      "Average loss at step 400 : 0.51163338691\n",
      "Average loss at step 500 : 0.510501566231\n",
      "Average loss at step 600 : 0.509001451731\n",
      "Average loss at step 700 : 0.510558714867\n",
      "Average loss at step 800 : 0.512638668716\n",
      "Average loss at step 900 : 0.512635251284\n",
      "Epoch training loss : 0.512261003057\n",
      "\n",
      "EPOCH 34\n",
      "Average loss at step 100 : 0.520059072375\n",
      "Average loss at step 200 : 0.510012821853\n",
      "Average loss at step 300 : 0.510871324241\n",
      "Average loss at step 400 : 0.510678734183\n",
      "Average loss at step 500 : 0.514458610415\n",
      "Average loss at step 600 : 0.50960731566\n",
      "Average loss at step 700 : 0.511958377957\n",
      "Average loss at step 800 : 0.508929969072\n",
      "Average loss at step 900 : 0.512331941426\n",
      "Epoch training loss : 0.512100907465\n",
      "\n",
      "EPOCH 35\n",
      "Average loss at step 100 : 0.518382565379\n",
      "Average loss at step 200 : 0.509435179532\n",
      "Average loss at step 300 : 0.510474703908\n",
      "Average loss at step 400 : 0.511825909913\n",
      "Average loss at step 500 : 0.51195633322\n",
      "Average loss at step 600 : 0.513259015381\n",
      "Average loss at step 700 : 0.51085426122\n",
      "Average loss at step 800 : 0.511919434965\n",
      "Average loss at step 900 : 0.513210873604\n",
      "Epoch training loss : 0.512368697458\n",
      "\n",
      "EPOCH 36\n",
      "Average loss at step 100 : 0.51885936588\n",
      "Average loss at step 200 : 0.509360143542\n",
      "Average loss at step 300 : 0.511729950011\n",
      "Average loss at step 400 : 0.511482262313\n",
      "Average loss at step 500 : 0.511520316005\n",
      "Average loss at step 600 : 0.51112409085\n",
      "Average loss at step 700 : 0.51534170866\n",
      "Average loss at step 800 : 0.510550068021\n",
      "Average loss at step 900 : 0.513425663114\n",
      "Epoch training loss : 0.512599285377\n",
      "\n",
      "EPOCH 37\n",
      "Average loss at step 100 : 0.518109036386\n",
      "Average loss at step 200 : 0.513345950842\n",
      "Average loss at step 300 : 0.512193855643\n",
      "Average loss at step 400 : 0.513689494729\n",
      "Average loss at step 500 : 0.510117560625\n",
      "Average loss at step 600 : 0.515467529893\n",
      "Average loss at step 700 : 0.511105687916\n",
      "Average loss at step 800 : 0.512368201613\n",
      "Average loss at step 900 : 0.511914793253\n",
      "Epoch training loss : 0.5131457901\n",
      "\n",
      "EPOCH 38\n",
      "Average loss at step 100 : 0.521062946022\n",
      "Average loss at step 200 : 0.51456458509\n",
      "Average loss at step 300 : 0.512921252847\n",
      "Average loss at step 400 : 0.513423654139\n",
      "Average loss at step 500 : 0.512406288683\n",
      "Average loss at step 600 : 0.51484734118\n",
      "Average loss at step 700 : 0.511574736834\n",
      "Average loss at step 800 : 0.513209777772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 900 : 0.513711080253\n",
      "Epoch training loss : 0.514191295869\n",
      "\n",
      "EPOCH 39\n",
      "Average loss at step 100 : 0.51779031992\n",
      "Average loss at step 200 : 0.512656520903\n",
      "Average loss at step 300 : 0.5103396523\n",
      "Average loss at step 400 : 0.511270091832\n",
      "Average loss at step 500 : 0.510736450255\n",
      "Average loss at step 600 : 0.51521083951\n",
      "Average loss at step 700 : 0.512123605609\n",
      "Average loss at step 800 : 0.511860006154\n",
      "Average loss at step 900 : 0.511661097407\n",
      "Epoch training loss : 0.512627620432\n",
      "\n",
      "EPOCH 40\n",
      "Average loss at step 100 : 0.517238877714\n",
      "Average loss at step 200 : 0.51126974076\n",
      "Average loss at step 300 : 0.51129624486\n",
      "Average loss at step 400 : 0.510609641075\n",
      "Average loss at step 500 : 0.511267052889\n",
      "Average loss at step 600 : 0.511721227467\n",
      "Average loss at step 700 : 0.513513585627\n",
      "Average loss at step 800 : 0.512820909619\n",
      "Average loss at step 900 : 0.513134363294\n",
      "Epoch training loss : 0.512541293701\n",
      "\n",
      "EPOCH 41\n",
      "Average loss at step 100 : 0.518883278966\n",
      "Average loss at step 200 : 0.513857164979\n",
      "Average loss at step 300 : 0.512896104455\n",
      "Average loss at step 400 : 0.509252357781\n",
      "Average loss at step 500 : 0.51087159276\n",
      "Average loss at step 600 : 0.510058023632\n",
      "Average loss at step 700 : 0.512954064608\n",
      "Average loss at step 800 : 0.514181393385\n",
      "Average loss at step 900 : 0.514681562781\n",
      "Epoch training loss : 0.513070615927\n",
      "\n",
      "EPOCH 42\n",
      "Average loss at step 100 : 0.518191873431\n",
      "Average loss at step 200 : 0.512397415638\n",
      "Average loss at step 300 : 0.515068108141\n",
      "Average loss at step 400 : 0.510217466354\n",
      "Average loss at step 500 : 0.514138819277\n",
      "Average loss at step 600 : 0.513309488595\n",
      "Average loss at step 700 : 0.510396839976\n",
      "Average loss at step 800 : 0.511600859761\n",
      "Average loss at step 900 : 0.510281796753\n",
      "Epoch training loss : 0.512844740881\n",
      "\n",
      "EPOCH 43\n",
      "Average loss at step 100 : 0.518902732432\n",
      "Average loss at step 200 : 0.51128113091\n",
      "Average loss at step 300 : 0.514206855595\n",
      "Average loss at step 400 : 0.511127504706\n",
      "Average loss at step 500 : 0.513693782091\n",
      "Average loss at step 600 : 0.511896734238\n",
      "Average loss at step 700 : 0.514082051814\n",
      "Average loss at step 800 : 0.51216961205\n",
      "Average loss at step 900 : 0.512454083562\n",
      "Epoch training loss : 0.513312720822\n",
      "\n",
      "EPOCH 44\n",
      "Average loss at step 100 : 0.518841170073\n",
      "Average loss at step 200 : 0.511903305054\n",
      "Average loss at step 300 : 0.510700342357\n",
      "Average loss at step 400 : 0.511491456032\n",
      "Average loss at step 500 : 0.511942128837\n",
      "Average loss at step 600 : 0.511477211118\n",
      "Average loss at step 700 : 0.513032033443\n",
      "Average loss at step 800 : 0.51032546252\n",
      "Average loss at step 900 : 0.514759634733\n",
      "Epoch training loss : 0.512719193796\n",
      "\n",
      "EPOCH 45\n",
      "Average loss at step 100 : 0.519769431353\n",
      "Average loss at step 200 : 0.511618433893\n",
      "Average loss at step 300 : 0.510782873333\n",
      "Average loss at step 400 : 0.513775450289\n",
      "Average loss at step 500 : 0.512385244071\n",
      "Average loss at step 600 : 0.513006893396\n",
      "Average loss at step 700 : 0.512297816575\n",
      "Average loss at step 800 : 0.510493915975\n",
      "Average loss at step 900 : 0.514025562108\n",
      "Epoch training loss : 0.513128402332\n",
      "\n",
      "EPOCH 46\n",
      "Average loss at step 100 : 0.518049632311\n",
      "Average loss at step 200 : 0.513794201612\n",
      "Average loss at step 300 : 0.511324829459\n",
      "Average loss at step 400 : 0.510412469208\n",
      "Average loss at step 500 : 0.511820831597\n",
      "Average loss at step 600 : 0.513611052632\n",
      "Average loss at step 700 : 0.512320221663\n",
      "Average loss at step 800 : 0.510278164744\n",
      "Average loss at step 900 : 0.511838309765\n",
      "Epoch training loss : 0.512605523666\n",
      "\n",
      "EPOCH 47\n",
      "Average loss at step 100 : 0.519526729584\n",
      "Average loss at step 200 : 0.510767923594\n",
      "Average loss at step 300 : 0.511506917477\n",
      "Average loss at step 400 : 0.512619228959\n",
      "Average loss at step 500 : 0.514682775736\n",
      "Average loss at step 600 : 0.512358615696\n",
      "Average loss at step 700 : 0.514081629217\n",
      "Average loss at step 800 : 0.512769666314\n",
      "Average loss at step 900 : 0.511455698609\n",
      "Epoch training loss : 0.513307687243\n",
      "\n",
      "EPOCH 48\n",
      "Average loss at step 100 : 0.518491001427\n",
      "Average loss at step 200 : 0.514165114462\n",
      "Average loss at step 300 : 0.512342517972\n",
      "Average loss at step 400 : 0.511569221616\n",
      "Average loss at step 500 : 0.514611717165\n",
      "Average loss at step 600 : 0.514019067287\n",
      "Average loss at step 700 : 0.511782078743\n",
      "Average loss at step 800 : 0.513554027379\n",
      "Average loss at step 900 : 0.512459741831\n",
      "Epoch training loss : 0.513666054209\n",
      "\n",
      "EPOCH 49\n",
      "Average loss at step 100 : 0.520001384914\n",
      "Average loss at step 200 : 0.514054324925\n",
      "Average loss at step 300 : 0.510769995153\n",
      "Average loss at step 400 : 0.512087914944\n",
      "Average loss at step 500 : 0.511487739086\n",
      "Average loss at step 600 : 0.512161780298\n",
      "Average loss at step 700 : 0.511971278787\n",
      "Average loss at step 800 : 0.510807159245\n",
      "Average loss at step 900 : 0.510915651023\n",
      "Epoch training loss : 0.512695247597\n",
      "\n",
      "EPOCH 50\n",
      "Average loss at step 100 : 0.516990150809\n",
      "Average loss at step 200 : 0.511996906996\n",
      "Average loss at step 300 : 0.510833765864\n",
      "Average loss at step 400 : 0.51099578321\n",
      "Average loss at step 500 : 0.513597733676\n",
      "Average loss at step 600 : 0.511426962614\n",
      "Average loss at step 700 : 0.514113222659\n",
      "Average loss at step 800 : 0.512561635375\n",
      "Average loss at step 900 : 0.50952773124\n",
      "Epoch training loss : 0.512449321383\n",
      "\n",
      "EPOCH 51\n",
      "Average loss at step 100 : 0.519048475027\n",
      "Average loss at step 200 : 0.510817973316\n",
      "Average loss at step 300 : 0.514225206971\n",
      "Average loss at step 400 : 0.511596522629\n",
      "Average loss at step 500 : 0.509662235677\n",
      "Average loss at step 600 : 0.507918409109\n",
      "Average loss at step 700 : 0.515148243904\n",
      "Average loss at step 800 : 0.5122936064\n",
      "Average loss at step 900 : 0.512016822696\n",
      "Epoch training loss : 0.512525277303\n",
      "\n",
      "EPOCH 52\n",
      "Average loss at step 100 : 0.52022505939\n",
      "Average loss at step 200 : 0.513197508752\n",
      "Average loss at step 300 : 0.511934449077\n",
      "Average loss at step 400 : 0.513281489909\n",
      "Average loss at step 500 : 0.513094874024\n",
      "Average loss at step 600 : 0.512087901235\n",
      "Average loss at step 700 : 0.511946391165\n",
      "Average loss at step 800 : 0.509407668114\n",
      "Average loss at step 900 : 0.510139944851\n",
      "Epoch training loss : 0.512812809613\n",
      "\n",
      "EPOCH 53\n",
      "Average loss at step 100 : 0.519794082046\n",
      "Average loss at step 200 : 0.514440797567\n",
      "Average loss at step 300 : 0.51171587348\n",
      "Average loss at step 400 : 0.512482724488\n",
      "Average loss at step 500 : 0.511081072986\n",
      "Average loss at step 600 : 0.511094387174\n",
      "Average loss at step 700 : 0.51470415622\n",
      "Average loss at step 800 : 0.515628762245\n",
      "Average loss at step 900 : 0.511453882158\n",
      "Epoch training loss : 0.513599526485\n",
      "\n",
      "EPOCH 54\n",
      "Average loss at step 100 : 0.518760154545\n",
      "Average loss at step 200 : 0.513459255397\n",
      "Average loss at step 300 : 0.512127935588\n",
      "Average loss at step 400 : 0.509609058499\n",
      "Average loss at step 500 : 0.512931655347\n",
      "Average loss at step 600 : 0.512850066125\n",
      "Average loss at step 700 : 0.512711962461\n",
      "Average loss at step 800 : 0.513650933802\n",
      "Average loss at step 900 : 0.513138976097\n",
      "Epoch training loss : 0.513248888652\n",
      "\n",
      "EPOCH 55\n",
      "Average loss at step 100 : 0.517578263283\n",
      "Average loss at step 200 : 0.513504133224\n",
      "Average loss at step 300 : 0.509887835085\n",
      "Average loss at step 400 : 0.513709682822\n",
      "Average loss at step 500 : 0.512975302339\n",
      "Average loss at step 600 : 0.512396329939\n",
      "Average loss at step 700 : 0.513367525935\n",
      "Average loss at step 800 : 0.512090156078\n",
      "Average loss at step 900 : 0.514053800702\n",
      "Epoch training loss : 0.513284781045\n",
      "\n",
      "EPOCH 56\n",
      "Average loss at step 100 : 0.519134472013\n",
      "Average loss at step 200 : 0.510737693608\n",
      "Average loss at step 300 : 0.512774145007\n",
      "Average loss at step 400 : 0.513007592857\n",
      "Average loss at step 500 : 0.509994931817\n",
      "Average loss at step 600 : 0.513233445585\n",
      "Average loss at step 700 : 0.511359253228\n",
      "Average loss at step 800 : 0.514853748083\n",
      "Average loss at step 900 : 0.512101651728\n",
      "Epoch training loss : 0.513021881547\n",
      "\n",
      "EPOCH 57\n",
      "Average loss at step 100 : 0.519255358875\n",
      "Average loss at step 200 : 0.510708333254\n",
      "Average loss at step 300 : 0.513770404756\n",
      "Average loss at step 400 : 0.511392660141\n",
      "Average loss at step 500 : 0.511288463473\n",
      "Average loss at step 600 : 0.514149856269\n",
      "Average loss at step 700 : 0.513310805261\n",
      "Average loss at step 800 : 0.511003411412\n",
      "Average loss at step 900 : 0.513026790023\n",
      "Epoch training loss : 0.513100675941\n",
      "\n",
      "EPOCH 58\n",
      "Average loss at step 100 : 0.517517297864\n",
      "Average loss at step 200 : 0.512292710543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 300 : 0.51205047667\n",
      "Average loss at step 400 : 0.512842430472\n",
      "Average loss at step 500 : 0.512682429254\n",
      "Average loss at step 600 : 0.51524489224\n",
      "Average loss at step 700 : 0.511859258115\n",
      "Average loss at step 800 : 0.511816689968\n",
      "Average loss at step 900 : 0.511179418564\n",
      "Epoch training loss : 0.513053955966\n",
      "\n",
      "EPOCH 59\n",
      "Average loss at step 100 : 0.518711709678\n",
      "Average loss at step 200 : 0.511847759783\n",
      "Average loss at step 300 : 0.513409706056\n",
      "Average loss at step 400 : 0.514358550012\n",
      "Average loss at step 500 : 0.513840199709\n",
      "Average loss at step 600 : 0.510704450905\n",
      "Average loss at step 700 : 0.511211754382\n",
      "Average loss at step 800 : 0.511980148852\n",
      "Average loss at step 900 : 0.516191908419\n",
      "Epoch training loss : 0.513584020866\n",
      "\n",
      "EPOCH 60\n",
      "Average loss at step 100 : 0.517624588907\n",
      "Average loss at step 200 : 0.51372943759\n",
      "Average loss at step 300 : 0.514872067273\n",
      "Average loss at step 400 : 0.510443359911\n",
      "Average loss at step 500 : 0.513296953142\n",
      "Average loss at step 600 : 0.512775512338\n",
      "Average loss at step 700 : 0.513573814034\n",
      "Average loss at step 800 : 0.51109454751\n",
      "Average loss at step 900 : 0.511338272095\n",
      "Epoch training loss : 0.513194283644\n",
      "\n",
      "EPOCH 61\n",
      "Average loss at step 100 : 0.516166566312\n",
      "Average loss at step 200 : 0.510820790231\n",
      "Average loss at step 300 : 0.512718853951\n",
      "Average loss at step 400 : 0.510592828989\n",
      "Average loss at step 500 : 0.513481232524\n",
      "Average loss at step 600 : 0.512460938096\n",
      "Average loss at step 700 : 0.512357722819\n",
      "Average loss at step 800 : 0.513108749092\n",
      "Average loss at step 900 : 0.513809775114\n",
      "Epoch training loss : 0.512835273014\n",
      "\n",
      "EPOCH 62\n",
      "Average loss at step 100 : 0.515363660157\n",
      "Average loss at step 200 : 0.51392496109\n",
      "Average loss at step 300 : 0.512521550059\n",
      "Average loss at step 400 : 0.513089217842\n",
      "Average loss at step 500 : 0.511068841815\n",
      "Average loss at step 600 : 0.509494887888\n",
      "Average loss at step 700 : 0.513407939672\n",
      "Average loss at step 800 : 0.508978213072\n",
      "Average loss at step 900 : 0.51329344362\n",
      "Epoch training loss : 0.51234919058\n",
      "\n",
      "EPOCH 63\n",
      "Average loss at step 100 : 0.518000125289\n",
      "Average loss at step 200 : 0.510593030155\n",
      "Average loss at step 300 : 0.51142804563\n",
      "Average loss at step 400 : 0.512754870951\n",
      "Average loss at step 500 : 0.513803994954\n",
      "Average loss at step 600 : 0.5136814484\n",
      "Average loss at step 700 : 0.511594747007\n",
      "Average loss at step 800 : 0.513227802217\n",
      "Average loss at step 900 : 0.512646029294\n",
      "Epoch training loss : 0.513081121544\n",
      "\n",
      "EPOCH 64\n",
      "Average loss at step 100 : 0.519755674601\n",
      "Average loss at step 200 : 0.513286747038\n",
      "Average loss at step 300 : 0.509806091189\n",
      "Average loss at step 400 : 0.512440988719\n",
      "Average loss at step 500 : 0.513572576642\n",
      "Average loss at step 600 : 0.511136856973\n",
      "Average loss at step 700 : 0.511847323477\n",
      "Average loss at step 800 : 0.513400796354\n",
      "Average loss at step 900 : 0.513129449487\n",
      "Epoch training loss : 0.513152944942\n",
      "\n",
      "EPOCH 65\n",
      "Average loss at step 100 : 0.515143094659\n",
      "Average loss at step 200 : 0.510525102615\n",
      "Average loss at step 300 : 0.51276062727\n",
      "Average loss at step 400 : 0.514443027079\n",
      "Average loss at step 500 : 0.512312821746\n",
      "Average loss at step 600 : 0.515274569094\n",
      "Average loss at step 700 : 0.51478502214\n",
      "Average loss at step 800 : 0.512542030811\n",
      "Average loss at step 900 : 0.51437759012\n",
      "Epoch training loss : 0.513573765059\n",
      "\n",
      "EPOCH 66\n",
      "Average loss at step 100 : 0.518060319424\n",
      "Average loss at step 200 : 0.509919356108\n",
      "Average loss at step 300 : 0.511139785945\n",
      "Average loss at step 400 : 0.509543243349\n",
      "Average loss at step 500 : 0.513373405337\n",
      "Average loss at step 600 : 0.512269444764\n",
      "Average loss at step 700 : 0.513488559425\n",
      "Average loss at step 800 : 0.512400677204\n",
      "Average loss at step 900 : 0.512759386897\n",
      "Epoch training loss : 0.512550464272\n",
      "\n",
      "EPOCH 67\n",
      "Average loss at step 100 : 0.522005573511\n",
      "Average loss at step 200 : 0.514971210361\n",
      "Average loss at step 300 : 0.511857068837\n",
      "Average loss at step 400 : 0.513108528852\n",
      "Average loss at step 500 : 0.515000095069\n",
      "Average loss at step 600 : 0.514434035122\n",
      "Average loss at step 700 : 0.514683994353\n",
      "Average loss at step 800 : 0.51182161957\n",
      "Average loss at step 900 : 0.510380815864\n",
      "Epoch training loss : 0.514251437949\n",
      "\n",
      "EPOCH 68\n",
      "Average loss at step 100 : 0.51963822633\n",
      "Average loss at step 200 : 0.508850247264\n",
      "Average loss at step 300 : 0.513528055251\n",
      "Average loss at step 400 : 0.514837031066\n",
      "Average loss at step 500 : 0.509847186208\n",
      "Average loss at step 600 : 0.516158160269\n",
      "Average loss at step 700 : 0.5126078403\n",
      "Average loss at step 800 : 0.51243835777\n",
      "Average loss at step 900 : 0.514297371805\n",
      "Epoch training loss : 0.513578052918\n",
      "\n",
      "EPOCH 69\n",
      "Average loss at step 100 : 0.518633299768\n",
      "Average loss at step 200 : 0.513021021485\n",
      "Average loss at step 300 : 0.512140025198\n",
      "Average loss at step 400 : 0.511586092412\n",
      "Average loss at step 500 : 0.511818670332\n",
      "Average loss at step 600 : 0.511325755715\n",
      "Average loss at step 700 : 0.514048247933\n",
      "Average loss at step 800 : 0.512040573061\n",
      "Average loss at step 900 : 0.51050088346\n",
      "Epoch training loss : 0.512790507707\n",
      "\n",
      "EPOCH 70\n",
      "Average loss at step 100 : 0.519492006302\n",
      "Average loss at step 200 : 0.513223656416\n",
      "Average loss at step 300 : 0.511113856435\n",
      "Average loss at step 400 : 0.511138762832\n",
      "Average loss at step 500 : 0.514619803131\n",
      "Average loss at step 600 : 0.511751979291\n",
      "Average loss at step 700 : 0.512162283659\n",
      "Average loss at step 800 : 0.513409453332\n",
      "Average loss at step 900 : 0.514979720414\n",
      "Epoch training loss : 0.513543502423\n",
      "\n",
      "EPOCH 71\n",
      "Average loss at step 100 : 0.520913838148\n",
      "Average loss at step 200 : 0.513546090126\n",
      "Average loss at step 300 : 0.514240124226\n",
      "Average loss at step 400 : 0.509449047446\n",
      "Average loss at step 500 : 0.513260481954\n",
      "Average loss at step 600 : 0.513051300645\n",
      "Average loss at step 700 : 0.509676529467\n",
      "Average loss at step 800 : 0.511530915499\n",
      "Average loss at step 900 : 0.511583759785\n",
      "Epoch training loss : 0.513028009699\n",
      "\n",
      "EPOCH 72\n",
      "Average loss at step 100 : 0.51774353385\n",
      "Average loss at step 200 : 0.511560699344\n",
      "Average loss at step 300 : 0.513533068895\n",
      "Average loss at step 400 : 0.513093166053\n",
      "Average loss at step 500 : 0.516086581647\n",
      "Average loss at step 600 : 0.515002459884\n",
      "Average loss at step 700 : 0.512214964628\n",
      "Average loss at step 800 : 0.513042171896\n",
      "Average loss at step 900 : 0.511949401796\n",
      "Epoch training loss : 0.513802894221\n",
      "\n",
      "EPOCH 73\n",
      "Average loss at step 100 : 0.520026305914\n",
      "Average loss at step 200 : 0.511617313921\n",
      "Average loss at step 300 : 0.513635928035\n",
      "Average loss at step 400 : 0.512844416797\n",
      "Average loss at step 500 : 0.51219570756\n",
      "Average loss at step 600 : 0.511154617071\n",
      "Average loss at step 700 : 0.511358836591\n",
      "Average loss at step 800 : 0.512438105345\n",
      "Average loss at step 900 : 0.514500712454\n",
      "Epoch training loss : 0.513307993743\n",
      "\n",
      "EPOCH 74\n",
      "Average loss at step 100 : 0.519969099164\n",
      "Average loss at step 200 : 0.512972511351\n",
      "Average loss at step 300 : 0.514773136079\n",
      "Average loss at step 400 : 0.513962536156\n",
      "Average loss at step 500 : 0.514046389163\n",
      "Average loss at step 600 : 0.514405554533\n",
      "Average loss at step 700 : 0.512307300866\n",
      "Average loss at step 800 : 0.511977817714\n",
      "Average loss at step 900 : 0.512344563603\n",
      "Epoch training loss : 0.514084323181\n",
      "\n",
      "EPOCH 75\n",
      "Average loss at step 100 : 0.518156079352\n",
      "Average loss at step 200 : 0.51408116281\n",
      "Average loss at step 300 : 0.51395799309\n",
      "Average loss at step 400 : 0.512366672456\n",
      "Average loss at step 500 : 0.511528925598\n",
      "Average loss at step 600 : 0.513841162324\n",
      "Average loss at step 700 : 0.509615415633\n",
      "Average loss at step 800 : 0.508885475695\n",
      "Average loss at step 900 : 0.511893638074\n",
      "Epoch training loss : 0.512702947226\n",
      "\n",
      "EPOCH 76\n",
      "Average loss at step 100 : 0.520249549747\n",
      "Average loss at step 200 : 0.513089565933\n",
      "Average loss at step 300 : 0.511145534515\n",
      "Average loss at step 400 : 0.512377098799\n",
      "Average loss at step 500 : 0.513113997579\n",
      "Average loss at step 600 : 0.511401124299\n",
      "Average loss at step 700 : 0.51116286546\n",
      "Average loss at step 800 : 0.514028383195\n",
      "Average loss at step 900 : 0.512885407209\n",
      "Epoch training loss : 0.513272614082\n",
      "\n",
      "EPOCH 77\n",
      "Average loss at step 100 : 0.519801493585\n",
      "Average loss at step 200 : 0.512363761067\n",
      "Average loss at step 300 : 0.515404934883\n",
      "Average loss at step 400 : 0.508174323142\n",
      "Average loss at step 500 : 0.514348693788\n",
      "Average loss at step 600 : 0.513705126643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 700 : 0.515216234624\n",
      "Average loss at step 800 : 0.512113589644\n",
      "Average loss at step 900 : 0.512918122411\n",
      "Epoch training loss : 0.513782919976\n",
      "\n",
      "EPOCH 78\n",
      "Average loss at step 100 : 0.516185113788\n",
      "Average loss at step 200 : 0.511806026995\n",
      "Average loss at step 300 : 0.511828243136\n",
      "Average loss at step 400 : 0.514357588291\n",
      "Average loss at step 500 : 0.510803595185\n",
      "Average loss at step 600 : 0.512762102187\n",
      "Average loss at step 700 : 0.509780189693\n",
      "Average loss at step 800 : 0.511015897393\n",
      "Average loss at step 900 : 0.513387682736\n",
      "Epoch training loss : 0.512436271045\n",
      "\n",
      "EPOCH 79\n",
      "Average loss at step 100 : 0.520454930663\n",
      "Average loss at step 200 : 0.513454666138\n",
      "Average loss at step 300 : 0.512385668755\n",
      "Average loss at step 400 : 0.512715559006\n",
      "Average loss at step 500 : 0.513227551579\n",
      "Average loss at step 600 : 0.513875183463\n",
      "Average loss at step 700 : 0.516542913914\n",
      "Average loss at step 800 : 0.511171957552\n",
      "Average loss at step 900 : 0.513126699328\n",
      "Epoch training loss : 0.5141061256\n",
      "\n",
      "EPOCH 80\n",
      "Average loss at step 100 : 0.515899376273\n",
      "Average loss at step 200 : 0.514727346301\n",
      "Average loss at step 300 : 0.512900446951\n",
      "Average loss at step 400 : 0.511683945358\n",
      "Average loss at step 500 : 0.50863224864\n",
      "Average loss at step 600 : 0.512493913174\n",
      "Average loss at step 700 : 0.511694039702\n",
      "Average loss at step 800 : 0.51427297473\n",
      "Average loss at step 900 : 0.51259126395\n",
      "Epoch training loss : 0.512766172787\n",
      "\n",
      "EPOCH 81\n",
      "Average loss at step 100 : 0.517725375891\n",
      "Average loss at step 200 : 0.511063248813\n",
      "Average loss at step 300 : 0.511431350112\n",
      "Average loss at step 400 : 0.512743641138\n",
      "Average loss at step 500 : 0.512116749287\n",
      "Average loss at step 600 : 0.509968521893\n",
      "Average loss at step 700 : 0.513071845174\n",
      "Average loss at step 800 : 0.516168657541\n",
      "Average loss at step 900 : 0.511651716232\n",
      "Epoch training loss : 0.51288234512\n",
      "\n",
      "EPOCH 82\n",
      "Average loss at step 100 : 0.520039372444\n",
      "Average loss at step 200 : 0.512906065881\n",
      "Average loss at step 300 : 0.51223862946\n",
      "Average loss at step 400 : 0.514623995423\n",
      "Average loss at step 500 : 0.511252848506\n",
      "Average loss at step 600 : 0.512589865029\n",
      "Average loss at step 700 : 0.509889269769\n",
      "Average loss at step 800 : 0.5123914814\n",
      "Average loss at step 900 : 0.510725802183\n",
      "Epoch training loss : 0.512961925566\n",
      "\n",
      "EPOCH 83\n",
      "Average loss at step 100 : 0.519640199244\n",
      "Average loss at step 200 : 0.512665984631\n",
      "Average loss at step 300 : 0.511977246404\n",
      "Average loss at step 400 : 0.513504412174\n",
      "Average loss at step 500 : 0.515318217278\n",
      "Average loss at step 600 : 0.513697703779\n",
      "Average loss at step 700 : 0.511849823594\n",
      "Average loss at step 800 : 0.512452315688\n",
      "Average loss at step 900 : 0.513339603543\n",
      "Epoch training loss : 0.513827278482\n",
      "\n",
      "EPOCH 84\n",
      "Average loss at step 100 : 0.517533597052\n",
      "Average loss at step 200 : 0.511805678308\n",
      "Average loss at step 300 : 0.509679423869\n",
      "Average loss at step 400 : 0.51238298893\n",
      "Average loss at step 500 : 0.511562992632\n",
      "Average loss at step 600 : 0.513035257161\n",
      "Average loss at step 700 : 0.512033161521\n",
      "Average loss at step 800 : 0.512562823296\n",
      "Average loss at step 900 : 0.511338196397\n",
      "Epoch training loss : 0.512437124352\n",
      "\n",
      "EPOCH 85\n",
      "Average loss at step 100 : 0.518406175971\n",
      "Average loss at step 200 : 0.510821280181\n",
      "Average loss at step 300 : 0.512296244502\n",
      "Average loss at step 400 : 0.512223296165\n",
      "Average loss at step 500 : 0.510595503151\n",
      "Average loss at step 600 : 0.511667597592\n",
      "Average loss at step 700 : 0.51266251713\n",
      "Average loss at step 800 : 0.51170725435\n",
      "Average loss at step 900 : 0.511655739546\n",
      "Epoch training loss : 0.512448400954\n",
      "\n",
      "EPOCH 86\n",
      "Average loss at step 100 : 0.517035678923\n",
      "Average loss at step 200 : 0.511102651954\n",
      "Average loss at step 300 : 0.513951305151\n",
      "Average loss at step 400 : 0.513980083168\n",
      "Average loss at step 500 : 0.515394255221\n",
      "Average loss at step 600 : 0.512207443118\n",
      "Average loss at step 700 : 0.512371818125\n",
      "Average loss at step 800 : 0.510849208534\n",
      "Average loss at step 900 : 0.512796791494\n",
      "Epoch training loss : 0.513298803965\n",
      "\n",
      "EPOCH 87\n",
      "Average loss at step 100 : 0.518011446893\n",
      "Average loss at step 200 : 0.514449003041\n",
      "Average loss at step 300 : 0.511540068388\n",
      "Average loss at step 400 : 0.510402119458\n",
      "Average loss at step 500 : 0.514385071397\n",
      "Average loss at step 600 : 0.512332983911\n",
      "Average loss at step 700 : 0.513443438411\n",
      "Average loss at step 800 : 0.512479022741\n",
      "Average loss at step 900 : 0.51492813319\n",
      "Epoch training loss : 0.51355236527\n",
      "\n",
      "EPOCH 88\n",
      "Average loss at step 100 : 0.519221141934\n",
      "Average loss at step 200 : 0.512684494555\n",
      "Average loss at step 300 : 0.514933373332\n",
      "Average loss at step 400 : 0.510997650921\n",
      "Average loss at step 500 : 0.514437910914\n",
      "Average loss at step 600 : 0.512245603204\n",
      "Average loss at step 700 : 0.512022765279\n",
      "Average loss at step 800 : 0.511858740747\n",
      "Average loss at step 900 : 0.514771469533\n",
      "Epoch training loss : 0.513685905602\n",
      "\n",
      "EPOCH 89\n",
      "Average loss at step 100 : 0.517658594847\n",
      "Average loss at step 200 : 0.510396282375\n",
      "Average loss at step 300 : 0.511738905013\n",
      "Average loss at step 400 : 0.512609302402\n",
      "Average loss at step 500 : 0.512802746892\n",
      "Average loss at step 600 : 0.510367000401\n",
      "Average loss at step 700 : 0.512635838985\n",
      "Average loss at step 800 : 0.513957802355\n",
      "Average loss at step 900 : 0.509451878071\n",
      "Epoch training loss : 0.512402039038\n",
      "\n",
      "EPOCH 90\n",
      "Average loss at step 100 : 0.518323062658\n",
      "Average loss at step 200 : 0.512078512907\n",
      "Average loss at step 300 : 0.513503990173\n",
      "Average loss at step 400 : 0.512103813291\n",
      "Average loss at step 500 : 0.509494765699\n",
      "Average loss at step 600 : 0.512674297392\n",
      "Average loss at step 700 : 0.514273744226\n",
      "Average loss at step 800 : 0.511334871352\n",
      "Average loss at step 900 : 0.511294293106\n",
      "Epoch training loss : 0.512786816756\n",
      "\n",
      "EPOCH 91\n",
      "Average loss at step 100 : 0.516211788952\n",
      "Average loss at step 200 : 0.511587700546\n",
      "Average loss at step 300 : 0.511268969774\n",
      "Average loss at step 400 : 0.511862713397\n",
      "Average loss at step 500 : 0.514277191758\n",
      "Average loss at step 600 : 0.514001217186\n",
      "Average loss at step 700 : 0.513573028147\n",
      "Average loss at step 800 : 0.51472060591\n",
      "Average loss at step 900 : 0.511892870665\n",
      "Epoch training loss : 0.513266231815\n",
      "\n",
      "EPOCH 92\n",
      "Average loss at step 100 : 0.518506028354\n",
      "Average loss at step 200 : 0.511144652963\n",
      "Average loss at step 300 : 0.512821178138\n",
      "Average loss at step 400 : 0.512350738645\n",
      "Average loss at step 500 : 0.513093966842\n",
      "Average loss at step 600 : 0.510357679427\n",
      "Average loss at step 700 : 0.515141741931\n",
      "Average loss at step 800 : 0.513744082451\n",
      "Average loss at step 900 : 0.510844838321\n",
      "Epoch training loss : 0.513111656341\n",
      "\n",
      "EPOCH 93\n",
      "Average loss at step 100 : 0.51736230582\n",
      "Average loss at step 200 : 0.513840998113\n",
      "Average loss at step 300 : 0.510323317647\n",
      "Average loss at step 400 : 0.512816036046\n",
      "Average loss at step 500 : 0.513403044939\n",
      "Average loss at step 600 : 0.514161356986\n",
      "Average loss at step 700 : 0.512380200028\n",
      "Average loss at step 800 : 0.515200537145\n",
      "Average loss at step 900 : 0.513419930935\n",
      "Epoch training loss : 0.513656414184\n",
      "\n",
      "EPOCH 94\n",
      "Average loss at step 100 : 0.5187999928\n",
      "Average loss at step 200 : 0.510820845068\n",
      "Average loss at step 300 : 0.511033222079\n",
      "Average loss at step 400 : 0.514050747752\n",
      "Average loss at step 500 : 0.513262666762\n",
      "Average loss at step 600 : 0.510686968863\n",
      "Average loss at step 700 : 0.512486343086\n",
      "Average loss at step 800 : 0.512824557424\n",
      "Average loss at step 900 : 0.514620335698\n",
      "Epoch training loss : 0.513176186615\n",
      "\n",
      "EPOCH 95\n",
      "Average loss at step 100 : 0.520572892427\n",
      "Average loss at step 200 : 0.51258287847\n",
      "Average loss at step 300 : 0.510374541283\n",
      "Average loss at step 400 : 0.51096955806\n",
      "Average loss at step 500 : 0.512218621671\n",
      "Average loss at step 600 : 0.513645104766\n",
      "Average loss at step 700 : 0.513356809616\n",
      "Average loss at step 800 : 0.513836729825\n",
      "Average loss at step 900 : 0.513564425111\n",
      "Epoch training loss : 0.513457951248\n",
      "\n",
      "EPOCH 96\n",
      "Average loss at step 100 : 0.516330421865\n",
      "Average loss at step 200 : 0.512244353592\n",
      "Average loss at step 300 : 0.514216702878\n",
      "Average loss at step 400 : 0.512832596898\n",
      "Average loss at step 500 : 0.510578184426\n",
      "Average loss at step 600 : 0.513956262171\n",
      "Average loss at step 700 : 0.515498251915\n",
      "Average loss at step 800 : 0.512517822683\n",
      "Average loss at step 900 : 0.514483425915\n",
      "Epoch training loss : 0.513628669149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 97\n",
      "Average loss at step 100 : 0.517581965327\n",
      "Average loss at step 200 : 0.509571530521\n",
      "Average loss at step 300 : 0.511613430977\n",
      "Average loss at step 400 : 0.510005876124\n",
      "Average loss at step 500 : 0.514351153672\n",
      "Average loss at step 600 : 0.511597859561\n",
      "Average loss at step 700 : 0.511934793591\n",
      "Average loss at step 800 : 0.511172236502\n",
      "Average loss at step 900 : 0.512904530466\n",
      "Epoch training loss : 0.512303708527\n",
      "\n",
      "EPOCH 98\n",
      "Average loss at step 100 : 0.517014267147\n",
      "Average loss at step 200 : 0.512943283617\n",
      "Average loss at step 300 : 0.511453853548\n",
      "Average loss at step 400 : 0.511368736327\n",
      "Average loss at step 500 : 0.512977685332\n",
      "Average loss at step 600 : 0.509814230204\n",
      "Average loss at step 700 : 0.512198905945\n",
      "Average loss at step 800 : 0.512038338184\n",
      "Average loss at step 900 : 0.515566394329\n",
      "Epoch training loss : 0.512819521626\n",
      "\n",
      "EPOCH 99\n",
      "Average loss at step 100 : 0.516847421229\n",
      "Average loss at step 200 : 0.515165396631\n",
      "Average loss at step 300 : 0.510756031275\n",
      "Average loss at step 400 : 0.510373124778\n",
      "Average loss at step 500 : 0.51033608079\n",
      "Average loss at step 600 : 0.511821944118\n",
      "Average loss at step 700 : 0.510619745255\n",
      "Average loss at step 800 : 0.512723864913\n",
      "Average loss at step 900 : 0.513995504975\n",
      "Epoch training loss : 0.512515457107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa2dd1f2b70>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxSJ5P9TlB0kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa2dfd3f588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_losses = train_network(100,num_steps, state_size)\n",
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
