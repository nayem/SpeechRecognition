{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn.python.ops import core_rnn_cell_impl\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.contrib.rnn import RNNCell\n",
    "import time\n",
    "\n",
    "_BIAS_VARIABLE_NAME = \"biases\"\n",
    "_WEIGHTS_VARIABLE_NAME = \"weights\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BasicLSTMCell(RNNCell):\n",
    "  \"\"\"Basic LSTM recurrent network cell.\n",
    "  The implementation is based on: http://arxiv.org/abs/1409.2329.\n",
    "  We add forget_bias (default: 1) to the biases of the forget gate in order to\n",
    "  reduce the scale of forgetting in the beginning of the training.\n",
    "  It does not allow cell clipping, a projection layer, and does not\n",
    "  use peep-hole connections: it is the basic baseline.\n",
    "  For advanced models, please use the full @{tf.nn.rnn_cell.LSTMCell}\n",
    "  that follows.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, num_units, forget_bias=1.0,\n",
    "               state_is_tuple=True, activation=None, reuse=None):\n",
    "    \"\"\"Initialize the basic LSTM cell.\n",
    "    Args:\n",
    "      num_units: int, The number of units in the LSTM cell.\n",
    "      forget_bias: float, The bias added to forget gates (see above).\n",
    "      state_is_tuple: If True, accepted and returned states are 2-tuples of\n",
    "        the `c_state` and `m_state`.  If False, they are concatenated\n",
    "        along the column axis.  The latter behavior will soon be deprecated.\n",
    "      activation: Activation function of the inner states.  Default: `tanh`.\n",
    "      reuse: (optional) Python boolean describing whether to reuse variables\n",
    "        in an existing scope.  If not `True`, and the existing scope already has\n",
    "        the given variables, an error is raised.\n",
    "    \"\"\"\n",
    "    super(BasicLSTMCell, self).__init__(_reuse=reuse)\n",
    "    if not state_is_tuple:\n",
    "      logging.warn(\"%s: Using a concatenated state is slower and will soon be \"\n",
    "                   \"deprecated.  Use state_is_tuple=True.\", self)\n",
    "    self._num_units = num_units\n",
    "    self._state_is_tuple = state_is_tuple\n",
    "    self._activation = activation or math_ops.tanh\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return (LSTMStateTuple(self._num_units, self._num_units)\n",
    "            if self._state_is_tuple else 2 * self._num_units)\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._num_units\n",
    "\n",
    "  def call(self, inputs, state):\n",
    "    \"\"\"Peephole Long short-term memory cell (LSTM).\"\"\"\n",
    "    sigmoid = math_ops.sigmoid\n",
    "    # Parameters of gates are concatenated into one multiply for efficiency.\n",
    "    if self._state_is_tuple:\n",
    "      c, h = state\n",
    "    else:\n",
    "      c, h = array_ops.split(value=state, num_or_size_splits=2, axis=1)\n",
    "\n",
    "    concat = _linear([inputs, h], 4 * self._num_units, True)\n",
    "    \n",
    "    with vs.variable_scope('peephole_conn'):\n",
    "        w_f_diag = vs.get_variable(\n",
    "              \"w_f_diag\", shape=[self._num_units], dtype=dtype)\n",
    "        w_i_diag = vs.get_variable(\n",
    "              \"w_i_diag\", shape=[self._num_units], dtype=dtype)\n",
    "        w_o_diag = vs.get_variable(\n",
    "              \"w_o_diag\", shape=[self._num_units], dtype=dtype)\n",
    "\n",
    "    # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "    i, j, f, o = array_ops.split(value=concat, num_or_size_splits=4, axis=1)\n",
    "\n",
    "    new_c = (\n",
    "        c * sigmoid(f + w_f_diag * c) + sigmoid(i + w_i_diag * c) * self._activation(j))\n",
    "    new_h = self._activation(new_c) * sigmoid(o + w_o_diag * c)\n",
    "\n",
    "    if self._state_is_tuple:\n",
    "      new_state = LSTMStateTuple(new_c, new_h)\n",
    "    else:\n",
    "      new_state = array_ops.concat([new_c, new_h], 1)\n",
    "    return new_h, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    \n",
    "batch_sz = 8\n",
    "n_timesteps = 12\n",
    "n_hidden = 512\n",
    "n_inputs = 513 #f\n",
    "n_outputs = n_inputs\n",
    "\n",
    "def build_gru_single_layer():\n",
    "    reset_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, n_timesteps, n_inputs])\n",
    "    y = tf.placeholder(tf.float32, [None, n_timesteps, n_outputs])\n",
    "    init_state = tf.placeholder(tf.float32, [None, n_hidden])\n",
    "\n",
    "    # Unpack columns\n",
    "    rnn_inputs = tf.unstack(x, axis=1) # <b arrays of shape (t,f)>. Converted like this for iteration\n",
    "    rnn_target_outputs = tf.unstack(y, axis=1) # <b arrays of shape (t,o)>\n",
    "\n",
    "\n",
    "    gru_cell = GRUCell(n_hidden)\n",
    "    hidden_l1_outputs, hidden_l1_final_state = tf.contrib.rnn.static_rnn(gru_cell, rnn_inputs, \n",
    "                                                                         initial_state=init_state)\n",
    "    \n",
    "\n",
    "    with tf.variable_scope('output_layer'):\n",
    "        W_hy = tf.get_variable('W_hy', [n_hidden, n_outputs])\n",
    "        b_y = tf.get_variable('b_y', [n_outputs], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    pre_activation_outputs = [tf.matmul(hidden_l1_output, W_hy) + b_y for hidden_l1_output in hidden_l1_outputs]\n",
    "    post_activation_outputs = [tf.nn.sigmoid(pre_activation_output) for pre_activation_output in pre_activation_outputs]\n",
    "    post_activation_outputs = tf.stack(post_activation_outputs, axis=1) # <n, t, o>\n",
    "\n",
    "    squared_losses = tf.pow(tf.subtract(y, post_activation_outputs), 2) # <n,t,o>\n",
    "    sum_of_squared_losses = tf.reduce_mean(squared_losses)\n",
    "    optimizer = tf.train.AdagradOptimizer(0.1).minimize(sum_of_squared_losses)\n",
    "\n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        init_state = init_state,\n",
    "        predictions = post_activation_outputs,\n",
    "        total_loss = sum_of_squared_losses,\n",
    "        optimizer = optimizer\n",
    "    )\n",
    "\n",
    "def test_weight_compressed_gru():\n",
    "    # Build Computational Graph\n",
    "\n",
    "    t = time.time()\n",
    "    g = build_gru_single_layer()\n",
    "    print(\"It took\", time.time() - t, \"seconds to build the graph.\")\n",
    "    for item in tf.trainable_variables():\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object.__init__() takes no parameters",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d8ae64bc81b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_gru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-1f630665da05>\u001b[0m in \u001b[0;36mtest_gru\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_gru_single_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"It took\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seconds to build the graph.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-3155ee3abe28>\u001b[0m in \u001b[0;36mbuild_gru_single_layer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mgru_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGRUCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     hidden_l1_outputs, hidden_l1_final_state = tf.contrib.rnn.static_rnn(gru_cell, rnn_inputs, \n\u001b[1;32m     27\u001b[0m                                                                          initial_state=init_state)\n",
      "\u001b[0;32m<ipython-input-18-df358a28d873>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_units, activation, reuse, kernel_initializer, bias_initializer, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m                \u001b[0mbias_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                name=None):\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRUCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_reuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Inputs must be 2-dimensional.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object.__init__() takes no parameters"
     ]
    }
   ],
   "source": [
    "test_gru()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
