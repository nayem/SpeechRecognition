{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn.python.ops import core_rnn_cell_impl\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops.math_ops import sigmoid\n",
    "from tensorflow.python.ops.math_ops import tanh\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.contrib.rnn import RNNCell\n",
    "import time\n",
    "\n",
    "_BIAS_VARIABLE_NAME = \"biases\"\n",
    "_WEIGHTS_VARIABLE_NAME = \"weights\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_LayerRNNCell' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-aa334ece08a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGRUCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LayerRNNCell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m   \"\"\"Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).\n\u001b[1;32m     59\u001b[0m   \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_LayerRNNCell' is not defined"
     ]
    }
   ],
   "source": [
    "class GRUCell(_LayerRNNCell):\n",
    "  \"\"\"Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).\n",
    "  Args:\n",
    "    num_units: int, The number of units in the GRU cell.\n",
    "    activation: Nonlinearity to use.  Default: `tanh`.\n",
    "    reuse: (optional) Python boolean describing whether to reuse variables\n",
    "     in an existing scope.  If not `True`, and the existing scope already has\n",
    "     the given variables, an error is raised.\n",
    "    kernel_initializer: (optional) The initializer to use for the weight and\n",
    "    projection matrices.\n",
    "    bias_initializer: (optional) The initializer to use for the bias.\n",
    "    name: String, the name of the layer. Layers with the same name will\n",
    "      share weights, but to avoid mistakes we require reuse=True in such\n",
    "      cases.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_units,\n",
    "               activation=None,\n",
    "               reuse=None,\n",
    "               kernel_initializer=None,\n",
    "               bias_initializer=None,\n",
    "               name=None):\n",
    "    super(GRUCell, self).__init__(_reuse=reuse, name=name)\n",
    "\n",
    "    # Inputs must be 2-dimensional.\n",
    "    self.input_spec = base_layer.InputSpec(ndim=2)\n",
    "\n",
    "    self._num_units = num_units\n",
    "    self._activation = activation or math_ops.tanh\n",
    "    self._kernel_initializer = kernel_initializer\n",
    "    self._bias_initializer = bias_initializer\n",
    "\n",
    "  @property\n",
    "  def state_size(self):\n",
    "    return self._num_units\n",
    "\n",
    "  @property\n",
    "  def output_size(self):\n",
    "    return self._num_units\n",
    "\n",
    "  def build(self, inputs_shape):\n",
    "    if inputs_shape[1].value is None:\n",
    "      raise ValueError(\"Expected inputs.shape[-1] to be known, saw shape: %s\"\n",
    "                       % inputs_shape)\n",
    "\n",
    "    input_depth = inputs_shape[1].value\n",
    "    self._gate_kernel = self.add_variable(\n",
    "        \"gates/%s\" % _WEIGHTS_VARIABLE_NAME,\n",
    "        shape=[input_depth + self._num_units, 2 * self._num_units],\n",
    "        initializer=self._kernel_initializer)\n",
    "    self._gate_bias = self.add_variable(\n",
    "        \"gates/%s\" % _BIAS_VARIABLE_NAME,\n",
    "        shape=[2 * self._num_units],\n",
    "        initializer=(\n",
    "            self._bias_initializer\n",
    "            if self._bias_initializer is not None\n",
    "            else init_ops.constant_initializer(1.0, dtype=self.dtype)))\n",
    "    self._candidate_kernel = self.add_variable(\n",
    "        \"candidate/%s\" % _WEIGHTS_VARIABLE_NAME,\n",
    "        shape=[input_depth + self._num_units, self._num_units],\n",
    "        initializer=self._kernel_initializer)\n",
    "    self._candidate_bias = self.add_variable(\n",
    "        \"candidate/%s\" % _BIAS_VARIABLE_NAME,\n",
    "        shape=[self._num_units],\n",
    "        initializer=(\n",
    "            self._bias_initializer\n",
    "            if self._bias_initializer is not None\n",
    "            else init_ops.zeros_initializer(dtype=self.dtype)))\n",
    "\n",
    "    self.built = True\n",
    "\n",
    "  def call(self, inputs, state):\n",
    "    \"\"\"Gated recurrent unit (GRU) with nunits cells.\"\"\"\n",
    "\n",
    "    gate_inputs = math_ops.matmul(\n",
    "        array_ops.concat([inputs, state], 1), self._gate_kernel)\n",
    "    gate_inputs = nn_ops.bias_add(gate_inputs, self._gate_bias)\n",
    "\n",
    "    value = math_ops.sigmoid(gate_inputs)\n",
    "    r, u = array_ops.split(value=value, num_or_size_splits=2, axis=1)\n",
    "\n",
    "    r_state = r * state\n",
    "\n",
    "    candidate = math_ops.matmul(\n",
    "        array_ops.concat([inputs, r_state], 1), self._candidate_kernel)\n",
    "    candidate = nn_ops.bias_add(candidate, self._candidate_bias)\n",
    "\n",
    "    c = self._activation(candidate)\n",
    "    new_h = u * state + (1 - u) * c\n",
    "    return new_h, new_h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    \n",
    "batch_sz = 8\n",
    "n_timesteps = 12\n",
    "n_hidden = 512\n",
    "n_inputs = 513 #f\n",
    "n_outputs = n_inputs\n",
    "\n",
    "def build_gru_single_layer():\n",
    "    reset_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.float32, [None, n_timesteps, n_inputs])\n",
    "    y = tf.placeholder(tf.float32, [None, n_timesteps, n_outputs])\n",
    "    init_state = tf.placeholder(tf.float32, [None, n_hidden])\n",
    "\n",
    "    # Unpack columns\n",
    "    rnn_inputs = tf.unstack(x, axis=1) # <b arrays of shape (t,f)>. Converted like this for iteration\n",
    "    rnn_target_outputs = tf.unstack(y, axis=1) # <b arrays of shape (t,o)>\n",
    "\n",
    "\n",
    "    gru_cell = GRUCell(n_hidden)\n",
    "    hidden_l1_outputs, hidden_l1_final_state = tf.contrib.rnn.static_rnn(gru_cell, rnn_inputs, \n",
    "                                                                         initial_state=init_state)\n",
    "    \n",
    "\n",
    "    with tf.variable_scope('output_layer'):\n",
    "        W_hy = tf.get_variable('W_hy', [n_hidden, n_outputs])\n",
    "        b_y = tf.get_variable('b_y', [n_outputs], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    pre_activation_outputs = [tf.matmul(hidden_l1_output, W_hy) + b_y for hidden_l1_output in hidden_l1_outputs]\n",
    "    post_activation_outputs = [tf.nn.sigmoid(pre_activation_output) for pre_activation_output in pre_activation_outputs]\n",
    "    post_activation_outputs = tf.stack(post_activation_outputs, axis=1) # <n, t, o>\n",
    "\n",
    "    squared_losses = tf.pow(tf.subtract(y, post_activation_outputs), 2) # <n,t,o>\n",
    "    sum_of_squared_losses = tf.reduce_mean(squared_losses)\n",
    "    optimizer = tf.train.AdagradOptimizer(0.1).minimize(sum_of_squared_losses)\n",
    "\n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        init_state = init_state,\n",
    "        predictions = post_activation_outputs,\n",
    "        total_loss = sum_of_squared_losses,\n",
    "        optimizer = optimizer\n",
    "    )\n",
    "\n",
    "def test_weight_compressed_gru():\n",
    "    # Build Computational Graph\n",
    "\n",
    "    t = time.time()\n",
    "    g = build_wb_gru_single_layer()\n",
    "    print(\"It took\", time.time() - t, \"seconds to build the graph.\")\n",
    "    for item in tf.trainable_variables():\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'unstack:0' shape=(?, 513) dtype=float32>, <tf.Tensor 'Placeholder_2:0' shape=(?, 512) dtype=float32>]\n",
      "[<tf.Tensor 'unstack:1' shape=(?, 513) dtype=float32>, <tf.Tensor 'rnn/GRUCell/add:0' shape=(?, 512) dtype=float32>]\n",
      "[<tf.Tensor 'unstack:2' shape=(?, 513) dtype=float32>, <tf.Tensor 'rnn/GRUCell_1/add:0' shape=(?, 512) dtype=float32>]\n",
      "[<tf.Tensor 'unstack:3' shape=(?, 513) dtype=float32>, <tf.Tensor 'rnn/GRUCell_2/add:0' shape=(?, 512) dtype=float32>]\n",
      "[<tf.Tensor 'unstack:4' shape=(?, 513) dtype=float32>, <tf.Tensor 'rnn/GRUCell_3/add:0' shape=(?, 512) dtype=float32>]\n",
      "[<tf.Tensor 'unstack:5' shape=(?, 513) dtype=float32>, <tf.Tensor 'rnn/GRUCell_4/add:0' shape=(?, 512) dtype=float32>]\n",
      "[<tf.Tensor 'unstack:6' shape=(?, 513) dtype=float32>, <tf.Tensor 'rnn/GRUCell_5/add:0' shape=(?, 512) dtype=float32>]\n",
      "[<tf.Tensor 'unstack:7' shape=(?, 513) dtype=float32>, <tf.Tensor 'rnn/GRUCell_6/add:0' shape=(?, 512) dtype=float32>]\n",
      "[<tf.Tensor 'unstack:8' shape=(?, 513) dtype=float32>, <tf.Tensor 'rnn/GRUCell_7/add:0' shape=(?, 512) dtype=float32>]\n",
      "[<tf.Tensor 'unstack:9' shape=(?, 513) dtype=float32>, <tf.Tensor 'rnn/GRUCell_8/add:0' shape=(?, 512) dtype=float32>]\n",
      "[<tf.Tensor 'unstack:10' shape=(?, 513) dtype=float32>, <tf.Tensor 'rnn/GRUCell_9/add:0' shape=(?, 512) dtype=float32>]\n",
      "[<tf.Tensor 'unstack:11' shape=(?, 513) dtype=float32>, <tf.Tensor 'rnn/GRUCell_10/add:0' shape=(?, 512) dtype=float32>]\n",
      "It took 1.3689978122711182 seconds to build the graph.\n",
      "<tf.Variable 'rnn/GRUCell/gates/weights:0' shape=(1025, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/GRUCell/gates/biases:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/GRUCell/candidate/weights:0' shape=(1025, 512) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/GRUCell/candidate/biases:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'output_layer/W_hy:0' shape=(512, 513) dtype=float32_ref>\n",
      "<tf.Variable 'output_layer/b_y:0' shape=(513,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "test_gru()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
