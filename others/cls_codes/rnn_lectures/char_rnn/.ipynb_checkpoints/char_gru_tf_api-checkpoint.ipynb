{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from : https://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ptb_iterator(raw_data, batch_size, num_steps, steps_ahead=1):\n",
    "\n",
    "    raw_data = np.array(raw_data, dtype=np.int32)\n",
    "    data_len = len(raw_data)\n",
    "    batch_len = data_len // batch_size\n",
    "    data = np.zeros([batch_size, batch_len], dtype=np.int32)\n",
    "    offset = 0\n",
    "    if data_len % batch_size:\n",
    "        offset = np.random.randint(0, data_len % batch_size)\n",
    "    for i in range(batch_size):\n",
    "        data[i] = raw_data[batch_len * i + offset:batch_len * (i + 1) + offset]\n",
    "    epoch_size = (batch_len - steps_ahead) // num_steps\n",
    "    if epoch_size == 0:\n",
    "        raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "    for i in range(epoch_size):\n",
    "        x = data[:, i*num_steps:(i+1)*num_steps]\n",
    "        y = data[:, i*num_steps+1:(i+1)*num_steps+steps_ahead]\n",
    "        yield (x, y)\n",
    "    if epoch_size * num_steps < batch_len - steps_ahead:\n",
    "        yield (data[:, epoch_size*num_steps : batch_len - steps_ahead], data[:, epoch_size*num_steps + 1:])\n",
    "\n",
    "\n",
    "def shuffled_ptb_iterator(raw_data, batch_size, num_steps):\n",
    "    raw_data = np.array(raw_data, dtype=np.int32)\n",
    "    r = len(raw_data) % num_steps\n",
    "    if r:\n",
    "        n = np.random.randint(0, r)\n",
    "        raw_data = raw_data[n:n + len(raw_data) - r]\n",
    "    raw_data = np.reshape(raw_data, [-1, num_steps])\n",
    "    np.random.shuffle(raw_data)\n",
    "    num_batches = int(np.ceil(len(raw_data) / batch_size))\n",
    "    for i in range(num_batches):\n",
    "        data = raw_data[i*batch_size:min(len(raw_data), (i+1)*batch_size),:]\n",
    "        yield (data[:,:-1], data[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 1115394\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load and process data, utility functions\n",
    "\"\"\"\n",
    "\n",
    "file_url = 'https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt'\n",
    "file_name = 'tinyshakespeare.txt'\n",
    "if not os.path.exists(file_name):\n",
    "    urllib.request.urlretrieve(file_url, file_name)\n",
    "\n",
    "with open(file_name,'r') as f:\n",
    "    raw_data = f.read()\n",
    "    print(\"Data length:\", len(raw_data))\n",
    "\n",
    "vocab = set(raw_data)\n",
    "vocab_size = len(vocab)\n",
    "idx_to_vocab = dict(enumerate(vocab))\n",
    "vocab_to_idx = dict(zip(idx_to_vocab.values(), idx_to_vocab.keys()))\n",
    "\n",
    "data = [vocab_to_idx[c] for c in raw_data]\n",
    "del raw_data\n",
    "\n",
    "def gen_epochs(n, num_steps, batch_size):\n",
    "    for i in range(n):\n",
    "        yield ptb_iterator(data, batch_size, num_steps)\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "def train_network(g, num_epochs, num_steps = 200, batch_size = 32, verbose = True, save=False):\n",
    "    tf.set_random_seed(2345)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps, batch_size)):\n",
    "            training_loss = 0\n",
    "            steps = 0\n",
    "            training_state = None\n",
    "            for X, Y in epoch:\n",
    "                steps += 1\n",
    "\n",
    "                if X.shape[1] != g['x'].shape[1]:\n",
    "                    break \n",
    "                    \n",
    "                feed_dict={g['x']: X, g['y']: Y}\n",
    "                if training_state is not None:\n",
    "                    feed_dict[g['init_state']] = training_state\n",
    "                training_loss_, training_state, _ = sess.run([g['total_loss'],\n",
    "                                                      g['final_state'],\n",
    "                                                      g['train_step']],\n",
    "                                                             feed_dict)\n",
    "                training_loss += training_loss_\n",
    "            if verbose:\n",
    "                print(\"Average training loss for Epoch\", idx, \":\", training_loss/steps)\n",
    "            training_losses.append(training_loss/steps)\n",
    "\n",
    "        g['saver'].save(sess, \"model_gru.ckpt\")\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_basic_rnn_graph_with_list(\n",
    "    state_size = 100,\n",
    "    num_classes = vocab_size,\n",
    "    batch_size = 32,\n",
    "    num_steps = 200,\n",
    "    learning_rate = 1e-4):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "\n",
    "    x_one_hot = tf.one_hot(x, num_classes) # <n, t, f>\n",
    "    rnn_inputs = [tf.squeeze(i,squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, axis=1)]\n",
    "\n",
    "    cell = tf.contrib.rnn.GRUCell(state_size)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "    predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "    y_as_list = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(y, num_steps, axis=1)] # t arrays of tensor <n, o>\n",
    "\n",
    "    loss_weights = [tf.ones([batch_size]) for i in range(num_steps)]\n",
    "    losses = tf.contrib.legacy_seq2seq.sequence_loss_by_example(logits, y_as_list, loss_weights)\n",
    "    total_loss = tf.reduce_mean(losses)\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        init_state = init_state,\n",
    "        final_state = final_state,\n",
    "        total_loss = total_loss,\n",
    "        train_step = train_step,\n",
    "        preds = predictions,\n",
    "        saver = saver\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 26.46511220932007 seconds to build the graph.\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "g = build_basic_rnn_graph_with_list()\n",
    "print(\"It took\", time.time() - t, \"seconds to build the graph.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss for Epoch 0 : 3.70014415877\n",
      "Average training loss for Epoch 1 : 3.28273518426\n",
      "Average training loss for Epoch 2 : 3.23873631477\n",
      "Average training loss for Epoch 3 : 3.14900733948\n",
      "Average training loss for Epoch 4 : 3.03178640911\n",
      "Average training loss for Epoch 5 : 2.90671122006\n",
      "Average training loss for Epoch 6 : 2.79576170513\n",
      "Average training loss for Epoch 7 : 2.70284665653\n",
      "Average training loss for Epoch 8 : 2.62354471207\n",
      "Average training loss for Epoch 9 : 2.5565119648\n",
      "It took 255.19610476493835 seconds to train for 10 epochs.\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "train_network(g, 10)\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 10 epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_characters(g, checkpoint, num_chars, prompt='A', pick_top_chars=None):\n",
    "    \"\"\" Accepts a current character, initial state\"\"\"\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        if checkpoint is None:\n",
    "            g['saver'].restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "        state = None\n",
    "        current_char = vocab_to_idx[prompt]\n",
    "        chars = [current_char]\n",
    "\n",
    "        for i in range(num_chars):\n",
    "            if state is not None:\n",
    "                feed_dict={g['x']: [[current_char]], g['init_state']: state}\n",
    "            else:\n",
    "                feed_dict={g['x']: [[current_char]]}\n",
    "\n",
    "            preds, state = sess.run([g['preds'],g['final_state']], feed_dict)\n",
    "\n",
    "            if pick_top_chars is not None:\n",
    "                p = np.squeeze(preds)\n",
    "                p[np.argsort(p)[:-pick_top_chars]] = 0\n",
    "                p = p / np.sum(p)\n",
    "                current_char = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "            else:\n",
    "                current_char = np.random.choice(vocab_size, 1, p=np.squeeze(preds))[0]\n",
    "\n",
    "            chars.append(current_char)\n",
    "\n",
    "    chars = map(lambda x: idx_to_vocab[x], chars)\n",
    "    print(\"\".join(chars))\n",
    "    return(\"\".join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model_gru.ckpt\n",
      "ANTIE TI heour band wise mire, be wised\n",
      "\n",
      "ord so the te th le anst\n",
      "Aed an than has sint oou hind shis,\n",
      "Anssan seend\n",
      "the ters tat ait an to seens ons aist oor,\n",
      "\n",
      "hon than there so thase th an thith wil aet,e nar the tho het sale mathe war thant aress oot tha to linet, tho  oord therse\n",
      " aatt or mire anet thand she torere th son ho ehes thes ao tharstare sire, mand soe me thet oo so le thet oou seas thir wone an te sind the sorthet tounde she teon,\n",
      "\n",
      "or hane thet sher sarert on teot one then, ahes the hese thes te th meril mond th ee thas hother, thes fase ahat  ine mone ther the th mite tonds and wite mher thes here hhe than se eith tean sh thes ore ae ee sou  hetter tae thond and are th thend whe thand,\n",
      "And while the  eree shir thetd mare he mil sien toe hher site tithe s ao thin, tit aou hon sang sath nind wir mant, an thon shes ho thest ea le sord whe teet the te thang hong aou thet orere hhas tha s ir she sirs ao thare thin the ler toe hot to shore tou thote\n",
      "st ae tees then,\n",
      "\n",
      "nite so sea\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training after 10 epoch\n",
    "g = build_basic_rnn_graph_with_list(batch_size=1, num_steps = 1)\n",
    "generate_characters(g, None, 1000, prompt='A', pick_top_chars=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
