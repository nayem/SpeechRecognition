{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted from : https://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ptb_iterator(raw_data, batch_size, num_steps, steps_ahead=1):\n",
    "\n",
    "    raw_data = np.array(raw_data, dtype=np.int32)\n",
    "    data_len = len(raw_data)\n",
    "    batch_len = data_len // batch_size\n",
    "    data = np.zeros([batch_size, batch_len], dtype=np.int32)\n",
    "    offset = 0\n",
    "    if data_len % batch_size:\n",
    "        offset = np.random.randint(0, data_len % batch_size)\n",
    "    for i in range(batch_size):\n",
    "        data[i] = raw_data[batch_len * i + offset:batch_len * (i + 1) + offset]\n",
    "    epoch_size = (batch_len - steps_ahead) // num_steps\n",
    "    if epoch_size == 0:\n",
    "        raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    "    for i in range(epoch_size):\n",
    "        x = data[:, i*num_steps:(i+1)*num_steps]\n",
    "        y = data[:, i*num_steps+1:(i+1)*num_steps+steps_ahead]\n",
    "        yield (x, y)\n",
    "    if epoch_size * num_steps < batch_len - steps_ahead:\n",
    "        yield (data[:, epoch_size*num_steps : batch_len - steps_ahead], data[:, epoch_size*num_steps + 1:])\n",
    "\n",
    "\n",
    "def shuffled_ptb_iterator(raw_data, batch_size, num_steps):\n",
    "    raw_data = np.array(raw_data, dtype=np.int32)\n",
    "    r = len(raw_data) % num_steps\n",
    "    if r:\n",
    "        n = np.random.randint(0, r)\n",
    "        raw_data = raw_data[n:n + len(raw_data) - r]\n",
    "    raw_data = np.reshape(raw_data, [-1, num_steps])\n",
    "    np.random.shuffle(raw_data)\n",
    "    num_batches = int(np.ceil(len(raw_data) / batch_size))\n",
    "    for i in range(num_batches):\n",
    "        data = raw_data[i*batch_size:min(len(raw_data), (i+1)*batch_size),:]\n",
    "        yield (data[:,:-1], data[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length: 1115394\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load and process data, utility functions\n",
    "\"\"\"\n",
    "\n",
    "file_url = 'https://raw.githubusercontent.com/jcjohnson/torch-rnn/master/data/tiny-shakespeare.txt'\n",
    "file_name = 'tinyshakespeare.txt'\n",
    "if not os.path.exists(file_name):\n",
    "    urllib.request.urlretrieve(file_url, file_name)\n",
    "\n",
    "with open(file_name,'r') as f:\n",
    "    raw_data = f.read()\n",
    "    print(\"Data length:\", len(raw_data))\n",
    "\n",
    "vocab = set(raw_data)\n",
    "vocab_size = len(vocab)\n",
    "idx_to_vocab = dict(enumerate(vocab))\n",
    "vocab_to_idx = dict(zip(idx_to_vocab.values(), idx_to_vocab.keys()))\n",
    "\n",
    "data = [vocab_to_idx[c] for c in raw_data]\n",
    "del raw_data\n",
    "\n",
    "def gen_epochs(n, num_steps, batch_size):\n",
    "    for i in range(n):\n",
    "        yield ptb_iterator(data, batch_size, num_steps)\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "def train_network(g, num_epochs, num_steps = 200, batch_size = 32, verbose = True, save=False):\n",
    "    tf.set_random_seed(2345)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps, batch_size)):\n",
    "            training_loss = 0\n",
    "            steps = 0\n",
    "            training_state = None\n",
    "            for X, Y in epoch:\n",
    "                steps += 1\n",
    "\n",
    "                if X.shape[1] != g['x'].shape[1]:\n",
    "                    break \n",
    "                    \n",
    "                feed_dict={g['x']: X, g['y']: Y}\n",
    "                if training_state is not None:\n",
    "                    feed_dict[g['init_state']] = training_state\n",
    "                training_loss_, training_state, _ = sess.run([g['total_loss'],\n",
    "                                                      g['final_state'],\n",
    "                                                      g['train_step']],\n",
    "                                                             feed_dict)\n",
    "                training_loss += training_loss_\n",
    "            if verbose:\n",
    "                print(\"Average training loss for Epoch\", idx, \":\", training_loss/steps)\n",
    "            training_losses.append(training_loss/steps)\n",
    "\n",
    "        g['saver'].save(sess, \"model.ckpt\")\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_basic_rnn_graph_with_list(\n",
    "    state_size = 100,\n",
    "    num_classes = vocab_size,\n",
    "    batch_size = 32,\n",
    "    num_steps = 200,\n",
    "    learning_rate = 1e-4):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "\n",
    "    x_one_hot = tf.one_hot(x, num_classes) # <n, t, f>\n",
    "    rnn_inputs = [tf.squeeze(i,squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, axis=1)]\n",
    "\n",
    "    cell = tf.contrib.rnn.BasicRNNCell(state_size)\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "    logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "    predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "    y_as_list = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(y, num_steps, axis=1)] # t arrays of tensor <n, o>\n",
    "#     print(y_as_list)\n",
    "\n",
    "    loss_weights = [tf.ones([batch_size]) for i in range(num_steps)]\n",
    "    losses = tf.contrib.legacy_seq2seq.sequence_loss_by_example(logits, y_as_list, loss_weights)\n",
    "    total_loss = tf.reduce_mean(losses)\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        init_state = init_state,\n",
    "        final_state = final_state,\n",
    "        total_loss = total_loss,\n",
    "        train_step = train_step,\n",
    "        preds = predictions,\n",
    "        saver = saver\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 11.272886514663696 seconds to build the graph.\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "g = build_basic_rnn_graph_with_list()\n",
    "print(\"It took\", time.time() - t, \"seconds to build the graph.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss for Epoch 0 : 3.6946648407\n",
      "Average training loss for Epoch 1 : 3.27801978929\n",
      "Average training loss for Epoch 2 : 3.21488930293\n",
      "Average training loss for Epoch 3 : 3.10075888089\n",
      "Average training loss for Epoch 4 : 2.97092319897\n",
      "Average training loss for Epoch 5 : 2.8610919748\n",
      "Average training loss for Epoch 6 : 2.76816867692\n",
      "Average training loss for Epoch 7 : 2.68949493544\n",
      "Average training loss for Epoch 8 : 2.62147042274\n",
      "Average training loss for Epoch 9 : 2.56234738895\n",
      "It took 148.73711967468262 seconds to train for 10 epochs.\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "train_network(g, 10)\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 10 epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_characters(g, checkpoint, num_chars, prompt='A', pick_top_chars=None):\n",
    "    \"\"\" Accepts a current character, initial state\"\"\"\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        if checkpoint is None:\n",
    "            g['saver'].restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "        state = None\n",
    "        current_char = vocab_to_idx[prompt]\n",
    "        chars = [current_char]\n",
    "\n",
    "        for i in range(num_chars):\n",
    "            if state is not None:\n",
    "                feed_dict={g['x']: [[current_char]], g['init_state']: state}\n",
    "            else:\n",
    "                feed_dict={g['x']: [[current_char]]}\n",
    "\n",
    "            preds, state = sess.run([g['preds'],g['final_state']], feed_dict)\n",
    "\n",
    "            if pick_top_chars is not None:\n",
    "                p = np.squeeze(preds)\n",
    "                p[np.argsort(p)[:-pick_top_chars]] = 0\n",
    "                p = p / np.sum(p)\n",
    "                current_char = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "            else:\n",
    "                current_char = np.random.choice(vocab_size, 1, p=np.squeeze(preds))[0]\n",
    "\n",
    "            chars.append(current_char)\n",
    "\n",
    "    chars = map(lambda x: idx_to_vocab[x], chars)\n",
    "    print(\"\".join(chars))\n",
    "    return(\"\".join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model.ckpt\n",
      "[[13]]\n",
      "Au,Iuh   tt e   tt  o eeoo eoth to o eeeo    oo eo ha  t o  ee e oo h e h e  t tr eto t toeeee eaeoa ee e  ot e e ete he toeaea oe t he   o  o t   etat ee  t teeoooo he oe hoe  eeetha eo  e  hae h ot httee t tet t  oa ht   het toa  o   e  eoe e  t  to ha to  o ha  eae   h teaet heo ttee eaaae  to e e h t       he     ea t  oa te eeeooh  e      ttoa ht  te  o t  ht hoothoeaoo e  ot  ot  o    oe ea ee e  t h hte oee ho  e heetoo eaaee het hothe eaa heo e e     h e oeeo  e     o oe tt  e tte hotto e  oot  t     totoot    eeotoo too  ottooeo  te  eteaeoa    o  t  o t tet taeoe e ootet    tea  e e  oee   heoo h   e to o e  hoe et  eeee t   et e th ooahaee eee ootet o hr   he eo   h  eaeoohe t   ea  ooo eooe  h e ho e e  t t e th o     h e te eo e\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training after 1 epoch\n",
    "g = build_basic_rnn_graph_with_list(batch_size=1, num_steps = 1)\n",
    "generate_characters(g, None, 750, prompt='A', pick_top_chars=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model.ckpt\n",
      "ARd IC:\n",
      "Wive weres tho se aor weale bott int and titt ee toe tou the thor sert an al sores or thir ait the th me tite wale the thime sare ton erithes\n",
      "\n",
      "has soeeees ta the hh thit ao din  oud tald boertere sis andt ie th teor arate bathet tous the that our and seree \n",
      "aats tisl so lare be the wot tires,\n",
      "I  er ine then sen sorte teathe thas tire senee \n",
      "on tin taas  ou hat thir, toe therthes  hin donge se mhount  or alt oo thirh mit and we than toun satder,er tor the soul tond toe tont\n",
      "toe hheres thes ood tor te she lond ar wolle se mat aad thete ao the lald botle tind whal, th mirhen thimh wh lonet\n",
      "\n",
      "oer ard ar  aothe the hor the sh seeer an ald wongh mous sare,\n",
      "Th tor sisl oo sot ian andi t aod boe sothen misthas ao toe dar soust tate tor teond warling oratho  hatle wanthi des aad tous te lo lerere she taat ao sist\n",
      "\n",
      "n tit aa sise tererat  oarss wou de tord ans sithet,\n",
      "\n",
      "he  aat orerstond tie  orderes  aot aal tore tha  ootther  ie thes or taree and whin thar sisee,\n",
      "Th me this aad ahe we sind\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training after 10 epoch\n",
    "g = build_basic_rnn_graph_with_list(batch_size=1, num_steps = 1)\n",
    "generate_characters(g, None, 1000, prompt='A', pick_top_chars=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
