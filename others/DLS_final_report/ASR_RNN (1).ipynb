{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Compatibility imports\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from six.moves import xrange as range\n",
    "\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from python_speech_features import mfcc\n",
    "except ImportError:\n",
    "    print(\"Failed to import python_speech_features.\\n Try pip install python_speech_features.\")\n",
    "    raise ImportError\n",
    "\n",
    "from utils import maybe_download as maybe_download\n",
    "from utils import sparse_tuple_from as sparse_tuple_from\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.stem.lancaster import LancasterStemmer\n",
    "# from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SPACE_TOKEN = '<space>'\n",
    "SPACE_INDEX = 0\n",
    "FIRST_INDEX = ord('a') - 1  # 0 is reserved to space\n",
    "\n",
    "# Some configs\n",
    "num_features = 13\n",
    "num_units=50 # Number of units in the LSTM cell\n",
    "# Accounting the 0th indice +  space + blank label = 28 characters\n",
    "num_classes = ord('z') - ord('a') + 1 + 1 + 1\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 200\n",
    "num_hidden = 50\n",
    "num_layers = 2\n",
    "batch_size = 1\n",
    "initial_learning_rate = 1e-2\n",
    "momentum = 0.9\n",
    "\n",
    "num_examples = 1\n",
    "num_batches_per_epoch = int(num_examples/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIRECTORY = 'TIMIT_full/'\n",
    "\n",
    "WAV_CLASS = '*.wav'\n",
    "TXT_CLASS = '*.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIRECTORY = 'train_16k/'\n",
    "VAL_DIRECTORY = 'val_16k/'\n",
    "TEST_DIRECTORY = 'test_16k/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIRECTORY = 'timit_train_16k/'\n",
    "VAL_DIRECTORY = 'timit_val_16k/'\n",
    "TEST_DIRECTORY = 'timit_test_16k/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(wav_path,txt_path):\n",
    "\n",
    "    x_s = []\n",
    "    y_s = []\n",
    "    text_s = []\n",
    "    print(wav_path, txt_path)\n",
    "    for e, (f1, f2) in enumerate( zip(sorted(glob.glob(wav_path)),sorted(glob.glob(txt_path))) ) :\n",
    "#         print('FileName:',f1, f2)\n",
    "        \n",
    "        ##### Read Audio features #####\n",
    "        ###############################\n",
    "        audio, fs = sf.read(f1)\n",
    "        inputs = mfcc(audio, samplerate=fs)\n",
    "        \n",
    "        # Tranform in 3D array\n",
    "        train_inputs = np.asarray(inputs[np.newaxis, :])\n",
    "        train_inputs = (train_inputs - np.mean(train_inputs))/np.std(train_inputs)\n",
    "#         print('train_inputs.shape:', train_inputs.shape)\n",
    "        \n",
    "        train_seq_len = [train_inputs.shape[1]]\n",
    "#         print('train_seq_len.len:', len(train_seq_len) )\n",
    "        \n",
    "        x_s.append(train_inputs)\n",
    "#         print('x_s.len', len(x_s))\n",
    "        \n",
    "        ##### Read Labels features #####\n",
    "        ###############################\n",
    "        with open(f2, 'r') as txt_f:\n",
    "            line = txt_f.readlines()[-1] #Only the last line is necessary\n",
    "\n",
    "            targets = preprocess_line(line)\n",
    "            text_s.append(targets)\n",
    "\n",
    "            # Adding blank label\n",
    "            targets = np.hstack([SPACE_TOKEN if x == '' else list(x) for x in targets])\n",
    "\n",
    "            # Transform char into index\n",
    "            targets = np.asarray([SPACE_INDEX if x == SPACE_TOKEN else ord(x) - FIRST_INDEX\n",
    "                                  for x in targets])\n",
    "\n",
    "            # Creating sparse representation to feed the placeholder\n",
    "            train_targets = sparse_tuple_from([targets])\n",
    "#             print('train_targets.shape:', len(train_targets))\n",
    "            y_s.append(train_targets)\n",
    "              \n",
    "    \n",
    "    print('x_s.len', len(x_s),', [-1]x_s.len', x_s[-1].shape)\n",
    "#     x_s = pad_sequences(x_s, maxlen=500, dtype='float', padding='post', truncating='post')\n",
    "#     print('x_s.shape', (x_s.shape))\n",
    "        \n",
    "    \n",
    "    return x_s, y_s,text_s\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_line(line):\n",
    "    \n",
    "    original = ' '.join(line.strip().lower().split(' ')[2:])\n",
    "    for c in set(original):\n",
    "        if c != ' ' and not c.isalpha():\n",
    "            original = original.replace(c,'')\n",
    "        \n",
    "    targets = original.replace(' ', '  ')\n",
    "    targets = targets.split(' ')\n",
    "\n",
    "    i = 0\n",
    "    while i<len(targets)-1:\n",
    "        if targets[i]==targets[i+1] and targets[i+1]=='':\n",
    "            del targets[i+1]\n",
    "        else:\n",
    "            i +=1\n",
    "            \n",
    "    return targets\n",
    "    \n",
    "    # Get only the words between [a-z] and replace period for none\n",
    "#     original = ' '.join(line.strip().lower().split(' ')[2:]).replace('.', '').replace('\\'','')\n",
    "#     targets = original.replace(' ', '  ')\n",
    "#     return targets.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Train, Val, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_, y_,text_ = load_dataset(ROOT_DIRECTORY+TRAIN_DIRECTORY+WAV_CLASS, ROOT_DIRECTORY+TRAIN_DIRECTORY+TXT_CLASS)  \n",
    "print(len(x_), len(y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x_, val_y_, val_text_ = load_dataset(ROOT_DIRECTORY+VAL_DIRECTORY+WAV_CLASS, ROOT_DIRECTORY+VAL_DIRECTORY+TXT_CLASS)  \n",
    "print(len(val_x_), len(val_y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_, test_y_, test_text_ = load_dataset(ROOT_DIRECTORY+TEST_DIRECTORY+WAV_CLASS, ROOT_DIRECTORY+TEST_DIRECTORY+TXT_CLASS)  \n",
    "print(len(test_x_), len(test_y_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE MAIN CODE! LSTM\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # e.g: log filter bank or MFCC features\n",
    "    # Has size [batch_size, max_stepsize, num_features], but the\n",
    "    # batch_size and max_stepsize can vary along each step\n",
    "    inputs = tf.placeholder(tf.float32, [None, None, num_features])\n",
    "\n",
    "    # Here we use sparse_placeholder that will generate a\n",
    "    # SparseTensor required by ctc_loss op.\n",
    "    targets = tf.sparse_placeholder(tf.int32)\n",
    "\n",
    "    # 1d array of size [batch_size]\n",
    "    seq_len = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    # Defining the cell\n",
    "    # Can be:\n",
    "    #   tf.nn.rnn_cell.RNNCell\n",
    "    #   tf.nn.rnn_cell.GRUCell \n",
    "    cells = []\n",
    "    for _ in range(num_layers):\n",
    "        cell = tf.contrib.rnn.LSTMCell(num_units)  # Or LSTMCell(num_units)\n",
    "        cells.append(cell)\n",
    "    stack = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "    # The second output is the last state and we will no use that\n",
    "    outputs, _ = tf.nn.dynamic_rnn(stack, inputs, seq_len, dtype=tf.float32)\n",
    "\n",
    "    shape = tf.shape(inputs)\n",
    "    batch_s, max_timesteps = shape[0], shape[1]\n",
    "\n",
    "    # Reshaping to apply the same weights over the timesteps\n",
    "    outputs = tf.reshape(outputs, [-1, num_hidden])\n",
    "\n",
    "    # Truncated normal with mean 0 and stdev=0.1\n",
    "    # Tip: Try another initialization\n",
    "    # see https://www.tensorflow.org/versions/r0.9/api_docs/python/contrib.layers.html#initializers\n",
    "    W = tf.Variable(tf.truncated_normal([num_hidden,\n",
    "                                         num_classes],\n",
    "                                        stddev=0.1))\n",
    "    # Zero initialization\n",
    "    # Tip: Is tf.zeros_initializer the same?\n",
    "    b = tf.Variable(tf.constant(0., shape=[num_classes]))\n",
    "\n",
    "    # Doing the affine projection\n",
    "    logits = tf.matmul(outputs, W) + b\n",
    "\n",
    "    # Reshaping back to the original shape\n",
    "    logits = tf.reshape(logits, [batch_s, -1, num_classes])\n",
    "\n",
    "    # Time major\n",
    "    logits = tf.transpose(logits, (1, 0, 2))\n",
    "\n",
    "    loss = tf.nn.ctc_loss(targets, logits, seq_len)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    optimizer = tf.train.MomentumOptimizer(initial_learning_rate,\n",
    "                                           0.9).minimize(cost)\n",
    "\n",
    "    # Option 2: tf.nn.ctc_beam_search_decoder\n",
    "    # (it's slower but you'll get better results)\n",
    "    decoded, log_prob = tf.nn.ctc_greedy_decoder(logits, seq_len)\n",
    "\n",
    "    # Inaccuracy: label error rate\n",
    "    ler = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32),\n",
    "                                          targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU and Bidirectional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE MAIN CODE! BiLSTM+GRU\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # e.g: log filter bank or MFCC features\n",
    "    # Has size [batch_size, max_stepsize, num_features], but the\n",
    "    # batch_size and max_stepsize can vary along each step\n",
    "    inputs = tf.placeholder(tf.float32, [None, None, num_features])\n",
    "\n",
    "    # Here we use sparse_placeholder that will generate a\n",
    "    # SparseTensor required by ctc_loss op.\n",
    "    targets = tf.sparse_placeholder(tf.int32)\n",
    "\n",
    "    # 1d array of size [batch_size]\n",
    "    seq_len = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    # Defining the cell\n",
    "    # Can be:\n",
    "    #   tf.nn.rnn_cell.RNNCell\n",
    "    #   tf.nn.rnn_cell.GRUCell \n",
    "    cells = []\n",
    "    for _ in range(num_layers):\n",
    "        cell = tf.contrib.rnn.GRUCell(num_units)  # Or LSTMCell(num_units)\n",
    "        cells.append(cell)\n",
    "    stack = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "    \n",
    "    cells_bw = []\n",
    "    for _ in range(num_layers):\n",
    "        cell_bw = tf.contrib.rnn.GRUCell(num_units)  # Or LSTMCell(num_units)\n",
    "        cells_bw.append(cell_bw)\n",
    "    stack_bw = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "    # The second output is the last state and we will no use that\n",
    "    outputs, _ = tf.nn.bidirectional_dynamic_rnn(stack,stack_bw, inputs, seq_len, dtype=tf.float32)\n",
    "\n",
    "    shape = tf.shape(inputs)\n",
    "    batch_s, max_timesteps = shape[0], shape[1]\n",
    "\n",
    "    # Reshaping to apply the same weights over the timesteps\n",
    "    outputs = tf.reshape(outputs, [-1, num_hidden])\n",
    "\n",
    "    # Truncated normal with mean 0 and stdev=0.1\n",
    "    # Tip: Try another initialization\n",
    "    # see https://www.tensorflow.org/versions/r0.9/api_docs/python/contrib.layers.html#initializers\n",
    "    W = tf.Variable(tf.truncated_normal([num_hidden,\n",
    "                                         num_classes],\n",
    "                                        stddev=0.1))\n",
    "    # Zero initialization\n",
    "    # Tip: Is tf.zeros_initializer the same?\n",
    "    b = tf.Variable(tf.constant(0., shape=[num_classes]))\n",
    "\n",
    "    # Doing the affine projection\n",
    "    logits = tf.matmul(outputs, W) + b\n",
    "\n",
    "    # Reshaping back to the original shape\n",
    "    logits = tf.reshape(logits, [batch_s, -1, num_classes])\n",
    "\n",
    "    # Time major\n",
    "    logits = tf.transpose(logits, (1, 0, 2))\n",
    "\n",
    "    loss = tf.nn.ctc_loss(targets, logits, seq_len)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    optimizer = tf.train.MomentumOptimizer(initial_learning_rate,\n",
    "                                           0.9).minimize(cost)\n",
    "\n",
    "    # Option 2: tf.nn.ctc_beam_search_decoder\n",
    "    # (it's slower but you'll get better results)\n",
    "    decoded, log_prob = tf.nn.ctc_greedy_decoder(logits, seq_len)\n",
    "\n",
    "    # Inaccuracy: label error rate\n",
    "    ler = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32),\n",
    "                                          targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration to control GPU use\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.33\n",
    "# sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "num_batches_per_epoch = int(len(x_)/batch_size)\n",
    "\n",
    "with tf.Session(graph=graph,config=config) as session:\n",
    "    # Initializate the weights and biases\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    train_epoch = range(num_epochs)\n",
    "    train_edist = []\n",
    "    val_edist = []\n",
    "\n",
    "    for curr_epoch in range(num_epochs):\n",
    "        train_cost = train_ler = 0\n",
    "        start = time.time()\n",
    "\n",
    "        for batch in range(num_batches_per_epoch//1000):\n",
    "            b = int(np.random.randint(len(x_), size=1))\n",
    "            if not batch%100:\n",
    "                print('batch:', batch, b, x_[b].shape)\n",
    "                \n",
    "            feed = {inputs: x_[b],\n",
    "                    targets: y_[b],\n",
    "                    seq_len: [ x_[b].shape[1] ]}\n",
    "\n",
    "            batch_cost, _ = session.run([cost, optimizer], feed)\n",
    "            train_cost += batch_cost*batch_size\n",
    "            train_ler += session.run(ler, feed_dict=feed)*batch_size\n",
    "\n",
    "        train_cost /= num_examples\n",
    "        train_ler /= num_examples\n",
    "        \n",
    "        print(num_epochs, train_cost, train_ler,time.time() - start)\n",
    "        train_edist.append(train_ler)\n",
    "\n",
    "        val_cost, val_ler = 0, 0\n",
    "        for batch in range(len(val_x_)//20):\n",
    "            b = int(np.random.randint(len(val_x_), size=1) )\n",
    "            val_feed = {inputs: val_x_[b],\n",
    "                        targets: val_y_[b],\n",
    "                        seq_len: [val_x_[b].shape[1]]}\n",
    "\n",
    "            val_c, val_l = session.run([cost, ler], feed_dict=val_feed)\n",
    "            val_cost += val_c\n",
    "            val_ler += val_l \n",
    "\n",
    "        log = \"Epoch {}/{}, train_cost = {:.3f}, train_ler = {:.3f}, val_cost = {:.3f}, val_ler = {:.3f}, time = {:.3f}\"\n",
    "        print(log.format(curr_epoch+1, num_epochs, train_cost, train_ler,\n",
    "                         val_cost, val_ler, time.time() - start))\n",
    "        val_edist.append(val_ler)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for batch in range(len(test_x_)//1000):\n",
    "        b = int(batch)\n",
    "        test_feed = {inputs: test_x_[b],\n",
    "                        targets: test_y_[b],\n",
    "                        seq_len: [test_x_[b].shape[1]]}\n",
    "            \n",
    "        # Decoding\n",
    "        d = session.run(decoded[0], feed_dict=test_feed)\n",
    "        str_decoded = ''.join([chr(x) for x in np.asarray(d[1]) + FIRST_INDEX])\n",
    "        # Replacing blank label to none\n",
    "        str_decoded = str_decoded.replace(chr(ord('z') + 1), '')\n",
    "        # Replacing space label to space\n",
    "        str_decoded = str_decoded.replace(chr(ord('a') - 1), ' ')\n",
    "\n",
    "        print('Original:', test_text_[b])\n",
    "        print('Decoded:' , str_decoded)\n",
    "        \n",
    "        %matplotlib notebook\n",
    "        plt.figure()\n",
    "        plt.plot(train_epoch, train_edist, 'r-')\n",
    "        plt.plot(train_epoch, val_edist, 'b-')\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(right=0.8) \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
